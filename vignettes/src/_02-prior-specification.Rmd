---
title: "Prior Specifications in Bayesian Power Analysis"
date: "`r Sys.Date()`"
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "/man/figures/02-prior-specification-",
  out.width = "90%",
  message = FALSE
)

# Load the package
devtools::load_all(".")

# Fast settings for examples
n_cores <- parallel::detectCores() - 1
n_sims <- n_cores * 10
```



The `rctbayespower` package uses two types of priors: **estimation priors** (for Bayesian model fitting) and **design priors** (for integrating effect size uncertainty). This vignette demonstrates their impact on power analysis through two focused examples.

# Example 1: Different Estimation Priors

Estimation priors affect how treatment effects are estimated during Bayesian model fitting. Here we compare default priors versus informative priors.

## Power Analysis with Different Priors

```{r}
# Create ANCOVA model with uninformative priors
uninformative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0.5,
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("normal(0, 100)", class = "b"),
  prior_covariate = brms::set_prior("normal(0, 100)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("normal(0, 100)", class = "Intercept"),
  prior_sigma = brms::set_prior("normal(0, 100)", class = "sigma")
)

# Create design
uninformative_design <- build_design(
  model = uninformative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

# Create conditions
uninformative_conditions <- build_conditions(
  design = uninformative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

# Run power analysis
uninformative_analysis <- power_analysis(
  conditions = uninformative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)

summary(uninformative_analysis)
```


Analysis with informative, sceptical prior:

```{r}
# Create ANCOVA model with informative/sceptical priors
informative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0.5,
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("student_t(3, 0, 1)", class = "b"),
  prior_covariate = brms::set_prior("student_t(3, 0, 0.5)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("student_t(3, 0, 1)", class = "Intercept"),
  prior_sigma = brms::set_prior("student_t(3, 0, 2)", class = "sigma")
)

# Create design (same as uninformative)
informative_design <- build_design(
  model = informative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

# Create conditions
informative_conditions <- build_conditions(
  design = informative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

# Run power analysis
informative_analysis <- power_analysis(
  conditions = informative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)

summary(informative_analysis)
```

Comparing the results of the two analyses:
```{r}
estimation_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  Power_Success = c(
    uninformative_analysis$results_df$power_success,
    informative_analysis$results_df$power_success
  ),
  Power_Futility = c(
    uninformative_analysis$results_df$power_futility,
    informative_analysis$results_df$power_futility
  ),
  Mean_Effect = c(
    uninformative_analysis$results_df$mean_effect_estimate,
    informative_analysis$results_df$mean_effect_estimate
  ),
  SD_Effect = c(
    uninformative_analysis$results_df$sd_effect_estimate,
    informative_analysis$results_df$sd_effect_estimate
  )
) |>
  # round the numeric columns for better readability
  dplyr::mutate(
    across(c(Power_Success, Power_Futility, Mean_Effect, SD_Effect), ~ round(.x, 3))
  )

print(estimation_comparison)
```


## Alpha Error Rate

To evaluate the alpha error rate (false positive rate) for both prior configurations, we need to run the power analysis under the null hypothesis (effect size = 0) and examine how often we incorrectly conclude for success. This tells us how well our Bayesian decision framework controls Type I error.

```{r alpha_error_uninformative}
# Alpha error rate with uninformative priors
alpha_uninformative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0, # Null hypothesis: no effect
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("normal(0, 100)", class = "b"),
  prior_covariate = brms::set_prior("normal(0, 100)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("normal(0, 100)", class = "Intercept"),
  prior_sigma = brms::set_prior("normal(0, 100)", class = "sigma")
)

alpha_uninformative_design <- build_design(
  model = alpha_uninformative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

alpha_uninformative_conditions <- build_conditions(
  design = alpha_uninformative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

alpha_uninformative <- power_analysis(
  conditions = alpha_uninformative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)

summary(alpha_uninformative)
```

```{r alpha_error_informative}
# Alpha error rate with informative, sceptical priors
alpha_informative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0, # Null hypothesis: no effect
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("student_t(3, 0, 1)", class = "b"),
  prior_covariate = brms::set_prior("student_t(3, 0, 0.5)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("student_t(3, 0, 1)", class = "Intercept"),
  prior_sigma = brms::set_prior("student_t(3, 0, 2)", class = "sigma")
)

alpha_informative_design <- build_design(
  model = alpha_informative_model,
  target_params = "b_armtreat_1",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

alpha_informative_conditions <- build_conditions(
  design = alpha_informative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

alpha_informative <- power_analysis(
  conditions = alpha_informative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)

summary(alpha_informative)
```

Comparing the alpha error rates:
```{r alpha_comparison}
# Compare alpha error rates
alpha_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  Alpha_Error_Rate = c(
    alpha_uninformative$results_df$power_success,
    alpha_informative$results_df$power_success
  ),
  Probability_False_Positive = c(
    alpha_uninformative$results_df$mean_prob_success,
    alpha_informative$results_df$mean_prob_success
  )
) |>
  dplyr::mutate(
    across(c(Alpha_Error_Rate, Probability_False_Positive), ~ round(.x, 3))
  )

print(alpha_comparison)
```

Using a sceptical prior reduces the probability of falsely concluding success when there is no true effect, thus controlling the alpha error rate more effectively than the uninformative priors.


***


# Example 2: Design Prior vs No Design Prior

Design priors integrate uncertainty about the true effect size. Here we compare integrated power (with design prior) versus point estimates (without design prior).

```{r design_prior}
# Create ANCOVA model for design prior analysis
design_prior_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0.5, # Will be varied in conditions
  b_covariate = 0.3
)

# Create design
design_prior_design <- build_design(
  model = design_prior_model,
  target_params = "b_armtreat_1",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

# Create conditions with varying effect sizes
design_prior_conditions <- build_conditions(
  design = design_prior_design,
  condition_values = list(
    n_total = 160,
    b_arm_treat = seq(0.2, 0.8, 0.1)
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5)),
    b_covariate = 0.3
  )
)

# Run power analysis with design prior
fit_design_prior <- power_analysis(
  conditions = design_prior_conditions,
  design_prior = "normal(0.5, 0.1)", # Informed design prior
  n_simulations = n_sims,
  n_cores = n_cores
)

summary(fit_design_prior)
```

You can also compare different design priors by running separate analyses:

```{r design_prior_comparison}
# Run analysis with different design priors
fit_conservative <- power_analysis(
  conditions = design_prior_conditions,
  design_prior = "normal(0.4, 0.1)",
  n_simulations = n_sims,
  n_cores = n_cores
)

fit_optimistic <- power_analysis(
  conditions = design_prior_conditions,
  design_prior = "normal(0.7, 0.1)",
  n_simulations = n_sims,
  n_cores = n_cores
)

# Compare the results by looking at the power analysis results
cat("Original design prior results (normal(0.5, 0.1)):\n")
print(fit_design_prior$results_df[c("b_arm_treat", "pow_success")])

cat("\nConservative design prior results (normal(0.4, 0.1)):\n") 
print(fit_conservative$results_df[c("b_arm_treat", "pow_success")])

cat("\nOptimistic design prior results (normal(0.7, 0.1)):\n")
print(fit_optimistic$results_df[c("b_arm_treat", "pow_success")])
```

This demonstrates how different design priors affect the power analysis:
1. Conservative priors weight smaller effect sizes more heavily
2. Optimistic priors weight larger effect sizes more heavily  
3. The choice of design prior affects the integrated power calculations

***


```{r session_info}
sessionInfo()
```
