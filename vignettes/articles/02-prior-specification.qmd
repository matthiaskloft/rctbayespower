---
title: "Prior Specifications in 'rctbayespower'"
author: 
 - name: Matthias Kloft
   orcid: 0000-0003-1845-6957
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    embed-resources: true
execute:
  message: false
  warning: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "/man/figures/02-prior-specification-",
  out.width = "90%",
  message = FALSE
)

# Load the package
library(rctbayespower)

# Fast settings for examples
n_cores <- parallel::detectCores() - 1
n_sims <- 1e3
```



The `rctbayespower` package uses two types of priors: **estimation priors** (for Bayesian model fitting) and **design priors** (for integrating effect size uncertainty). This vignette demonstrates their impact on power analysis through two focused examples.

# Example 1: Different Estimation Priors

Estimation priors affect how treatment effects are estimated during Bayesian model fitting. Here we compare default priors versus informative priors.

## Power Analysis with Different Priors

Ancova model with two arms is used to illustrate the impact of different priors on power analysis results. We will compare uninformative and sceptical priors.


Analysis with uninformative priors:
```{r}
# Create ANCOVA model with uninformative priors
uninformative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0.5,
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("normal(0, 100)", class = "b"),
  prior_covariate = brms::set_prior("normal(0, 100)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("normal(0, 100)", class = "Intercept"),
  prior_sigma = brms::set_prior("normal(0, 100)", class = "sigma")
)

# Create design
uninformative_design <- build_design(
  model = uninformative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

# Create conditions
uninformative_conditions <- build_conditions(
  design = uninformative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

# Run power analysis
uninformative_analysis <- power_analysis(
  conditions = uninformative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)

uninformative_analysis$results_df|>
  # round the numeric columns for better readability
  kableExtra::kable(digits = 2)
```


Analysis with informative, sceptical prior:

```{r}
# Create ANCOVA model with informative/sceptical priors
informative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0.5,
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("student_t(3, 0, 1)", class = "b"),
  prior_covariate = brms::set_prior("student_t(3, 0, 0.5)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("student_t(3, 0, 1)", class = "Intercept"),
  prior_sigma = brms::set_prior("student_t(3, 0, 2)", class = "sigma")
)

# Create design (same as uninformative)
informative_design <- build_design(
  model = informative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

# Create conditions
informative_conditions <- build_conditions(
  design = informative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

# Run power analysis
informative_analysis <- power_analysis(
  conditions = informative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)
informative_analysis$results_df|>
  # round the numeric columns for better readability
  kableExtra::kable(digits = 2)
```

Comparing the results of the two analyses:
```{r}

estimation_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  "Success Power" = c(
    uninformative_analysis$results_df$success_power,
    informative_analysis$results_df$success_power
  ),
  "Futility Power" = c(
    uninformative_analysis$results_df$futility_power,
    informative_analysis$results_df$futility_power
  ),
  "Success Probability" = c(
    uninformative_analysis$results_df$success_prob,
    informative_analysis$results_df$success_prob
  ),
  "Futility Probability" = c(
    uninformative_analysis$results_df$futility_prob,
    informative_analysis$results_df$futility_prob
  ),
  "Mean Post Median" = c(
    uninformative_analysis$results_df$est_median,
    informative_analysis$results_df$est_median
  ),
  "Mean Post SD" = c(
    uninformative_analysis$results_df$est_sd,
    informative_analysis$results_df$est_sd
  )
) 

estimation_comparison|>
  # round the numeric columns for better readability
  kableExtra::kable(digits = 2)
```


## Alpha Error Rate

To evaluate the alpha error rate (false positive rate) for both prior configurations, we need to run the power analysis under the null hypothesis (effect size = 0) and examine how often we incorrectly conclude for success. This tells us how well our Bayesian decision framework controls Type I error.

```{r alpha_error_uninformative}
# Alpha error rate with uninformative priors
alpha_uninformative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0, # Null hypothesis: no effect
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("normal(0, 100)", class = "b"),
  prior_covariate = brms::set_prior("normal(0, 100)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("normal(0, 100)", class = "Intercept"),
  prior_sigma = brms::set_prior("normal(0, 100)", class = "sigma")
)

alpha_uninformative_design <- build_design(
  model = alpha_uninformative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

alpha_uninformative_conditions <- build_conditions(
  design = alpha_uninformative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

alpha_uninformative <- power_analysis(
  conditions = alpha_uninformative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)
```

```{r alpha_error_informative}
# Alpha error rate with informative, sceptical priors
alpha_informative_model <- build_model_ancova_cont_2arms(
  b_arm_treat = 0, # Null hypothesis: no effect
  b_covariate = 0.3,
  prior_treatment = brms::set_prior("student_t(3, 0, 1)", class = "b"),
  prior_covariate = brms::set_prior("student_t(3, 0, 0.5)", class = "b", coef = "covariate"),
  prior_intercept = brms::set_prior("student_t(3, 0, 1)", class = "Intercept"),
  prior_sigma = brms::set_prior("student_t(3, 0, 2)", class = "sigma")
)

alpha_informative_design <- build_design(
  model = alpha_informative_model,
  target_params = "b_arm2",
  n_interim_analyses = 0,
  thresholds_success = 0.3,
  thresholds_futility = 0.1,
  p_sig_success = 0.975,
  p_sig_futility = 0.5
)

alpha_informative_conditions <- build_conditions(
  design = alpha_informative_design,
  condition_values = list(
    n_total = 120
  ),
  static_values = list(
    p_alloc = list(c(0.5, 0.5))
  )
)

alpha_informative <- power_analysis(
  conditions = alpha_informative_conditions,
  n_simulations = n_sims,
  n_cores = n_cores
)
```

Comparing the alpha error rates:
```{r alpha_comparison}
# Compare alpha error rates
alpha_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  Alpha_Error_Rate = c(
    alpha_uninformative$results_df$success_power,
    alpha_informative$results_df$success_power
  ),
  Probability_False_Positive = c(
    alpha_uninformative$results_df$success_prob,
    alpha_informative$results_df$success_prob
  )
)
alpha_comparison |>
  kableExtra::kable(digits = 3)
```

Using a sceptical prior reduces the probability of falsely concluding success when there is no true effect, thus controlling the alpha error rate more effectively than the uninformative priors.


***


```{r session_info}
sessionInfo()
```
