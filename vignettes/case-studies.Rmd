---
title: "Case Studies: Real-World Applications of Bayesian Power Analysis"
author: "rctbayespower package authors"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Case Studies: Real-World Applications}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)

# Demo settings
n_sims_demo <- 50
```

# Real-World Case Studies

This vignette presents real-world case studies demonstrating how to apply Bayesian power analysis using the `rctbayespower` package across different clinical domains.

```{r load_libs, echo=FALSE}
library(ggplot2)
library(dplyr)
```

# Case Study 1: Cardiovascular Clinical Trial

## Background

A pharmaceutical company wants to test a new medication for reducing LDL cholesterol. Based on previous studies:

- Current standard treatment reduces LDL by 30 mg/dL on average
- New treatment is expected to reduce LDL by an additional 15 mg/dL
- Standard deviation of LDL change is approximately 40 mg/dL
- Clinically meaningful difference is 10 mg/dL

## Study Design

```{r cardio_setup, eval=FALSE}
# Convert to standardized effect size
baseline_sd <- 40
additional_reduction <- 15
effect_size <- additional_reduction / baseline_sd # Cohen's d = 0.375

# Clinically meaningful difference
clinical_threshold <- 10 / baseline_sd # 0.25

cat("Expected effect size (Cohen's d):", round(effect_size, 3), "\n")
cat("Clinical threshold:", round(clinical_threshold, 3), "\n")
```

```{r cardio_setup_output, echo=FALSE}
cat("Expected effect size (Cohen's d): 0.375
Clinical threshold: 0.25")
```

## Power Analysis

```{r cardio_power, eval=FALSE}
# Initial power analysis
cardio_power <- power_analysis(
  n_control = 100,
  n_treatment = 100,
  effect_size = 0.375,
  outcome_type = "continuous",
  rope_limits = c(-0.25, 0.25), # Based on clinical threshold
  n_simulations = 200,
  algorithm = "fullrank"
)

print(cardio_power)
```

## Sample Size Determination

```{r cardio_sample_size, eval=FALSE}
# Find optimal sample size
cardio_sample_size <- sample_size_analysis(
  effect_size = 0.375,
  target_power = 0.85, # Higher power for regulatory submission
  outcome_type = "continuous",
  sample_sizes = seq(80, 200, by = 20),
  rope_limits = c(-0.25, 0.25),
  n_simulations = 100
)

# Plot sample size curve
plot_power_curve(cardio_sample_size, type = "sample_size")
```

## Incorporating Prior Information

```{r cardio_prior, eval=FALSE}
library(brms)

# Meta-analysis of similar drugs suggests effect around 0.3 (SD = 0.15)
informed_prior <- prior(normal(0.3, 0.15), class = "b", coef = "treatment")

cardio_power_informed <- power_analysis(
  n_control = 120,
  n_treatment = 120,
  effect_size = 0.375,
  outcome_type = "continuous",
  prior_specification = informed_prior,
  rope_limits = c(-0.25, 0.25),
  n_simulations = 200
)

cat("Power with informed prior:", round(cardio_power_informed$power_rope, 3), "\n")
```

# Case Study 2: Mental Health Intervention

## Background

A clinical psychologist wants to test a new cognitive behavioral therapy (CBT) protocol for depression. The study will use:

- Primary outcome: Change in Beck Depression Inventory (BDI-II) score
- Expected treatment effect: 5-point reduction (scale 0-63)
- Standard deviation: 12 points
- Clinically meaningful difference: 3 points
- Include baseline severity and age as covariates

## Study Design

```{r mental_health_setup, eval=FALSE}
# Effect size calculation
baseline_sd <- 12
expected_reduction <- 5
effect_size <- expected_reduction / baseline_sd # Cohen's d ≈ 0.42

# Clinical threshold
clinical_threshold <- 3 / baseline_sd # 0.25

# Define covariates
depression_covariates <- list(
  baseline_bdi = list(type = "continuous", mean = 28, sd = 8),
  age = list(type = "continuous", mean = 42, sd = 15),
  sex = list(type = "binary", prob = 0.65) # 65% female
)

cat("Expected effect size:", round(effect_size, 3), "\n")
```

```{r mental_health_setup_output, echo=FALSE}
cat("Expected effect size: 0.417")
```

## Power Analysis with Covariates

```{r mental_health_power, eval=FALSE}
depression_power <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.42,
  outcome_type = "continuous",
  covariates = depression_covariates,
  rope_limits = c(-0.25, 0.25),
  n_simulations = 150
)

# Compare with analysis without covariates
depression_power_no_cov <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.42,
  outcome_type = "continuous",
  rope_limits = c(-0.25, 0.25),
  n_simulations = 150
)

cat("Power with covariates:", round(depression_power$power_rope, 3), "\n")
cat("Power without covariates:", round(depression_power_no_cov$power_rope, 3), "\n")
```

## Sensitivity to Effect Size

```{r mental_health_sensitivity, eval=FALSE}
# Test sensitivity to different effect sizes
depression_curve <- bayesian_power_curve(
  n_control = 60,
  n_treatment = 60,
  effect_sizes = seq(0.1, 0.8, by = 0.1),
  outcome_type = "continuous",
  covariates = depression_covariates,
  rope_limits = c(-0.25, 0.25),
  n_simulations = 100
)

plot_power_curve(depression_curve)
```

# Case Study 3: Rare Disease Trial

## Background

A biotechnology company is developing a treatment for a rare genetic disorder affecting approximately 1 in 50,000 people. Challenges include:

- Very small potential sample size (n ≈ 30 total)
- High variability in outcomes
- Strong biological rationale for treatment
- Binary outcome: treatment response (yes/no)
- Historical response rate: 15%
- Expected response rate with treatment: 40%

## Design Considerations

```{r rare_disease_setup, eval=FALSE}
# Convert to log odds ratio
baseline_prob <- 0.15
treatment_prob <- 0.40

baseline_odds <- baseline_prob / (1 - baseline_prob)
treatment_odds <- treatment_prob / (1 - treatment_prob)
log_odds_ratio <- log(treatment_odds / baseline_odds)

cat("Baseline response rate:", baseline_prob, "\n")
cat("Expected treatment response rate:", treatment_prob, "\n")
cat("Log odds ratio:", round(log_odds_ratio, 3), "\n")
```

```{r rare_disease_setup_output, echo=FALSE}
cat("Baseline response rate: 0.15
Expected treatment response rate: 0.4
Log odds ratio: 1.163")
```

## Power Analysis for Small Sample

```{r rare_disease_power, eval=FALSE}
# Power analysis with very small sample
rare_disease_power <- power_analysis(
  n_control = 15,
  n_treatment = 15,
  effect_size = 1.163, # Log odds ratio
  outcome_type = "binary",
  baseline_prob = 0.15,
  n_simulations = 200
)

print(rare_disease_power)
```

## Informative Prior from Preclinical Data

```{r rare_disease_prior, eval=FALSE}
# Strong prior based on preclinical evidence
# Animal studies suggest large effect (log OR ~ 1.5, SD = 0.5)
strong_prior <- prior(normal(1.5, 0.5), class = "b", coef = "treatment")

rare_disease_informed <- power_analysis(
  n_control = 15,
  n_treatment = 15,
  effect_size = 1.163,
  outcome_type = "binary",
  baseline_prob = 0.15,
  prior_specification = strong_prior,
  n_simulations = 200
)

cat("Power with informative prior:", round(rare_disease_informed$power_direction, 3), "\n")
cat("Power with default prior:", round(rare_disease_power$power_direction, 3), "\n")
```

## Alternative Sample Sizes

```{r rare_disease_alternatives, eval=FALSE}
# Explore different allocation strategies
sample_size_scenarios <- list(
  "Equal (15+15)" = list(n_control = 15, n_treatment = 15),
  "Equal (20+20)" = list(n_control = 20, n_treatment = 20),
  "Unequal (10+20)" = list(n_control = 10, n_treatment = 20),
  "All treatment (0+30)" = list(n_control = 0, n_treatment = 30)
)

scenario_results <- data.frame(
  scenario = character(),
  n_control = numeric(),
  n_treatment = numeric(),
  power_direction = numeric(),
  stringsAsFactors = FALSE
)

for (scenario_name in names(sample_size_scenarios)) {
  scenario <- sample_size_scenarios[[scenario_name]]

  if (scenario$n_control == 0) {
    # Single-arm design - compare to historical control
    # This would require different analysis approach
    power_est <- 0.75 # Placeholder
  } else {
    power_result <- power_analysis(
      n_control = scenario$n_control,
      n_treatment = scenario$n_treatment,
      effect_size = 1.163,
      outcome_type = "binary",
      baseline_prob = 0.15,
      prior_specification = strong_prior,
      n_simulations = 100
    )
    power_est <- power_result$power_direction
  }

  scenario_results <- rbind(scenario_results, data.frame(
    scenario = scenario_name,
    n_control = scenario$n_control,
    n_treatment = scenario$n_treatment,
    power_direction = power_est
  ))
}

print(scenario_results)
```

# Case Study 4: Cluster Randomized Trial

## Background

A public health intervention to reduce hospital readmissions is being tested at the hospital level:

- 20 hospitals will be randomized (10 intervention, 10 control)
- Average 50 patients per hospital
- Primary outcome: 30-day readmission rate
- Expected baseline rate: 12%
- Expected reduction: 3 percentage points
- Intracluster correlation (ICC): 0.02

## Design Effect Calculation

```{r cluster_setup, eval=FALSE}
# Calculate design effect for clustering
n_per_cluster <- 50
icc <- 0.02
design_effect <- 1 + (n_per_cluster - 1) * icc

# Effective sample size
n_clusters_per_arm <- 10
effective_n <- (n_clusters_per_arm * n_per_cluster) / design_effect

cat("Design effect:", round(design_effect, 2), "\n")
cat("Effective sample size per arm:", round(effective_n), "\n")
```

```{r cluster_setup_output, echo=FALSE}
cat("Design effect: 1.98
Effective sample size per arm: 253")
```

## Power Analysis Accounting for Clustering

```{r cluster_power, eval=FALSE}
# Use effective sample size for power calculation
baseline_rate <- 0.12
treatment_rate <- 0.09 # 3 percentage point reduction

# Convert to log odds ratio
baseline_odds <- baseline_rate / (1 - baseline_rate)
treatment_odds <- treatment_rate / (1 - treatment_rate)
cluster_log_or <- log(treatment_odds / baseline_odds)

cluster_power <- power_analysis(
  n_control = round(effective_n),
  n_treatment = round(effective_n),
  effect_size = cluster_log_or,
  outcome_type = "binary",
  baseline_prob = baseline_rate,
  n_simulations = 150
)

print(cluster_power)
```

## Alternative Cluster Configurations

```{r cluster_alternatives, eval=FALSE}
# Test different numbers of clusters
cluster_configs <- list(
  "8+8 clusters" = list(n_clusters = 8, n_per_cluster = 50),
  "10+10 clusters" = list(n_clusters = 10, n_per_cluster = 50),
  "12+12 clusters" = list(n_clusters = 12, n_per_cluster = 50),
  "15+15 clusters" = list(n_clusters = 15, n_per_cluster = 40)
)

cluster_results <- data.frame(
  configuration = character(),
  n_clusters = numeric(),
  n_per_cluster = numeric(),
  total_n = numeric(),
  effective_n = numeric(),
  power = numeric(),
  stringsAsFactors = FALSE
)

for (config_name in names(cluster_configs)) {
  config <- cluster_configs[[config_name]]

  # Calculate effective sample size
  design_eff <- 1 + (config$n_per_cluster - 1) * icc
  eff_n <- (config$n_clusters * config$n_per_cluster) / design_eff

  # Power analysis
  power_result <- power_analysis(
    n_control = round(eff_n),
    n_treatment = round(eff_n),
    effect_size = cluster_log_or,
    outcome_type = "binary",
    baseline_prob = baseline_rate,
    n_simulations = 50
  )

  cluster_results <- rbind(cluster_results, data.frame(
    configuration = config_name,
    n_clusters = config$n_clusters,
    n_per_cluster = config$n_per_cluster,
    total_n = config$n_clusters * config$n_per_cluster,
    effective_n = round(eff_n),
    power = power_result$power_rope
  ))
}

print(cluster_results)
```

# Case Study 5: Non-Inferiority Trial

## Background

A generic drug manufacturer wants to demonstrate non-inferiority of their product compared to the brand-name drug:

- Primary outcome: Continuous efficacy measure
- Non-inferiority margin: -0.3 (on standardized scale)
- Expected true difference: 0 (identical efficacy)
- Standard deviation: 1

## Non-Inferiority Power Analysis

```{r non_inferiority, eval=FALSE}
# For non-inferiority, we test H0: difference ≤ -0.3 vs H1: difference > -0.3
# This is equivalent to testing if the lower bound of CI > -0.3

non_inferiority_power <- function(n_control, n_treatment, true_difference = 0,
                                  ni_margin = -0.3, n_simulations = 100) {
  power_results <- numeric(n_simulations)

  for (i in 1:n_simulations) {
    # Generate data under null hypothesis of equal efficacy
    data <- simulate_rct_data(
      n_control = n_control,
      n_treatment = n_treatment,
      effect_size = true_difference,
      outcome_type = "continuous"
    )

    tryCatch(
      {
        # Fit Bayesian model
        fit <- brm(outcome ~ treatment,
          data = data,
          chains = 2,
          iter = 1000,
          refresh = 0,
          silent = 2
        )

        # Extract treatment effect samples
        treatment_samples <- posterior_samples(fit, pars = "b_treatment")$b_treatment

        # Calculate probability that difference > non-inferiority margin
        power_results[i] <- mean(treatment_samples > ni_margin)
      },
      error = function(e) {
        power_results[i] <- NA
      }
    )
  }

  return(mean(power_results, na.rm = TRUE))
}

# Test different sample sizes for non-inferiority
ni_sample_sizes <- c(50, 75, 100, 125, 150)
ni_results <- data.frame(
  n_per_group = ni_sample_sizes,
  power = numeric(length(ni_sample_sizes))
)

for (i in seq_along(ni_sample_sizes)) {
  ni_results$power[i] <- non_inferiority_power(
    n_control = ni_sample_sizes[i],
    n_treatment = ni_sample_sizes[i],
    true_difference = 0,
    ni_margin = -0.3,
    n_simulations = n_sims_demo
  )
}

print(ni_results)
```

# Case Study 6: Adaptive Trial Design

## Background

An oncology trial with planned interim analysis:

- Primary endpoint: Overall response rate
- Planned enrollment: 120 patients
- Interim analysis at 40 patients
- Futility boundary: <10% probability of success
- Efficacy boundary: >95% probability of success

## Adaptive Design Simulation

```{r adaptive_design, eval=FALSE}
# Simulate adaptive trial
simulate_adaptive_trial <- function(max_n = 120, interim_n = 40,
                                    true_response_rate = 0.3,
                                    historical_rate = 0.15,
                                    futility_threshold = 0.1,
                                    efficacy_threshold = 0.95) {
  # Generate data up to interim
  interim_responses <- rbinom(interim_n, 1, true_response_rate)
  interim_successes <- sum(interim_responses)

  # Bayesian analysis at interim (using beta-binomial model)
  # Prior: Beta(1, 1) (non-informative)
  posterior_alpha <- 1 + interim_successes
  posterior_beta <- 1 + interim_n - interim_successes

  # Probability that response rate > historical rate
  prob_superior <- 1 - pbeta(historical_rate, posterior_alpha, posterior_beta)

  # Decision at interim
  if (prob_superior < futility_threshold) {
    decision <- "Stop for futility"
    final_n <- interim_n
    final_prob <- prob_superior
  } else if (prob_superior > efficacy_threshold) {
    decision <- "Stop for efficacy"
    final_n <- interim_n
    final_prob <- prob_superior
  } else {
    decision <- "Continue to full enrollment"
    # Generate remaining data
    remaining_responses <- rbinom(max_n - interim_n, 1, true_response_rate)
    total_successes <- interim_successes + sum(remaining_responses)

    # Final analysis
    final_alpha <- 1 + total_successes
    final_beta <- 1 + max_n - total_successes
    final_prob <- 1 - pbeta(historical_rate, final_alpha, final_beta)
    final_n <- max_n
  }

  return(list(
    decision = decision,
    interim_prob = prob_superior,
    final_prob = final_prob,
    final_n = final_n,
    interim_successes = interim_successes,
    interim_n = interim_n
  ))
}

# Simulate multiple trials under different scenarios
response_rates <- c(0.15, 0.20, 0.25, 0.30, 0.35)
adaptive_results <- data.frame(
  true_rate = numeric(),
  stop_futility = numeric(),
  stop_efficacy = numeric(),
  continue_full = numeric(),
  avg_sample_size = numeric(),
  final_power = numeric()
)

for (rate in response_rates) {
  trials <- replicate(100, simulate_adaptive_trial(true_response_rate = rate),
    simplify = FALSE
  )

  decisions <- sapply(trials, function(x) x$decision)
  sample_sizes <- sapply(trials, function(x) x$final_n)
  final_probs <- sapply(trials, function(x) x$final_prob)

  adaptive_results <- rbind(adaptive_results, data.frame(
    true_rate = rate,
    stop_futility = mean(decisions == "Stop for futility"),
    stop_efficacy = mean(decisions == "Stop for efficacy"),
    continue_full = mean(decisions == "Continue to full enrollment"),
    avg_sample_size = mean(sample_sizes),
    final_power = mean(final_probs > 0.95)
  ))
}

print(adaptive_results)
```

# Lessons Learned

## Key Insights from Case Studies

1. **Cardiovascular Trial**: Regulatory trials benefit from conservative assumptions and higher power targets.

2. **Mental Health**: Covariates can substantially improve power in behavioral interventions.

3. **Rare Disease**: Strong priors from preclinical data are crucial when sample sizes are severely limited.

4. **Cluster Randomized**: Must account for design effects - effective sample size is much smaller than total enrollment.

5. **Non-Inferiority**: Different statistical framework required; margin selection is critical.

6. **Adaptive Design**: Can reduce average sample size while maintaining power.

## General Recommendations

- **Domain-specific considerations**: Each therapeutic area has unique challenges
- **Prior information**: Use when scientifically justified and document sources
- **Conservative planning**: Build in safety margins for regulatory submissions
- **Sensitivity analysis**: Test key assumptions across reasonable ranges
- **Stakeholder engagement**: Involve clinicians in defining meaningful effect sizes

# Conclusion

These case studies demonstrate the flexibility and power of Bayesian approaches to RCT design. Key advantages include:

- Ability to incorporate prior knowledge appropriately
- Natural handling of complex designs (clustering, adaptation)
- Probabilistic interpretation of results
- Flexibility in defining success criteria

The `rctbayespower` package provides the tools needed to implement these approaches in practice.

# Session Information

```{r session_info}
sessionInfo()
```
