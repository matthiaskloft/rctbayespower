---
title: "Advanced Bayesian Power Analysis Techniques"
author: "rctbayespower package authors"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Advanced Bayesian Power Analysis Techniques}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)

# Reduced simulation counts for vignette building
n_sims_demo <- 2 # Use minimal simulations for fast demo builds
brms_args_demo <- list(algorithm = "meanfield") # Fast algorithm for testing
```

# Advanced Techniques in Bayesian RCT Power Analysis

This vignette covers advanced techniques and scenarios for Bayesian power analysis using the `rctbayespower` package.

```{r load_libs, echo=FALSE}
library(ggplot2)
library(dplyr)
```

# Complex Study Designs

## Unequal Allocation Ratios

Sometimes studies use unequal allocation ratios (e.g., 2:1 treatment to control):

```{r unequal_allocation, eval=FALSE}
# 2:1 allocation ratio
power_unequal <- power_analysis(
  n_control = 50,
  n_treatment = 100, # 2:1 ratio
  effect_size = 0.4,
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)

# Compare with equal allocation
power_equal <- power_analysis(
  n_control = 75,
  n_treatment = 75, # Same total N = 150
  effect_size = 0.4,
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)

# Compare power
cat("Unequal allocation (2:1) power:", round(power_unequal$power_rope, 3), "\n")
cat("Equal allocation power:", round(power_equal$power_rope, 3), "\n")
```

## Multi-arm Studies

For studies with multiple treatment arms, analyze each comparison separately:

```{r multiarm, eval=FALSE}
# Treatment A vs Control
power_A_vs_C <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.5, # Treatment A effect
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)

# Treatment B vs Control
power_B_vs_C <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.3, # Treatment B effect (smaller)
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)

# Treatment A vs Treatment B (head-to-head)
power_A_vs_B <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.2, # Difference between treatments
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)
```

# Advanced Prior Specifications

## Informative Priors

Using informative priors based on previous studies:

```{r informative_priors, eval=FALSE}
library(brms)

# Moderately informative prior based on meta-analysis
# Previous studies suggest effect around 0.3 with SD 0.2
informed_prior <- prior(normal(0.3, 0.2), class = "b", coef = "treatment")

power_informed <- power_analysis(
  n_control = 40,
  n_treatment = 40,
  effect_size = 0.4, # True effect
  outcome_type = "continuous",
  prior_specification = informed_prior,
  n_simulations = n_sims_demo
)

# Compare with default (weakly informative) prior
power_default <- power_analysis(
  n_control = 40,
  n_treatment = 40,
  effect_size = 0.4,
  outcome_type = "continuous",
  n_simulations = n_sims_demo
)

cat("Power with informed prior:", round(power_informed$power_rope, 3), "\n")
cat("Power with default prior:", round(power_default$power_rope, 3), "\n")
```

## Skeptical Priors

When you want to be conservative about treatment effects:

```{r skeptical_priors, eval=FALSE}
# Skeptical prior centered at zero with narrow SD
skeptical_prior <- prior(normal(0, 0.1), class = "b", coef = "treatment")

power_skeptical <- power_analysis(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.5,
  outcome_type = "continuous",
  prior_specification = skeptical_prior,
  n_simulations = n_sims_demo
)
```

# Complex Outcome Models

## Hierarchical/Mixed Models

For clustered data (e.g., patients within clinics):

```{r hierarchical, eval=FALSE}
# Simulate clustered data
simulate_clustered_data <- function(n_clusters_control, n_clusters_treatment,
                                    n_per_cluster, effect_size, icc = 0.05) {
  total_control <- n_clusters_control * n_per_cluster
  total_treatment <- n_clusters_treatment * n_per_cluster

  # Create cluster IDs
  cluster_id <- c(
    rep(1:n_clusters_control, each = n_per_cluster),
    rep((n_clusters_control + 1):(n_clusters_control + n_clusters_treatment),
      each = n_per_cluster
    )
  )

  # Treatment assignment
  treatment <- c(rep(0, total_control), rep(1, total_treatment))

  # Generate random effects for clusters
  cluster_effects <- rnorm(n_clusters_control + n_clusters_treatment, 0, sqrt(icc))

  # Generate outcomes with cluster effects
  outcome <- rnorm(
    total_control + total_treatment,
    treatment * effect_size + cluster_effects[cluster_id],
    sqrt(1 - icc)
  )

  data.frame(
    id = 1:(total_control + total_treatment),
    cluster_id = cluster_id,
    treatment = treatment,
    outcome = outcome
  )
}

# Custom power analysis for clustered design
clustered_power_analysis <- function(n_clusters_control, n_clusters_treatment,
                                     n_per_cluster, effect_size, n_simulations = n_sims_demo) {
  power_results <- vector("numeric", n_simulations)

  for (i in 1:n_simulations) {
    # Generate clustered data
    data <- simulate_clustered_data(
      n_clusters_control, n_clusters_treatment,
      n_per_cluster, effect_size
    )

    # Fit mixed model
    tryCatch(
      {
        fit <- brm(outcome ~ treatment + (1 | cluster_id),
          data = data,
          chains = 2,
          iter = 500,
          refresh = 0,
          silent = 2
        )

        # Extract treatment effect
        treatment_samples <- posterior_samples(fit, pars = "b_treatment")$b_treatment

        # Calculate power (probability effect > 0)
        power_results[i] <- mean(treatment_samples > 0)
      },
      error = function(e) {
        power_results[i] <- NA
      }
    )
  }

  return(mean(power_results, na.rm = TRUE))
}

# Example usage
clustered_power <- clustered_power_analysis(
  n_clusters_control = 10,
  n_clusters_treatment = 10,
  n_per_cluster = 5,
  effect_size = 0.5,
  n_simulations = n_sims_demo
)

cat("Power for clustered design:", round(clustered_power, 3), "\n")
```

# Adaptive Designs

## Sequential Monitoring

Planning for interim analyses with Bayesian monitoring:

```{r sequential, eval=FALSE}
# Function for sequential Bayesian monitoring
sequential_analysis <- function(max_n_per_group, interim_points, effect_size,
                                stop_threshold = 0.95) {
  results <- list()

  for (interim_n in interim_points) {
    # Power analysis at interim point
    interim_power <- power_analysis(
      n_control = interim_n,
      n_treatment = interim_n,
      effect_size = effect_size,
      outcome_type = "continuous",
      n_simulations = n_sims_demo
    )

    results[[as.character(interim_n)]] <- list(
      n_per_group = interim_n,
      power_direction = interim_power$power_direction,
      power_rope = interim_power$power_rope,
      stop_for_efficacy = interim_power$power_direction > stop_threshold,
      stop_for_futility = interim_power$power_direction < 0.1
    )
  }

  return(results)
}

# Plan sequential analysis
interim_points <- c(20, 40, 60, 80, 100)
sequential_results <- sequential_analysis(
  max_n_per_group = 100,
  interim_points = interim_points,
  effect_size = 0.5
)

# Display results
for (n in names(sequential_results)) {
  result <- sequential_results[[n]]
  cat(
    "N =", result$n_per_group,
    "Power =", round(result$power_direction, 3),
    "Stop for efficacy:", result$stop_for_efficacy, "\n"
  )
}
```

# Simulation-Based Validation

## Operating Characteristics

Evaluate the operating characteristics of your design under different scenarios:

```{r operating_characteristics, eval=FALSE}
# Function to evaluate design under multiple scenarios
evaluate_design_performance <- function(n_control, n_treatment,
                                        true_effect_sizes, n_simulations = n_sims_demo) {
  results <- data.frame(
    true_effect = numeric(),
    power_rope = numeric(),
    power_direction = numeric(),
    type_i_error = numeric(),
    bias = numeric()
  )

  for (true_effect in true_effect_sizes) {
    power_result <- power_analysis(
      n_control = n_control,
      n_treatment = n_treatment,
      effect_size = true_effect,
      outcome_type = "continuous",
      n_simulations = n_simulations
    )

    # Calculate metrics
    type_i_error <- if (true_effect == 0) power_result$power_direction else NA
    bias <- power_result$mean_effect_estimate - true_effect

    results <- rbind(results, data.frame(
      true_effect = true_effect,
      power_rope = power_result$power_rope,
      power_direction = power_result$power_direction,
      type_i_error = type_i_error,
      bias = bias
    ))
  }

  return(results)
}

# Evaluate design under multiple scenarios
design_performance <- evaluate_design_performance(
  n_control = 50,
  n_treatment = 50,
  true_effect_sizes = c(0, 0.2, 0.5, 0.8),
  n_simulations = n_sims_demo
)

print(design_performance)
```

# Handling Missing Data

## Multiple Imputation Sensitivity

```{r missing_data, eval=FALSE}
# Function to simulate missing data and analyze power
power_with_missing_data <- function(n_control, n_treatment, effect_size,
                                    missing_rate = 0.1, n_simulations = n_sims_demo) {
  successful_analyses <- 0
  power_estimates <- numeric()

  for (i in 1:n_simulations) {
    # Generate complete data
    data <- simulate_rct_data(
      n_control = n_control,
      n_treatment = n_treatment,
      effect_size = effect_size,
      outcome_type = "continuous"
    )

    # Introduce missing data
    n_missing <- round(nrow(data) * missing_rate)
    missing_indices <- sample(nrow(data), n_missing)
    data$outcome[missing_indices] <- NA

    # Analyze with complete cases only
    complete_data <- data[complete.cases(data), ]

    if (nrow(complete_data) > 10) { # Minimum sample size
      tryCatch(
        {
          fit <- brm(outcome ~ treatment,
            data = complete_data,
            chains = 2,
            iter = 500,
            refresh = 0,
            silent = 2
          )

          treatment_samples <- posterior_samples(fit, pars = "b_treatment")$b_treatment
          power_estimates[i] <- mean(treatment_samples > 0)
          successful_analyses <- successful_analyses + 1
        },
        error = function(e) {
          power_estimates[i] <- NA
        }
      )
    }
  }

  return(list(
    power = mean(power_estimates, na.rm = TRUE),
    successful_rate = successful_analyses / n_simulations,
    effective_sample_size = n_control + n_treatment -
      round((n_control + n_treatment) * missing_rate)
  ))
}

# Compare power with and without missing data
power_complete <- power_analysis(50, 50, 0.5, "continuous", n_sims_demo)
power_missing <- power_with_missing_data(50, 50, 0.5, missing_rate = 0.15, n_sims_demo)

cat("Power with complete data:", round(power_complete$power_direction, 3), "\n")
cat("Power with 15% missing data:", round(power_missing$power, 3), "\n")
cat("Effective sample size:", power_missing$effective_sample_size, "\n")
```

# Cost-Effectiveness Considerations

## Power vs. Cost Trade-offs

```{r cost_effectiveness, eval=FALSE}
# Function to calculate power per dollar
power_cost_analysis <- function(effect_size, cost_per_participant = 1000,
                                fixed_costs = 50000, target_power = 0.8) {
  # Test different sample sizes
  sample_sizes <- seq(20, 150, by = 10)
  results <- data.frame(
    n_per_group = numeric(),
    total_cost = numeric(),
    power = numeric(),
    cost_per_power = numeric()
  )

  for (n in sample_sizes) {
    power_result <- power_analysis(
      n_control = n,
      n_treatment = n,
      effect_size = effect_size,
      outcome_type = "continuous",
      n_simulations = n_sims_demo
    )

    total_cost <- fixed_costs + (2 * n * cost_per_participant)
    cost_per_power <- total_cost / power_result$power_rope

    results <- rbind(results, data.frame(
      n_per_group = n,
      total_cost = total_cost,
      power = power_result$power_rope,
      cost_per_power = cost_per_power
    ))
  }

  # Find most cost-effective design achieving target power
  adequate_power <- results$power >= target_power
  if (any(adequate_power)) {
    optimal_design <- results[adequate_power, ][which.min(results$cost_per_power[adequate_power]), ]
    return(list(results = results, optimal_design = optimal_design))
  } else {
    return(list(results = results, optimal_design = NULL))
  }
}

# Analyze cost-effectiveness
cost_analysis <- power_cost_analysis(
  effect_size = 0.5,
  cost_per_participant = 1000,
  fixed_costs = 50000,
  target_power = 0.8
)

if (!is.null(cost_analysis$optimal_design)) {
  cat("Optimal design:\n")
  cat("Sample size per group:", cost_analysis$optimal_design$n_per_group, "\n")
  cat("Total cost: $", format(cost_analysis$optimal_design$total_cost, big.mark = ","), "\n")
  cat("Power:", round(cost_analysis$optimal_design$power, 3), "\n")
}
```

# Regulatory Considerations

## FDA/EMA Guidelines

When planning studies for regulatory submission:

```{r regulatory, eval=FALSE}
# Conservative analysis for regulatory submission
regulatory_power_analysis <- function(n_control, n_treatment, effect_size) {
  # Use conservative settings
  power_result <- power_analysis(
    n_control = n_control,
    n_treatment = n_treatment,
    effect_size = effect_size,
    outcome_type = "continuous",
    prior_specification = "default", # Weakly informative
    rope_limits = c(-0.05, 0.05), # Narrow ROPE
    prob_threshold = 0.975, # Higher threshold
    n_simulations = n_sims_demo
  )

  # Additional frequentist analysis for comparison
  freq_power <- power.t.test(
    n = n_control,
    delta = effect_size,
    sd = 1,
    sig.level = 0.025, # One-sided
    type = "two.sample",
    alternative = "one.sided"
  )

  return(list(
    bayesian_power = power_result,
    frequentist_power = freq_power$power
  ))
}

# Example regulatory analysis
regulatory_result <- regulatory_power_analysis(75, 75, 0.4)
cat("Bayesian power (conservative):", round(regulatory_result$bayesian_power$power_rope, 3), "\n")
cat("Frequentist power:", round(regulatory_result$frequentist_power, 3), "\n")
```

# Best Practices Summary

1. **Simulation Count**: Use ≥1000 simulations for final analyses
2. **Prior Sensitivity**: Test multiple prior specifications
3. **Model Checking**: Always check convergence diagnostics
4. **Multiple Scenarios**: Evaluate under pessimistic and optimistic scenarios
5. **Documentation**: Document all assumptions and decisions
6. **Validation**: Compare with frequentist methods when appropriate
7. **Missing Data**: Plan for and simulate missing data scenarios
8. **Interim Analyses**: Consider adaptive designs for long studies

# Conclusion

Advanced Bayesian power analysis provides flexibility and insights not available with traditional methods. The key is to:

- Use informative priors when justified
- Consider the full range of possible scenarios
- Plan for real-world complications (missing data, clustering, etc.)
- Validate assumptions through simulation

This comprehensive approach leads to more robust and realistic study designs.

# Session Information

```{r session_info}
sessionInfo()
```
