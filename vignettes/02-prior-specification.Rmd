---
title: "Prior Specifications in Bayesian Power Analysis"
date: "2025-07-17"
always_allow_html: true
---

The `rctbayespower` package uses two types of priors: **estimation
priors** (for Bayesian model fitting) and **design priors** (for
integrating effect size uncertainty). This vignette demonstrates their
impact on power analysis through two focused examples.

# Example 1: Different Estimation Priors

Estimation priors affect how treatment effects are estimated during
Bayesian model fitting. Here we compare default priors versus
informative priors.

## Power Analysis with Different Priors

``` r
uninformative_analysis <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60, 
  effect_size = 0.4, 
  baseline_effect = 0.3, 
  outcome_type = "continuous", 
  threshold_success = 0.3, 
  threshold_futility = 0.1, 
  n_simulation = n_sims, 
  n_cores = n_cores, 
  priors_treatment = "normal(0, 100)", 
  priors_baseline = "normal(0, 100)", 
  priors_intercept = "normal(0, 100)", 
  priors_sigma = "normal(0, 100)"
  )
#> Running 150 power simulations in parallel using 15 cores...
#> Setting up design model with true parameters...
#> Setting up design model with estimation priors...
#> Progress: 15/150 (10.0%) - Elapsed: 0.1 min - ETC: 0.6 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.1 min - ETC: 0.6 min
#> Progress: 45/150 (30.0%) - Elapsed: 0.2 min - ETC: 0.5 min
#> Progress: 60/150 (40.0%) - Elapsed: 0.2 min - ETC: 0.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 0.3 min - ETC: 0.3 min
#> Progress: 90/150 (60.0%) - Elapsed: 0.4 min - ETC: 0.2 min
#> Progress: 105/150 (70.0%) - Elapsed: 0.4 min - ETC: 0.2 min
#> Progress: 120/150 (80.0%) - Elapsed: 0.5 min - ETC: 0.1 min
#> Progress: 135/150 (90.0%) - Elapsed: 0.5 min - ETC: 0.1 min
#> Completed: 150/150 (100%) - Total time: 0.6 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.411 
#> SD of effect estimate (median): 0.187 
#> Power - Success: 0.06 
#> Power - Futility: 0.06 
#> Mean probability of success: 0.651 
#> Mean probability of futility: 0.134

summary(uninformative_analysis)
#> 
#> === Bayesian Power Analysis Summary ===
#> 
#> Study Design:
#>   Control group size: 60 
#>   Treatment group size: 60 
#>   Target parameter: grouptreat 
#>   Success threshold: 0.3 
#>   Futility threshold: 0.1 
#>   Success probability threshold: 0.975 
#> 
#>   Futility probability threshold: 0.5 
#> 
#> Model Information:
#>   Family: gaussian(identity) 
#>   Estimation formula: baseline + group 
#>   Design formula: baseline + group 
#> 
#> Simulation Overview:
#>   Total simulations: 150 
#>   Successful fits: 150 
#>   Convergence rate: 100 %
#> 
#> Treatment Effect Estimates:
#>   Mean effect estimate (median): 0.411 
#>   SD of effect estimates (median): 0.187 
#> 
#> Power Analysis Results:
#>   Power - Success: 0.06 (MCSE: 0.0194 )
#>   Power - Futility: 0.06 (MCSE: 0.0194 )
#> 
#> Decision Probabilities:
#>   Mean probability of success: 0.651 (MCSE: 0.0235 )
#>   Mean probability of futility: 0.134 (MCSE: 0.0155 )
#> 
#> === End Summary ===
```

Analysis with informative, sceptical prior:

``` r
informative_analysis <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.4,
  baseline_effect = 0.3,
  outcome_type = "continuous",
  threshold_success = 0.3,
  threshold_futility = 0.1,
  n_simulation = n_sims,
  n_cores = n_cores,
  priors_treatment = "student_t(3, 0, 1)",
  priors_baseline = "student_t(3, 0, .5)",
  priors_intercept = "student_t(3, 0, 1)",
  priors_sigma = "student_t(3, 0, 2)"
)
#> Running 150 power simulations in parallel using 15 cores...
#> Setting up design model with true parameters...
#> Setting up design model with estimation priors...
#> Progress: 15/150 (10.0%) - Elapsed: 0.1 min - ETC: 0.8 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.2 min - ETC: 0.7 min
#> Progress: 45/150 (30.0%) - Elapsed: 0.2 min - ETC: 0.6 min
#> Progress: 60/150 (40.0%) - Elapsed: 0.3 min - ETC: 0.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 0.4 min - ETC: 0.4 min
#> Progress: 90/150 (60.0%) - Elapsed: 0.5 min - ETC: 0.3 min
#> Progress: 105/150 (70.0%) - Elapsed: 0.6 min - ETC: 0.3 min
#> Progress: 120/150 (80.0%) - Elapsed: 0.7 min - ETC: 0.2 min
#> Progress: 135/150 (90.0%) - Elapsed: 0.8 min - ETC: 0.1 min
#> Completed: 150/150 (100%) - Total time: 0.9 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.407 
#> SD of effect estimate (median): 0.172 
#> Power - Success: 0.067 
#> Power - Futility: 0.04 
#> Mean probability of success: 0.657 
#> Mean probability of futility: 0.118
summary(informative_analysis)
#> 
#> === Bayesian Power Analysis Summary ===
#> 
#> Study Design:
#>   Control group size: 60 
#>   Treatment group size: 60 
#>   Target parameter: grouptreat 
#>   Success threshold: 0.3 
#>   Futility threshold: 0.1 
#>   Success probability threshold: 0.975 
#> 
#>   Futility probability threshold: 0.5 
#> 
#> Model Information:
#>   Family: gaussian(identity) 
#>   Estimation formula: baseline + group 
#>   Design formula: baseline + group 
#> 
#> Simulation Overview:
#>   Total simulations: 150 
#>   Successful fits: 150 
#>   Convergence rate: 100 %
#> 
#> Treatment Effect Estimates:
#>   Mean effect estimate (median): 0.407 
#>   SD of effect estimates (median): 0.172 
#> 
#> Power Analysis Results:
#>   Power - Success: 0.067 (MCSE: 0.0204 )
#>   Power - Futility: 0.04 (MCSE: 0.016 )
#> 
#> Decision Probabilities:
#>   Mean probability of success: 0.657 (MCSE: 0.0221 )
#>   Mean probability of futility: 0.118 (MCSE: 0.0134 )
#> 
#> === End Summary ===
```

Comparing the results of the two analyses:

``` r
estimation_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  Power_Success = c(
    uninformative_analysis$power_success,
    informative_analysis$power_success
  ),
  Power_Futility = c(
    uninformative_analysis$power_futility,
    informative_analysis$power_futility
  ),
  Mean_Effect = c(
    uninformative_analysis$mean_effect_estimate,
    informative_analysis$mean_effect_estimate
  ),
  SD_Effect = c(
    uninformative_analysis$sd_mean_effect_estimate,
    informative_analysis$sd_mean_effect_estimate
  )
) |> 
  # round the numeric columns for better readability
  dplyr::mutate(
    across(c(Power_Success, Power_Futility, Mean_Effect, SD_Effect), ~ round(.x, 3)))

print(estimation_comparison)
#>      Prior_Type Power_Success Power_Futility Mean_Effect SD_Effect
#> 1 Uninformative         0.060           0.06       0.396     0.187
#> 2     Sceptical         0.067           0.04       0.398     0.172
```

## Alpha Error Rate

To evaluate the alpha error rate (false positive rate) for both prior
configurations, we need to run the power analysis under the null
hypothesis (effect size = 0) and examine how often we incorrectly
conclude for success. This tells us how well our Bayesian decision
framework controls Type I error.

``` r
# Alpha error rate with uninformative priors
alpha_uninformative <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60, 
  effect_size = 0,  # Null hypothesis: no effect
  baseline_effect = 0.3, 
  outcome_type = "continuous", 
  threshold_success = 0.3, 
  threshold_futility = 0.1, 
  n_simulation = n_sims, 
  n_cores = n_cores, 
  priors_treatment = "normal(0, 100)", 
  priors_baseline = "normal(0, 100)", 
  priors_intercept = "normal(0, 100)", 
  priors_sigma = "normal(0, 100)"
)
#> Running 150 power simulations in parallel using 15 cores...
#> Setting up design model with true parameters...
#> Setting up design model with estimation priors...
#> Progress: 15/150 (10.0%) - Elapsed: 0.2 min - ETC: 1.5 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.3 min - ETC: 1.0 min
#> Progress: 45/150 (30.0%) - Elapsed: 0.3 min - ETC: 0.8 min
#> Progress: 60/150 (40.0%) - Elapsed: 0.4 min - ETC: 0.6 min
#> Progress: 75/150 (50.0%) - Elapsed: 0.5 min - ETC: 0.5 min
#> Progress: 90/150 (60.0%) - Elapsed: 0.6 min - ETC: 0.4 min
#> Progress: 105/150 (70.0%) - Elapsed: 0.7 min - ETC: 0.3 min
#> Progress: 120/150 (80.0%) - Elapsed: 0.8 min - ETC: 0.2 min
#> Progress: 135/150 (90.0%) - Elapsed: 0.9 min - ETC: 0.1 min
#> Completed: 150/150 (100%) - Total time: 1.0 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.009 
#> SD of effect estimate (median): 0.184 
#> Power - Success: 0 
#> Power - Futility: 0.7 
#> Mean probability of success: 0.128 
#> Mean probability of futility: 0.64

summary(alpha_uninformative)
#> 
#> === Bayesian Power Analysis Summary ===
#> 
#> Study Design:
#>   Control group size: 60 
#>   Treatment group size: 60 
#>   Target parameter: grouptreat 
#>   Success threshold: 0.3 
#>   Futility threshold: 0.1 
#>   Success probability threshold: 0.975 
#> 
#>   Futility probability threshold: 0.5 
#> 
#> Model Information:
#>   Family: gaussian(identity) 
#>   Estimation formula: baseline + group 
#>   Design formula: baseline + group 
#> 
#> Simulation Overview:
#>   Total simulations: 150 
#>   Successful fits: 150 
#>   Convergence rate: 100 %
#> 
#> Treatment Effect Estimates:
#>   Mean effect estimate (median): 0.009 
#>   SD of effect estimates (median): 0.184 
#> 
#> Power Analysis Results:
#>   Power - Success: 0 (MCSE: 0 )
#>   Power - Futility: 0.7 (MCSE: 0.0374 )
#> 
#> Decision Probabilities:
#>   Mean probability of success: 0.128 (MCSE: 0.0137 )
#>   Mean probability of futility: 0.64 (MCSE: 0.0223 )
#> 
#> === End Summary ===
```

``` r
# Alpha error rate with informative, sceptical priors
alpha_informative <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0,  # Null hypothesis: no effect
  baseline_effect = 0.3,
  outcome_type = "continuous",
  threshold_success = 0.3,
  threshold_futility = 0.1,
  n_simulation = n_sims,
  n_cores = n_cores,
  priors_treatment = "student_t(3, 0, 1)",
  priors_baseline = "student_t(3, 0, .5)",
  priors_intercept = "student_t(3, 0, 1)",
  priors_sigma = "student_t(3, 0, 2)"
)
#> Running 150 power simulations in parallel using 15 cores...
#> Setting up design model with true parameters...
#> Setting up design model with estimation priors...
#> Progress: 15/150 (10.0%) - Elapsed: 0.2 min - ETC: 1.5 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.3 min - ETC: 1.1 min
#> Progress: 45/150 (30.0%) - Elapsed: 0.4 min - ETC: 0.9 min
#> Progress: 60/150 (40.0%) - Elapsed: 0.5 min - ETC: 0.7 min
#> Progress: 75/150 (50.0%) - Elapsed: 0.6 min - ETC: 0.6 min
#> Progress: 90/150 (60.0%) - Elapsed: 0.7 min - ETC: 0.4 min
#> Progress: 105/150 (70.0%) - Elapsed: 0.8 min - ETC: 0.3 min
#> Progress: 120/150 (80.0%) - Elapsed: 0.9 min - ETC: 0.2 min
#> Progress: 135/150 (90.0%) - Elapsed: 1.0 min - ETC: 0.1 min
#> Completed: 150/150 (100%) - Total time: 1.1 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.002 
#> SD of effect estimate (median): 0.158 
#> Power - Success: 0 
#> Power - Futility: 0.733 
#> Mean probability of success: 0.105 
#> Mean probability of futility: 0.663

summary(alpha_informative)
#> 
#> === Bayesian Power Analysis Summary ===
#> 
#> Study Design:
#>   Control group size: 60 
#>   Treatment group size: 60 
#>   Target parameter: grouptreat 
#>   Success threshold: 0.3 
#>   Futility threshold: 0.1 
#>   Success probability threshold: 0.975 
#> 
#>   Futility probability threshold: 0.5 
#> 
#> Model Information:
#>   Family: gaussian(identity) 
#>   Estimation formula: baseline + group 
#>   Design formula: baseline + group 
#> 
#> Simulation Overview:
#>   Total simulations: 150 
#>   Successful fits: 150 
#>   Convergence rate: 100 %
#> 
#> Treatment Effect Estimates:
#>   Mean effect estimate (median): 0.002 
#>   SD of effect estimates (median): 0.158 
#> 
#> Power Analysis Results:
#>   Power - Success: 0 (MCSE: 0 )
#>   Power - Futility: 0.733 (MCSE: 0.0361 )
#> 
#> Decision Probabilities:
#>   Mean probability of success: 0.105 (MCSE: 0.011 )
#>   Mean probability of futility: 0.663 (MCSE: 0.0206 )
#> 
#> === End Summary ===
```

Comparing the alpha error rates:

``` r
# Compare alpha error rates
alpha_comparison <- data.frame(
  Prior_Type = c("Uninformative", "Sceptical"),
  Alpha_Error_Rate = c(
    alpha_uninformative$power_success,
    alpha_informative$power_success
  ),
  Probabilty_False_Positive = c(
    alpha_uninformative$mean_prob_success,
    alpha_informative$mean_prob_success
  )
) |> 
  dplyr::mutate(
    across(c(Alpha_Error_Rate, Probabilty_False_Positive), ~ round(.x, 3))
  )

print(alpha_comparison)
#>      Prior_Type Alpha_Error_Rate Probabilty_False_Positive
#> 1 Uninformative                0                     0.128
#> 2     Sceptical                0                     0.105
```

Using a sceptical prior reduces the probability of falsely concluding
success when there is no true effect, thus controlling the alpha error
rate more effectively than the uninformative priors.

------------------------------------------------------------------------

# Example 2: Design Prior vs No Design Prior

Design priors integrate uncertainty about the true effect size. Here we
compare integrated power (with design prior) versus point estimates
(without design prior).

``` r
fit_design_prior <- power_grid_analysis(
  sample_sizes = c(160),
effect_sizes = seq(0.2, 0.8, 0.1),
  design_prior = "normal(0.5, 0.1)",  # Informed design prior
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.3,
  threshold_success = 0.3,
  threshold_futility = 0.1,
  n_simulations = n_sims,
  n_cores = n_cores
)
#> Successfully parsed design prior: normal(0.5, 0.1) 
#>   Distribution: normal 
#>   Density function: stats::dnorm(0.5, 0.1) (using stats package)
#>   Quantile function: stats::qnorm(0.5, 0.1) (using stats package)
#> 
#> === Effect Size Analysis ===
#> Fixed sample size: 160 
#> Effect sizes to test: 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8 
#> Threshold - Success: 0.3 
#> Threshold - Futility: 0.1 
#> Allocation (treatment %): 50 %
#> Power analysis function: power_analysis_ancova 
#> Design prior: normal(0.5, 0.1) 
#> Total combinations: 7 
#> 
#> 
#> === Phase 1: Parallel Model Compilation ===
#> Compiling models for 7 unique effect sizes in parallel using 15 cores...
#> [SUCCESS] Successfully compiled models for effect size 0.2 
#> [SUCCESS] Successfully compiled models for effect size 0.3 
#> [SUCCESS] Successfully compiled models for effect size 0.4 
#> [SUCCESS] Successfully compiled models for effect size 0.5 
#> [SUCCESS] Successfully compiled models for effect size 0.6 
#> [SUCCESS] Successfully compiled models for effect size 0.7 
#> [SUCCESS] Successfully compiled models for effect size 0.8 
#> 
#> === Phase 2: Grid Analysis with Cached Models ===
#> 
#> --- Processing Effect Size 0.2 ( 1 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 1 of 7 : N = 160 , Effect = 0.2 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 4.0 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.5 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.9 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.5 min
#> Progress: 75/150 (50.0%) - Elapsed: 2.0 min - ETC: 2.0 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.7 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 3.1 min - ETC: 0.8 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.4 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.8 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.193 
#> SD of effect estimate (median): 0.162 
#> Power - Success: 0.007 
#> Power - Futility: 0.28 
#> Mean probability of success: 0.319 
#> Mean probability of futility: 0.337 
#> 
#> 
#> --- Processing Effect Size 0.3 ( 2 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 2 of 7 : N = 160 , Effect = 0.3 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 3.7 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.4 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.8 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 1.9 min - ETC: 1.9 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 3.0 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.3 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.6 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.276 
#> SD of effect estimate (median): 0.161 
#> Power - Success: 0.033 
#> Power - Futility: 0.113 
#> Mean probability of success: 0.481 
#> Mean probability of futility: 0.196 
#> 
#> 
#> --- Processing Effect Size 0.4 ( 3 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 3 of 7 : N = 160 , Effect = 0.4 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 3.6 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.8 min - ETC: 3.4 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.7 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.3 min
#> Progress: 75/150 (50.0%) - Elapsed: 1.9 min - ETC: 1.9 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.2 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 2.9 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.2 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.6 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.385 
#> SD of effect estimate (median): 0.165 
#> Power - Success: 0.087 
#> Power - Futility: 0.053 
#> Mean probability of success: 0.649 
#> Mean probability of futility: 0.109 
#> 
#> 
#> --- Processing Effect Size 0.5 ( 4 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 4 of 7 : N = 160 , Effect = 0.5 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 3.9 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.5 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.8 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 2.0 min - ETC: 2.0 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 3.0 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.3 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.6 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.483 
#> SD of effect estimate (median): 0.168 
#> Power - Success: 0.253 
#> Power - Futility: 0.013 
#> Mean probability of success: 0.792 
#> Mean probability of futility: 0.045 
#> 
#> 
#> --- Processing Effect Size 0.6 ( 5 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 5 of 7 : N = 160 , Effect = 0.6 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 4.0 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.6 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.9 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.5 min
#> Progress: 75/150 (50.0%) - Elapsed: 2.0 min - ETC: 2.0 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.6 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 3.0 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.3 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.7 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.629 
#> SD of effect estimate (median): 0.168 
#> Power - Success: 0.527 
#> Power - Futility: 0 
#> Mean probability of success: 0.913 
#> Mean probability of futility: 0.012 
#> 
#> 
#> --- Processing Effect Size 0.7 ( 6 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 6 of 7 : N = 160 , Effect = 0.7 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 3.6 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.5 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.8 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 1.9 min - ETC: 1.9 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 2.9 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.2 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.6 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.707 
#> SD of effect estimate (median): 0.156 
#> Power - Success: 0.7 
#> Power - Futility: 0 
#> Mean probability of success: 0.959 
#> Mean probability of futility: 0.005 
#> 
#> 
#> --- Processing Effect Size 0.8 ( 7 of 7 ) ---
#> Combinations for this effect size: 1 
#> Using pre-compiled models from Phase 1 cache
#> Testing combination 7 of 7 : N = 160 , Effect = 0.8 (using cached models)
#> Using pre-fitted design models...
#> Running 150 power simulations in parallel using 15 cores...
#> Progress: 15/150 (10.0%) - Elapsed: 0.4 min - ETC: 3.9 min
#> Progress: 30/150 (20.0%) - Elapsed: 0.9 min - ETC: 3.5 min
#> Progress: 45/150 (30.0%) - Elapsed: 1.2 min - ETC: 2.8 min
#> Progress: 60/150 (40.0%) - Elapsed: 1.6 min - ETC: 2.4 min
#> Progress: 75/150 (50.0%) - Elapsed: 2.0 min - ETC: 2.0 min
#> Progress: 90/150 (60.0%) - Elapsed: 2.3 min - ETC: 1.5 min
#> Progress: 105/150 (70.0%) - Elapsed: 2.6 min - ETC: 1.1 min
#> Progress: 120/150 (80.0%) - Elapsed: 3.0 min - ETC: 0.7 min
#> Progress: 135/150 (90.0%) - Elapsed: 3.3 min - ETC: 0.4 min
#> Completed: 150/150 (100%) - Total time: 3.7 min
#> 
#> Power Analysis Complete!
#> Successful fits: 150 out of 150 
#> Mean effect estimate (median): 0.78 
#> SD of effect estimate (median): 0.163 
#> Power - Success: 0.833 
#> Power - Futility: 0 
#> Mean probability of success: 0.986 
#> Mean probability of futility: 0.001 
#> 
#> 
#> Total analysis time: 40.26 minutes
#> 
#> Computing integrated power using design prior...
#> 
#> === Effect Size Analysis Complete ===
#> Target success power not achieved with tested effect sizes
#> Target futility power not achieved with tested effect sizes
#> 
#> Integrated power results:
#>   Target integrated success power >= 0.9 not achieved with tested sample sizes
#>   Target integrated futility power >= 0.95 not achieved with tested sample sizes

summary(fit_design_prior)
#> Bayesian RCT Effect Size Analysis - Detailed Summary
#> ====================================================
#> 
#> Analysis Parameters:
#>   Target power - Success: 0.9 
#>   Target power - Futility: 0.95 
#>   Threshold - Success: 0.3 
#>   Threshold - Futility: 0.1 
#>   Fixed sample size: 160 
#>   Effect sizes tested: 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8 
#>   Allocation (treatment %): 50%
#>   Power analysis function: power_analysis_ancova 
#>   Design prior: normal(0.5, 0.1) 
#>   Design prior type: brms 
#>   Analysis time: 40.26 minutes
#> 
#> Power Results Across Effect Sizes:
#> ==================================
#>  Effect_Size Convergence   Power_Success    Prob_Success  Power_Futility
#>          0.2        100%  0.7% (+/-0.66) 31.9% (+/-2.15)   28% (+/-3.67)
#>          0.3        100%  3.3% (+/-1.47) 48.1% (+/-2.31) 11.3% (+/-2.59)
#>          0.4        100%   8.7% (+/-2.3) 64.9% (+/-2.27)  5.3% (+/-1.83)
#>          0.5        100% 25.3% (+/-3.55) 79.2% (+/-1.83)  1.3% (+/-0.94)
#>          0.6        100% 52.7% (+/-4.08) 91.3% (+/-1.13)       0% (+/-0)
#>          0.7        100%   70% (+/-3.74) 95.9% (+/-0.76)       0% (+/-0)
#>          0.8        100% 83.3% (+/-3.04) 98.6% (+/-0.25)       0% (+/-0)
#>    Prob_Futility
#>  33.7% (+/-2.24)
#>  19.6% (+/-1.73)
#>  10.9% (+/-1.39)
#>   4.5% (+/-0.73)
#>   1.2% (+/-0.27)
#>   0.5% (+/-0.19)
#>   0.1% (+/-0.02)
#> 
#> Optimal Combinations:
#> ====================
#> No combinations achieved target success power.
#> 
#> No combinations achieved target futility power.
#> 
#> Integrated Power & Probability Results:
#> ======================================
#> (Using design prior from main analysis: normal(0.5, 0.1) )
#> 
#>  N_total   Power_Success    Prob_Success Power_Futility  Prob_Futility
#>      160 29.3% (+/-3.72) 77.8% (+/-1.04) 2.6% (+/-1.29) 5.9% (+/-0.43)
#> 
#> Integrated results represent weighted averages across effect sizes using the specified design prior.
#> Power = probability of making correct decision, Mean Probability = mean posterior probability of exceeding threshold.
#> Values shown as percentage (+/-MCSE) where MCSE = Monte Carlo Standard Error.
```

Now let’s demonstrate the flexibility of specifying different design
priors in the summary method:

Summary with design prior specified in fit:

``` r
summ_used$integrated_power |>
  dplyr::mutate(across(where( ~ is.numeric(.)), ~ round(.x, 3)))
#>   n_total integrated_power_success integrated_power_futility
#> 1     160                    0.293                     0.026
#>   integrated_prob_success integrated_prob_futility
#> 1                   0.778                    0.059
#>   mcse_integrated_power_success mcse_integrated_power_futility
#> 1                         0.037                          0.013
#>   mcse_integrated_prob_success mcse_integrated_prob_futility
#> 1                         0.01                         0.004
```

Summary with optimistic design prior:

``` r
summ_optimistic$integrated_power |>
  dplyr::mutate(across(where( ~ is.numeric(.)), ~ round(.x, 3)))
#>   n_total integrated_power_success integrated_power_futility
#> 1     160                    0.293                     0.026
#>   integrated_prob_success integrated_prob_futility
#> 1                   0.778                    0.059
#>   mcse_integrated_power_success mcse_integrated_power_futility
#> 1                         0.037                          0.013
#>   mcse_integrated_prob_success mcse_integrated_prob_futility
#> 1                         0.01                         0.004
```

Summary with conservative design prior:

``` r
summ_conservative$integrated_power |>
  dplyr::mutate(across(where( ~ is.numeric(.)), ~ round(.x, 3)))
#>   n_total integrated_power_success integrated_power_futility
#> 1     160                    0.293                     0.026
#>   integrated_prob_success integrated_prob_futility
#> 1                   0.778                    0.059
#>   mcse_integrated_power_success mcse_integrated_power_futility
#> 1                         0.037                          0.013
#>   mcse_integrated_prob_success mcse_integrated_prob_futility
#> 1                         0.01                         0.004
```

Summary with wide design prior:

``` r
summ_wide$integrated_power |>
  dplyr::mutate(across(where( ~ is.numeric(.)), ~ round(.x, 3)))
#>   n_total integrated_power_success integrated_power_futility
#> 1     160                    0.293                     0.026
#>   integrated_prob_success integrated_prob_futility
#> 1                   0.778                    0.059
#>   mcse_integrated_power_success mcse_integrated_power_futility
#> 1                         0.037                          0.013
#>   mcse_integrated_prob_success mcse_integrated_prob_futility
#> 1                         0.01                         0.004
```

This demonstrates the enhanced flexibility where you can: 1. Run the
analysis once to get raw results 2. Try different design priors at
summary time 3. Compare how different assumptions about effect size
uncertainty affect integrated power

------------------------------------------------------------------------

``` r
sessionInfo()
#> R version 4.5.0 (2025-04-11 ucrt)
#> Platform: x86_64-w64-mingw32/x64
#> Running under: Windows 11 x64 (build 26100)
#> 
#> Matrix products: default
#>   LAPACK version 3.12.1
#> 
#> locale:
#> [1] LC_COLLATE=German_Germany.utf8  LC_CTYPE=German_Germany.utf8   
#> [3] LC_MONETARY=German_Germany.utf8 LC_NUMERIC=C                   
#> [5] LC_TIME=German_Germany.utf8    
#> 
#> time zone: Europe/Berlin
#> tzcode source: internal
#> 
#> attached base packages:
#> [1] stats     graphics  grDevices utils     datasets  methods   base     
#> 
#> other attached packages:
#> [1] rctbayespower_0.1.0 testthat_3.2.3      here_1.0.1         
#> [4] rmarkdown_2.29      devtools_2.4.5      usethis_3.1.0      
#> 
#> loaded via a namespace (and not attached):
#>  [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        
#>  [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    
#>  [7] promises_1.3.3       digest_0.6.37        mime_0.13           
#> [10] lifecycle_1.0.4      StanHeaders_2.32.10  ellipsis_0.3.2      
#> [13] processx_3.8.6       magrittr_2.0.3       posterior_1.6.1     
#> [16] compiler_4.5.0       rlang_1.1.6          tools_4.5.0         
#> [19] yaml_2.3.10          knitr_1.50           bridgesampling_1.1-2
#> [22] htmlwidgets_1.6.4    pkgbuild_1.4.8       RColorBrewer_1.1-3  
#> [25] pkgload_1.4.0        abind_1.4-8          miniUI_0.1.2        
#> [28] withr_3.0.2          purrr_1.0.4          desc_1.4.3          
#> [31] grid_4.5.0           stats4_4.5.0         urlchecker_1.0.1    
#> [34] profvis_0.4.0        xtable_1.8-4         inline_0.3.21       
#> [37] ggplot2_3.5.2        scales_1.4.0         cli_3.6.5           
#> [40] mvtnorm_1.3-3        generics_0.1.4       remotes_2.5.0       
#> [43] RcppParallel_5.1.10  rstudioapi_0.17.1    sessioninfo_1.2.3   
#> [46] cachem_1.1.0         rstan_2.32.7         stringr_1.5.1       
#> [49] bayesplot_1.13.0     parallel_4.5.0       matrixStats_1.5.0   
#> [52] brms_2.22.0          vctrs_0.6.5          Matrix_1.7-3        
#> [55] callr_3.7.6          tidyr_1.3.1          glue_1.8.0          
#> [58] codetools_0.2-20     ps_1.9.1             distributional_0.5.0
#> [61] stringi_1.8.7        gtable_0.3.6         later_1.4.2         
#> [64] QuickJSR_1.8.0       tibble_3.3.0         pillar_1.10.2       
#> [67] htmltools_0.5.8.1    Brobdingnag_1.2-9    brio_1.1.5          
#> [70] R6_2.6.1             rprojroot_2.0.4      evaluate_1.0.4      
#> [73] shiny_1.11.0         lattice_0.22-7       backports_1.5.0     
#> [76] memoise_2.0.1        httpuv_1.6.16        rstantools_2.4.0    
#> [79] Rcpp_1.0.14          coda_0.19-4.1        gridExtra_2.3       
#> [82] nlme_3.1-168         checkmate_2.3.2      xfun_0.52           
#> [85] fs_1.6.6             pkgconfig_2.0.3
```
