---
title: "Case Studies: Real-World Applications of Bayesian Power Analysis"
author: "rctbayespower package authors"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Case Studies: Real-World Applications of Bayesian Power Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)

# Demo settings
n_sims_demo <- 6 # Minimal simulations for fast demo builds
n_cores_demo <- 1 # Single core for vignette building
```

# Real-World Case Studies

This vignette presents real-world case studies demonstrating how to apply Bayesian power analysis using the `rctbayespower` package across different clinical domains, with actual model fitting.

```{r load_libs, echo=FALSE}
library(rctbayespower)
library(ggplot2)
library(dplyr)
```

# Case Study 1: Cardiovascular Clinical Trial

## Background

A pharmaceutical company wants to test a new medication for reducing LDL cholesterol. Based on previous studies:

- Current standard treatment reduces LDL by 30 mg/dL on average
- New treatment is expected to reduce LDL by an additional 15 mg/dL
- Standard deviation of LDL change is approximately 40 mg/dL
- Clinically meaningful difference is 10 mg/dL

## Study Design

```{r cardio_setup}
# Convert to standardized effect size
baseline_sd <- 40
additional_reduction <- 15
effect_size <- additional_reduction / baseline_sd # Cohen's d = 0.375

# Clinically meaningful difference
clinical_threshold <- 10 / baseline_sd # 0.25

cat("Expected effect size (Cohen's d):", round(effect_size, 3), "\n")
cat("Clinical threshold:", round(clinical_threshold, 3), "\n")
```

## Power Analysis

```{r cardio_power}
# Initial power analysis
cardio_power <- power_analysis_ancova(
  n_control = 100,
  n_treatment = 100,
  effect_size = 0.375,
  baseline_effect = 0.5, # Strong baseline effect (LDL at screening)
  outcome_type = "continuous",
  threshold_success = 0.25, # Based on clinical threshold
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

print(cardio_power)
```

## Sample Size Determination

```{r cardio_sample_size}
# Find optimal sample size for regulatory submission
cardio_sample_size <- power_grid_analysis(
  sample_sizes = c(80, 120, 160, 200),
  effect_sizes = 0.375, # Fixed effect size
  target_power_success = 0.85, # Higher power for regulatory submission
  target_power_futility = 0.8,
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.5,
  threshold_success = 0.25,
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo
)

print(cardio_sample_size)
```

```{r cardio_plot}
# Plot sample size curve
plot(cardio_sample_size, type = "power_curve")
```

## Incorporating Prior Information

```{r cardio_prior}
# Meta-analysis of similar drugs with design prior
cardio_informed <- power_grid_analysis(
  sample_sizes = 120,
  effect_sizes = seq(0.2, 0.6, by = 0.05),
  design_prior = "normal(0.375, 0.1)", # Informed by meta-analysis
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.5,
  threshold_success = 0.25,
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo
)

cat("Integrated power with informed prior:", round(cardio_informed$integrated_power_success, 3), "\n")
```

```{r cardio_prior_plot}
plot(cardio_informed, type = "power_curve", show_integrated = TRUE)
```

# Case Study 2: Mental Health Intervention

## Background

A clinical psychologist wants to test a new cognitive behavioral therapy (CBT) protocol for depression. The study will use:

- Primary outcome: Change in Beck Depression Inventory (BDI-II) score
- Expected treatment effect: 5-point reduction (scale 0-63)
- Standard deviation: 12 points
- Clinically meaningful difference: 3 points
- Include baseline severity and age as covariates

## Study Design

```{r mental_health_setup}
# Effect size calculation
baseline_sd <- 12
expected_reduction <- 5
effect_size <- expected_reduction / baseline_sd # Cohen's d ≈ 0.42

# Clinical threshold
clinical_threshold <- 3 / baseline_sd # 0.25

cat("Expected effect size:", round(effect_size, 3), "\n")
cat("Clinical threshold:", round(clinical_threshold, 3), "\n")
```

## Power Analysis with Strong Baseline Effects

Mental health studies typically have strong baseline covariate effects:

```{r mental_health_power}
# Power analysis with strong baseline effect
depression_power <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.42,
  baseline_effect = 0.7, # Strong baseline severity effect
  outcome_type = "continuous",
  threshold_success = 0.25, # Clinically meaningful threshold
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

# Compare with analysis without strong baseline effect
depression_power_weak <- power_analysis_ancova(
  n_control = 60,
  n_treatment = 60,
  effect_size = 0.42,
  baseline_effect = 0.2, # Weak baseline effect
  outcome_type = "continuous",
  threshold_success = 0.25,
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

cat("Power with strong baseline effect:", round(depression_power$power_success, 3), "\n")
cat("Power with weak baseline effect:", round(depression_power_weak$power_success, 3), "\n")
```

## Sensitivity to Effect Size

```{r mental_health_sensitivity}
# Test sensitivity to different effect sizes
depression_curve <- power_grid_analysis(
  sample_sizes = 60,
  effect_sizes = seq(0.2, 0.7, by = 0.1),
  design_prior = "normal(0.42, 0.1)", # Prior based on pilot study
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.7,
  threshold_success = 0.25,
  threshold_futility = 0.1,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo
)

print(depression_curve)
```

```{r mental_health_plot}
plot(depression_curve, type = "power_curve", show_integrated = TRUE)
```

# Case Study 3: Rare Disease Trial

## Background

A biotechnology company is developing a treatment for a rare genetic disorder affecting approximately 1 in 50,000 people. Challenges include:

- Very small potential sample size (n ≈ 30 total)
- High variability in outcomes
- Strong biological rationale for treatment
- Binary outcome: treatment response (yes/no)
- Historical response rate: 15%
- Expected response rate with treatment: 40%

## Design Considerations

```{r rare_disease_setup}
# Convert to log odds ratio
baseline_prob <- 0.15
treatment_prob <- 0.40

baseline_odds <- baseline_prob / (1 - baseline_prob)
treatment_odds <- treatment_prob / (1 - treatment_prob)
log_odds_ratio <- log(treatment_odds / baseline_odds)

cat("Baseline response rate:", baseline_prob, "\n")
cat("Expected treatment response rate:", treatment_prob, "\n")
cat("Log odds ratio:", round(log_odds_ratio, 3), "\n")
```

## Power Analysis for Small Sample

```{r rare_disease_power}
# Power analysis with very small sample
rare_disease_power <- power_analysis_ancova(
  n_control = 15,
  n_treatment = 15,
  effect_size = log_odds_ratio, # Log odds ratio
  baseline_effect = 0.2, # Some baseline predictors
  outcome_type = "binary",
  baseline_prob = 0.15,
  threshold_success = 0.8, # Lower threshold given small sample
  threshold_futility = 0.2,
  p_sig_success = 0.9, # Lower probability threshold
  p_sig_futility = 0.9,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

print(rare_disease_power)
```

## Effect of Different Sample Sizes

```{r rare_disease_alternatives}
# Explore different allocation strategies
rare_disease_sample_sizes <- power_grid_analysis(
  sample_sizes = c(15, 20, 25, 30),
  effect_sizes = log_odds_ratio,
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "binary",
  baseline_prob = 0.15,
  baseline_effect = 0.2,
  threshold_success = 0.8,
  threshold_futility = 0.2,
  p_sig_success = 0.9,
  p_sig_futility = 0.9,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

print(rare_disease_sample_sizes)
```

```{r rare_disease_plot}
plot(rare_disease_sample_sizes, type = "power_curve")
```

# Case Study 4: Public Health Intervention

## Background

A public health intervention to reduce hospital readmissions is being tested:

- Primary outcome: 30-day readmission rate (binary)
- Expected baseline rate: 12%
- Expected reduction: 3 percentage points (to 9%)
- Large sample available (hospital-level implementation)
- Strong administrative predictors available

## Design Effect Calculation

```{r public_health_setup}
# Calculate effect size
baseline_rate <- 0.12
treatment_rate <- 0.09 # 3 percentage point reduction

# Convert to log odds ratio
baseline_odds <- baseline_rate / (1 - baseline_rate)
treatment_odds <- treatment_rate / (1 - treatment_rate)
public_health_log_or <- log(treatment_odds / baseline_odds)

cat("Baseline readmission rate:", baseline_rate, "\n")
cat("Expected treatment rate:", treatment_rate, "\n")
cat("Log odds ratio:", round(public_health_log_or, 3), "\n")
```

## Power Analysis

```{r public_health_power}
# Power analysis for public health intervention
public_health_power <- power_analysis_ancova(
  n_control = 1000, # Large sample from administrative data
  n_treatment = 1000,
  effect_size = public_health_log_or,
  baseline_effect = 0.4, # Strong administrative predictors
  outcome_type = "binary",
  baseline_prob = baseline_rate,
  threshold_success = abs(public_health_log_or) * 0.5, # 50% of expected effect
  threshold_futility = 0.05,
  p_sig_success = 0.95,
  p_sig_futility = 0.5,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

print(public_health_power)
```

## Cost-Effectiveness Analysis

```{r public_health_cost}
# Analyze cost-effectiveness for different sample sizes
public_health_cost_analysis <- function(sample_sizes, cost_per_participant = 50, fixed_costs = 100000) {
  cost_results <- data.frame(
    Sample_Size = numeric(),
    Total_Cost = numeric(),
    Power_Success = numeric(),
    Cost_Per_Power_Unit = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (n in sample_sizes) {
    power_result <- power_analysis_ancova(
      n_control = n,
      n_treatment = n,
      effect_size = public_health_log_or,
      baseline_effect = 0.4,
      outcome_type = "binary",
      baseline_prob = baseline_rate,
      threshold_success = abs(public_health_log_or) * 0.5,
      threshold_futility = 0.05,
      n_simulations = n_sims_demo,
      n_cores = n_cores_demo,
      brms_args = list(
        algorithm = "meanfield",
        importance_resampling = TRUE,
        iter = 1e4,
        output_samples = 1e3
      )
    )
    
    total_cost <- fixed_costs + (2 * n * cost_per_participant)
    cost_per_power <- total_cost / power_result$power_success
    
    cost_results <- rbind(cost_results, data.frame(
      Sample_Size = n,
      Total_Cost = total_cost,
      Power_Success = power_result$power_success,
      Cost_Per_Power_Unit = cost_per_power
    ))
  }
  
  return(cost_results)
}

# Analyze cost-effectiveness
ph_cost_analysis <- public_health_cost_analysis(
  sample_sizes = c(500, 750, 1000, 1250),
  cost_per_participant = 50,
  fixed_costs = 100000
)

print(ph_cost_analysis)

# Find most cost-effective design
optimal_ph_design <- ph_cost_analysis[which.min(ph_cost_analysis$Cost_Per_Power_Unit), ]
cat("Most cost-effective design:\n")
cat("Sample size per group:", optimal_ph_design$Sample_Size, "\n")
cat("Total cost: $", format(optimal_ph_design$Total_Cost, big.mark = ","), "\n")
cat("Power:", round(optimal_ph_design$Power_Success, 3), "\n")
```

# Case Study 5: Dose-Response Study

## Background

A pharmaceutical company wants to test multiple doses of a new drug:

- Primary outcome: Continuous efficacy measure
- Three dose levels plus placebo
- Expected dose-response relationship
- Need to determine optimal dose

## Dose-Response Design

```{r dose_response_setup}
# Define dose levels and expected effects
doses <- c(0, 25, 50, 100) # mg/day
expected_effects <- c(0, 0.2, 0.4, 0.5) # Standardized effect sizes

dose_response_data <- data.frame(
  Dose = doses,
  Expected_Effect = expected_effects
)

print(dose_response_data)
```

## Power Analysis for Each Dose Comparison

```{r dose_response_power}
# Analyze power for each dose vs placebo
dose_power_results <- data.frame(
  Dose = numeric(),
  Effect_Size = numeric(),
  Power_Success = numeric(),
  Power_Futility = numeric(),
  stringsAsFactors = FALSE
)

for (i in 2:length(doses)) { # Skip placebo (dose 1)
  dose_power <- power_analysis_ancova(
    n_control = 80, # Placebo group
    n_treatment = 80, # Each dose group
    effect_size = expected_effects[i],
    baseline_effect = 0.3,
    outcome_type = "continuous",
    threshold_success = 0.25, # Clinically meaningful threshold
    threshold_futility = 0.1,
    p_sig_success = 0.95,
    p_sig_futility = 0.5,
    n_simulations = n_sims_demo,
    n_cores = n_cores_demo,
    brms_args = list(
      algorithm = "meanfield",
      importance_resampling = TRUE,
      iter = 1e4,
      output_samples = 1e3
    )
  )
  
  dose_power_results <- rbind(dose_power_results, data.frame(
    Dose = doses[i],
    Effect_Size = expected_effects[i],
    Power_Success = dose_power$power_success,
    Power_Futility = dose_power$power_futility
  ))
}

print(dose_power_results)
```

## Optimal Dose Selection

```{r dose_response_optimal}
# Grid analysis for dose optimization
dose_grid <- power_grid_analysis(
  sample_sizes = c(60, 80, 100),
  effect_sizes = expected_effects[2:4], # Exclude placebo
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.3,
  threshold_success = 0.25,
  threshold_futility = 0.1,
  target_power_success = 0.8,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo
)

print(dose_grid)
```

```{r dose_response_plot}
plot(dose_grid, type = "heatmap")
```

# Case Study 6: Non-Inferiority Trial

## Background

A generic drug manufacturer wants to demonstrate non-inferiority of their product compared to the brand-name drug:

- Primary outcome: Continuous efficacy measure
- Non-inferiority margin: -0.3 (on standardized scale)
- Expected true difference: 0 (identical efficacy)
- Regulatory requirements for non-inferiority

## Non-Inferiority Power Analysis

```{r non_inferiority_setup}
# Non-inferiority margin and expected difference
ni_margin <- -0.3
true_difference <- 0 # Assume identical efficacy

cat("Non-inferiority margin:", ni_margin, "\n")
cat("Expected true difference:", true_difference, "\n")
```

```{r non_inferiority_power}
# For non-inferiority, we test that treatment is not worse than margin
# This requires different threshold interpretation
ni_power <- power_analysis_ancova(
  n_control = 150,
  n_treatment = 150,
  effect_size = true_difference, # Null effect
  baseline_effect = 0.4,
  outcome_type = "continuous",
  threshold_success = abs(ni_margin), # Success = effect > margin
  threshold_futility = ni_margin, # Different interpretation for NI
  p_sig_success = 0.975, # One-sided test at 2.5% level
  p_sig_futility = 0.025,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo,
  brms_args = list(
    algorithm = "meanfield",
    importance_resampling = TRUE,
    iter = 1e4,
    output_samples = 1e3
  )
)

print(ni_power)
```

## Sample Size for Non-Inferiority

```{r non_inferiority_sample_size}
# Find adequate sample size for non-inferiority
ni_sample_sizes <- power_grid_analysis(
  sample_sizes = c(100, 150, 200, 250),
  effect_sizes = true_difference,
  target_power_success = 0.8, # 80% power to show non-inferiority
  power_analysis_fn = "power_analysis_ancova",
  outcome_type = "continuous",
  baseline_effect = 0.4,
  threshold_success = abs(ni_margin),
  threshold_futility = ni_margin,
  p_sig_success = 0.975,
  p_sig_futility = 0.025,
  n_simulations = n_sims_demo,
  n_cores = n_cores_demo
)

print(ni_sample_sizes)
```

```{r non_inferiority_plot}
plot(ni_sample_sizes, type = "power_curve")
```

# Lessons Learned

## Key Insights from Case Studies

### 1. Cardiovascular Trial
- **Regulatory considerations**: Higher power targets (85%) often required
- **Strong baseline effects**: Covariate adjustment substantially improves power
- **Design priors**: Meta-analysis data provides valuable prior information

### 2. Mental Health Intervention
- **Baseline severity**: Strong predictor effects can dramatically improve precision
- **Effect size uncertainty**: Design priors help account for uncertainty
- **Clinical thresholds**: Domain-specific meaningful effect sizes crucial

### 3. Rare Disease Trial
- **Small samples**: Lower probability thresholds may be necessary
- **High effect sizes**: Biological rationale supports larger expected effects
- **Flexible thresholds**: Adapt success criteria to realistic expectations

### 4. Public Health Intervention
- **Large samples**: Administrative data enables large, powerful studies
- **Cost considerations**: Low per-participant costs favor larger samples
- **Population impact**: Small effect sizes can have large public health benefits

### 5. Dose-Response Study
- **Multiple comparisons**: Plan for testing multiple dose levels
- **Dose optimization**: Grid analysis helps identify optimal doses
- **Resource allocation**: Balance sample size across dose groups

### 6. Non-Inferiority Trial
- **Different framework**: Success/futility thresholds have different meanings
- **Regulatory standards**: Higher statistical standards typically required
- **Conservative planning**: Account for regulatory scrutiny

## Algorithm Performance Insights

```{r algorithm_insights}
# Compare algorithm performance across different study types
algorithm_comparison <- data.frame(
  Study_Type = character(),
  Sample_Size = numeric(),
  Outcome_Type = character(),
  Meanfield_Time = numeric(),
  Fullrank_Time = numeric(),
  stringsAsFactors = FALSE
)

# Example timing comparison for different study types
study_configs <- list(
  "Cardiovascular" = list(n = 100, outcome = "continuous"),
  "Mental Health" = list(n = 60, outcome = "continuous"),
  "Rare Disease" = list(n = 15, outcome = "binary"),
  "Public Health" = list(n = 1000, outcome = "binary")
)

for (study_name in names(study_configs)) {
  config <- study_configs[[study_name]]
  
  # Time meanfield
  start_time <- Sys.time()
  power_analysis_ancova(
    n_control = config$n,
    n_treatment = config$n,
    effect_size = 0.4,
    baseline_effect = 0.3,
    outcome_type = config$outcome,
    threshold_success = 0.3,
    threshold_futility = 0.1,
    n_simulations = 3, # Minimal for timing
    n_cores = 1,
    brms_args = list(algorithm = "meanfield", importance_resampling = TRUE, iter = 1e4, output_samples = 1e3)
  )
  meanfield_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  # Time fullrank
  start_time <- Sys.time()
  power_analysis_ancova(
    n_control = config$n,
    n_treatment = config$n,
    effect_size = 0.4,
    baseline_effect = 0.3,
    outcome_type = config$outcome,
    threshold_success = 0.3,
    threshold_futility = 0.1,
    n_simulations = 3, # Minimal for timing
    n_cores = 1,
    brms_args = list(algorithm = "fullrank", importance_resampling = TRUE, iter = 1e4, output_samples = 1e3)
  )
  fullrank_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  algorithm_comparison <- rbind(algorithm_comparison, data.frame(
    Study_Type = study_name,
    Sample_Size = config$n,
    Outcome_Type = config$outcome,
    Meanfield_Time = meanfield_time,
    Fullrank_Time = fullrank_time
  ))
}

print(algorithm_comparison)
```

## General Recommendations

### Study Design
1. **Domain-specific considerations**: Each therapeutic area has unique challenges and requirements
2. **Baseline covariates**: Always consider including strong predictors to improve precision
3. **Effect size justification**: Base effect sizes on systematic reviews or meta-analyses when possible
4. **Clinical thresholds**: Involve domain experts in defining meaningful effect sizes

### Prior Information
1. **Design priors**: Use when scientifically justified and document sources clearly
2. **Sensitivity analysis**: Test robustness to different prior specifications
3. **Conservative planning**: Build in safety margins for regulatory submissions
4. **Prior elicitation**: Formal methods can help when expert opinion varies

### Implementation
1. **Algorithm selection**: Start with fast algorithms for exploration, confirm with more accurate methods
2. **Model caching**: Leverage caching for efficient grid analyses
3. **Convergence monitoring**: Always check convergence rates and effective sample sizes
4. **Computational resources**: Plan computing time and resources appropriately

### Decision Making
1. **Multiple metrics**: Consider both success and futility power
2. **Cost-effectiveness**: Integrate economic considerations into design decisions
3. **Stakeholder engagement**: Involve clinicians, regulators, and other stakeholders throughout
4. **Adaptive planning**: Design studies that can adapt based on interim results when appropriate

# Conclusion

These case studies demonstrate the flexibility and power of Bayesian approaches to RCT design across diverse clinical contexts. Key advantages include:

- **Context-specific modeling**: Ability to incorporate domain knowledge and constraints
- **Flexible decision criteria**: Success and futility thresholds tailored to study goals
- **Uncertainty quantification**: Explicit modeling of uncertainty about effect sizes
- **Resource optimization**: Integration of cost and other practical considerations
- **Regulatory acceptance**: Growing acceptance of Bayesian methods in regulatory contexts

The `rctbayespower` package provides the computational infrastructure needed to implement these sophisticated approaches efficiently in practice, enabling researchers to design more informative and efficient clinical trials.

# Session Information

```{r session_info}
sessionInfo()
```