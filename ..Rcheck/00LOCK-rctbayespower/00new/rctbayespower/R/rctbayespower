.packageName <- "rctbayespower"
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/MCSE.R"
#' Monte Carlo Standard Error (MCSE) Calculation
#'
#' Calculate Monte Carlo Standard Error for power metrics based on simulation results.
#' MCSE provides an estimate of the uncertainty in power estimates due to Monte Carlo sampling.
#'
#' @param successes Vector of success indicators (TRUE/FALSE or 1/0)
#' @param n_sims Total number of simulations
#' @return Monte Carlo Standard Error
#' @importFrom stats sd
#' @keywords internal
calculate_mcse_power <- function(successes, n_sims) {
  if (length(successes) == 0 || n_sims == 0) {
    return(NA_real_)
  }

  # Convert to numeric if needed
  if (is.logical(successes)) {
    successes <- as.numeric(successes)
  }

  # Calculate proportion
  p <- mean(successes, na.rm = TRUE)

  # MCSE for proportion = sqrt(p * (1 - p) / n)
  mcse <- sqrt(p * (1 - p) / n_sims)

  return(mcse)
}

#' Monte Carlo Standard Error for Continuous Metrics
#'
#' Calculate Monte Carlo Standard Error for continuous metrics like mean probabilities.
#'
#' @param values Vector of continuous values
#' @param n_sims Total number of simulations
#' @return Monte Carlo Standard Error
#' @keywords internal
calculate_mcse_mean <- function(values, n_sims) {
  if (length(values) == 0 || n_sims == 0) {
    return(NA_real_)
  }

  # Remove NA values
  values <- values[!is.na(values)]

  if (length(values) == 0) {
    return(NA_real_)
  }

  # MCSE for mean = standard deviation / sqrt(n)
  mcse <- sd(values) / sqrt(length(values))

  return(mcse)
}

#' Monte Carlo Standard Error for Integrated Power Metrics
#'
#' Calculate Monte Carlo Standard Error for integrated power metrics that combine
#' results across multiple effect sizes or sample sizes using weighted averages.
#'
#' @param values Vector of power or probability values
#' @param weights Vector of weights for integration
#' @param n_sims Total number of simulations
#' @param is_power_metric Logical indicating if this is a power metric (TRUE) or probability metric (FALSE)
#' @return Monte Carlo Standard Error for integrated metric
#' @keywords internal
calculate_mcse_integrated_power <- function(values, weights, n_sims, is_power_metric = TRUE) {
  if (length(values) == 0 || length(weights) == 0 || n_sims == 0) {
    return(NA_real_)
  }

  if (length(values) != length(weights)) {
    cli::cli_abort(c(
      "{.arg values} and {.arg weights} must have the same length",
      "x" = "values has length {.val {length(values)}}, weights has length {.val {length(weights)}}",
      "i" = "Ensure both vectors have matching lengths"
    ))
  }

  # Remove NA values
  valid_idx <- !is.na(values) & !is.na(weights)
  values <- values[valid_idx]
  weights <- weights[valid_idx]

  if (length(values) == 0) {
    return(NA_real_)
  }

  # Normalize weights to sum to 1
  weights <- weights / sum(weights)

  # Calculate weighted average
  weighted_avg <- sum(values * weights)

  if (is_power_metric) {
    # For power metrics (proportions), use binomial-based MCSE
    # MCSE for weighted proportion approximation
    mcse <- sqrt(weighted_avg * (1 - weighted_avg) / n_sims)
  } else {
    # For probability metrics (continuous), use weighted variance approach
    # Approximate MCSE using weighted variance
    weighted_var <- sum(weights * (values - weighted_avg)^2)
    mcse <- sqrt(weighted_var / n_sims)
  }

  return(mcse)
}

#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/S7_helpers.R"
#' Update Properties of an S7 Object with `...`
#'
#' Updates properties of an S7 object by replacing values with those passed
#' via `...`. Only existing properties of the object's class are updated;
#' non-matching names in `...` trigger a warning.
#'
#' @param object An S7 object whose class defines properties via `S7::new_class()`.
#' @param ... Named arguments corresponding to properties to update.
#'
#' @return The modified S7 object with updated property values.
#'
#' @keywords internal
update_S7_with_dots <- function(object, ...) {
  dots <- list(...)
  
  # Get all property names of the object
  prop_names <- S7::prop_names(object)
  
  # Replace any matching properties with those from dots
  for (name in names(dots)) {
    if (name %in% prop_names) {
      S7::prop(object = object,name =  name) <- dots[[name]]
    } else {
      cli::cli_warn(c(
        "Invalid property name",
        "x" = "'{name}' is not a property of class '{class(object)}'",
        "i" = "Check valid property names with S7::prop_names()"
      ))
    }
  }
  # revalidate the object to ensure it meets class requirements
  S7::validate(object)
  
  return(object)
}



#' S7 Class Validator for Data Frames
#'
#' Defines an S7 class `class_data_frame` that accepts only S3 `data.frame` objects.
#' This is useful for validating properties in S7 classes that should hold standard
#' data frame values. Used when `S7::class_data.frame` fails to match S3 objects.
#'
#' @format An S7 class object for validating standard data frames.
#'
#' @return An S7 class validator that passes only if the object is a `data.frame`.
#'
#' @keywords internal
class_data_frame <- S7::new_class(
  "class_data_frame",
  parent = S7::new_S3_class("data.frame"),
  validator = function(self) {
    if (!is.data.frame(self)) {
      cli::cli_abort(c(
        "Must be a data.frame",
        "x" = "You supplied {.cls {class(self)}}",
        "i" = "Provide a standard data.frame object"
      ))
    }
    NULL
  }
)
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/analytical_power.R"
#' Power Calculation for ANCOVA with Continuous Covariate and Two Groups
#'
#' Computes the analytical power to detect a group effect in an Analysis of Covariance
#' (ANCOVA) model with one continuous covariate and two groups. The model takes the form:
#' \code{y ~ covariate + group}, where the covariate is assumed to follow N(0,1).
#' This function is vectorized and can handle multiple values for n and/or d parameters.
#'
#' @param n Integer or vector of integers. Total sample size(s) across both groups.
#' @param d Numeric or vector of numerics. Cohen's d for the group effect (standardized mean difference).
#'   This represents the difference in group means divided by the pooled standard
#'   deviation, after adjusting for the covariate. For two-sided tests, the sign
#'   doesn't matter. For one-sided tests, positive d means group 1 > group 0.
#' @param beta_cov Numeric. True regression coefficient for the continuous covariate.
#'   Assumes the covariate follows a standard normal distribution N(0,1).
#' @param sigma Numeric. Residual standard deviation of the outcome after fitting
#'   the full model (including both covariate and group effects).
#' @param equal_groups Logical. If TRUE (default), assumes equal group sizes (n1 = n2).
#'   If FALSE, requires p_group to be specified.
#' @param p_group Numeric. Proportion in group 1 (only used if equal_groups = FALSE).
#' @param alpha Numeric. Type I error rate (significance level). Default is 0.05.
#' @param alternative Character. Specifies the alternative hypothesis. Must be one of
#'   "two.sided" (default), "greater", or "less". For one-sided tests, "greater"
#'   tests if group 1 > group 0, "less" tests if group 1 < group 0.
#' @param method Character. Specifies the lambda calculation method. Must be one of:
#'   \itemize{
#'     \item "cohen" (default): Uses Cohen's convention where \eqn{\lambda = f^{2}(u + v + 1)}
#'     \item "theory": Uses statistical theory where \eqn{\lambda = n \cdot f^{2}}
#'   }
#' @param covariate_method Character. Specifies how to handle the covariate. Must be one of:
#'   \itemize{
#'     \item "fixed" (default): Assumes fixed covariate values (conditional power)
#'     \item "expected": Integrates over covariate distribution (expected power, SAS-style)
#'   }
#' @param numint Integer. Number of integration points for covariate_method = "expected" (default = 2000).
#'
#' @return Numeric vector. The statistical power(s) to detect the specified group effect(s).
#'   If both n and d are vectors, returns a vector with length equal to the maximum of their lengths.
#'
#' @details
#' The function calculates power for testing the group effect in a linear model:
#' \itemize{
#'   \item The outcome y is modeled as: \eqn{y = \beta_0 + \beta_1 \cdot \text{covariate} + \beta_2 \cdot \text{group} + \epsilon}
#'   \item The null hypothesis is \eqn{H_0: \beta_2 = 0}
#'   \item For two-sided tests: \eqn{H_1: \beta_2 \neq 0}
#'   \item For one-sided tests: \eqn{H_1: \beta_2 > 0} (if alternative = "greater") or \eqn{H_1: \beta_2 < 0} (if alternative = "less")
#'   \item The covariate follows \eqn{N(0,1)}
#'   \item Group is coded as 0 or 1
#'   \item Residuals \eqn{\epsilon} follow \eqn{N(0, \sigma^{2})}
#' }
#'
#' Method options:
#' \itemize{
#'   \item "cohen": Uses Cohen's convention matching pwr.f2.test
#'   \item "theory": Uses classical statistical theory
#' }
#'
#' Covariate method options:
#' \itemize{
#'   \item "fixed": Conditional power given fixed covariate values
#'   \item "expected": Expected power integrating over covariate distribution (SAS approach)
#' }
#'
#' For two-sided tests, the function uses the F-test which is equivalent to the
#' square of a two-sided t-test. For one-sided tests, it uses the t-distribution
#' directly.
#'
#' @note
#' This function assumes:
#' \itemize{
#'   \item No interaction between group and covariate
#'   \item The covariate has the same variance in both groups
#'   \item The residual variance is homogeneous across groups
#' }
#'
#' @examples
#' \donttest{
#' # Two-sided test with Cohen's method and fixed covariate
#' analytical_power_ancova_cont_2arms(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1)
#'
#' # Using statistical theory method
#' analytical_power_ancova_cont_2arms(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                    method = "theory")
#'
#' # Using expected power with Cohen's lambda
#' analytical_power_ancova_cont_2arms(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                    covariate_method = "expected")
#'
#' # Using expected power with theory lambda
#' analytical_power_ancova_cont_2arms(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                    method = "theory", covariate_method = "expected")
#'
#' # Vectorized: multiple sample sizes
#' analytical_power_ancova_cont_2arms(n = c(50, 100, 150), d = 0.5, beta_cov = 0.4, sigma = 1)
#'
#' # One-sided test (group 1 > group 0)
#' analytical_power_ancova_cont_2arms(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                    alternative = "greater")
#'
#' # Unequal groups (70/30 split)
#' analytical_power_ancova_cont_2arms(n = 150, d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                    equal_groups = FALSE, p_group = 0.7)
#'}
#'
#' @references
#' Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.).
#' Lawrence Erlbaum Associates.
#'
#' Shieh, G. (2020). Power analysis and sample size planning in ANCOVA designs.
#' Psychometrika, 85(1), 101-120.
#'
#' @export
analytical_power_ancova_cont_2arms <- function(n,
                                               d,
                                               beta_cov,
                                               sigma,
                                               equal_groups = TRUE,
                                               p_group = 0.5,
                                               alpha = 0.05,
                                               alternative = c("two.sided", "greater", "less"),
                                               method = c("cohen", "theory"),
                                               covariate_method = c("fixed", "expected"),
                                               numint = 2000) {
  # Match arguments
  alternative <- match.arg(alternative)
  method <- match.arg(method)
  covariate_method <- match.arg(covariate_method)
  
  # Input validation for single-valued parameters
  if (!is.numeric(beta_cov) || length(beta_cov) != 1) {
    cli::cli_abort(c(
      "{.arg beta_cov} must be a single numeric value",
      "x" = "You supplied {.val {beta_cov}}",
      "i" = "Provide a single covariate coefficient"
    ))
  }
  if (!is.numeric(sigma) || length(sigma) != 1 || sigma <= 0) {
    cli::cli_abort(c(
      "{.arg sigma} must be a single positive numeric value",
      "x" = "You supplied {.val {sigma}}",
      "i" = "Use a single value > 0"
    ))
  }
  if (!is.numeric(alpha) ||
      length(alpha) != 1 || alpha <= 0 || alpha >= 1) {
    cli::cli_abort(c(
      "{.arg alpha} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {alpha}}",
      "i" = "Use a value like 0.05 or 0.01"
    ))
  }
  if (!is.logical(equal_groups) || length(equal_groups) != 1) {
    cli::cli_abort(c(
      "{.arg equal_groups} must be a single logical value",
      "x" = "You supplied {.type {equal_groups}}",
      "i" = "Use TRUE or FALSE"
    ))
  }
  if (!equal_groups && (
    is.null(p_group) || !is.numeric(p_group) ||
    length(p_group) != 1 ||
    p_group <= 0 || p_group >= 1
  )) {
    cli::cli_abort(c(
      "When {.arg equal_groups} = FALSE, {.arg p_group} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {p_group}}",
      "i" = "Use a proportion like 0.5 or 0.7"
    ))
  }
  if (!is.numeric(numint) || length(numint) != 1 || numint < 10) {
    cli::cli_abort(c(
      "{.arg numint} must be a single numeric value >= 10",
      "x" = "You supplied {.val {numint}}",
      "i" = "Use a value like 2000 for accurate integration"
    ))
  }
  
  # Input validation for vectorizable parameters
  if (!is.numeric(n) || any(n <= 0) || any(n != floor(n))) {
    cli::cli_abort(c(
      "{.arg n} must be positive integer(s)",
      "x" = "You supplied {.val {n}}",
      "i" = "Use positive whole numbers"
    ))
  }
  if (!is.numeric(d)) {
    cli::cli_abort(c(
      "{.arg d} must be numeric",
      "x" = "You supplied {.type {d}}",
      "i" = "Provide Cohen's d effect size as a number"
    ))
  }
  
  # Recycle n and d to the same length
  max_length <- max(length(n), length(d))
  n <- rep_len(n, max_length)
  d <- rep_len(d, max_length)
  
  # Check consistency of d and alternative for one-sided tests (vectorized)
  if (alternative == "greater" && any(d < 0)) {
    cli::cli_warn(c(
      "Effect size direction inconsistent with alternative hypothesis",
      "x" = "Some 'd' values are negative but 'alternative' is 'greater'",
      "i" = "Power will be very low for those values"
    ))
  }
  if (alternative == "less" && any(d > 0)) {
    cli::cli_warn(c(
      "Effect size direction inconsistent with alternative hypothesis",
      "x" = "Some 'd' values are positive but 'alternative' is 'less'",
      "i" = "Power will be very low for those values"
    ))
  }
  
  # Sample sizes per group (vectorized)
  if (equal_groups) {
    n1 <- floor(n / 2)
    n2 <- n - n1
  } else {
    n1 <- floor(n * p_group)
    n2 <- n - n1
  }
  
  # Variance of group indicator (0/1 coding) (vectorized)
  var_group <- (n1 * n2) / n^2
  
  # Convert Cohen's d to regression coefficient (vectorized)
  beta_group <- d * sigma
  
  # Calculate f^2 (vectorized)
  f2 <- (beta_group^2 * var_group) / sigma^2
  
  # For expected method, need to handle one-at-a-time
  if (covariate_method == "expected") {
    power <- numeric(max_length)
    for (i in 1:max_length) {
      power[i] <- power_ancova_expected_single(
        n = n[i],
        d = d[i],
        beta_cov = beta_cov,
        sigma = sigma,
        n1 = n1[i],
        n2 = n2[i],
        var_group = var_group[i],
        f2 = f2[i],
        alpha = alpha,
        alternative = alternative,
        lambda_method = method,
        numint = numint
      )
    }
    return(power)
  }
  
  # Fixed covariate method (vectorized)
  # Standard error of group coefficient (vectorized)
  se_group <- sigma / sqrt(n * var_group)
  
  # Degrees of freedom (vectorized)
  df <- n - 3   # n - number of parameters (intercept, covariate, group)
  
  if (alternative == "two.sided") {
    # For two-sided test, use F-test approach
    
    # Non-centrality parameter based on method
    if (method == "cohen") {
      # Cohen's convention: lambda = f^2 * (u + v + 1)
      u <- 1  # numerator df
      lambda <- f2 * (u + df + 1)
    } else {
      # method == "theory"
      # Statistical theory: lambda = n * f^2
      lambda <- n * f2
    }
    
    # Critical F-value (vectorized)
    F_crit <- stats::qf(1 - alpha, df1 = 1, df2 = df)
    
    # Power (vectorized)
    power <- 1 - stats::pf(F_crit,
                    df1 = 1,
                    df2 = df,
                    ncp = lambda)
    
  } else {
    # For one-sided tests, use t-test approach
    # Test statistic under alternative: t = beta_group / se_group (vectorized)
    t_alt <- beta_group / se_group
    
    # Critical t-value (vectorized)
    if (alternative == "greater") {
      t_crit <- stats::qt(1 - alpha, df = df)
      # Power = P(T > t_crit | H1) (vectorized)
      power <- 1 - stats::pt(t_crit, df = df, ncp = t_alt)
    } else {
      # alternative == "less"
      t_crit <- stats::qt(alpha, df = df)
      # Power = P(T < t_crit | H1) (vectorized)
      power <- stats::pt(t_crit, df = df, ncp = t_alt)
    }
  }
  
  return(power)
}

#' @keywords internal
#' @importFrom stats qf qt pf dt pt
# Helper function for expected power calculation (single value)
power_ancova_expected_single <- function(n,
                                          d,
                                          beta_cov,
                                          sigma,
                                          n1,
                                          n2,
                                          var_group,
                                          f2,
                                          alpha,
                                          alternative,
                                          lambda_method,
                                          numint) {
  # Setup similar to SAS
  dd <- 1e-5
  g <- 2  # number of groups
  n_cov <- 1  # number of covariates
  
  # Adjusted mean difference
  mean_diff <- d * sigma
  
  # Degrees of freedom
  df1 <- 1       # numerator df (testing one group parameter)
  df2 <- n - 3   # denominator df
  dfx <- df2 + 1
  
  # Integration weights (Simpson's rule coefficients)
  coevec <- c(1, rep(c(4, 2), numint / 2 - 1), 4, 1)
  
  if (alternative == "two.sided") {
    # Critical F-value
    f_crit <- stats::qf(1 - alpha, df1 = df1, df2 = df2)
    
    # For P=1 covariate case (matching SAS logic)
    b <- n_cov / dfx
    
    # Integration over t-distribution
    tl <- stats::qt(dd, dfx)
    tu <- stats::qt(1 - dd, dfx)
    intl <- (tu - tl) / numint
    tvec <- tl + intl * (0:numint)
    
    # Weights for integration
    wtpdf <- (intl / 3) * coevec * stats::dt(tvec, dfx)
    
    # Non-centrality parameter at each integration point
    if (lambda_method == "theory") {
      # Statistical theory: lambda = n * f^2
      ncp_vec <- n * f2 / (1 + b * tvec^2)
    } else {
      # lambda_method == "cohen"
      # Cohen's convention: lambda = f^2 * (u + v + 1)
      ncp_vec <- f2 * (df1 + df2 + 1) / (1 + b * tvec^2)
    }
    
    # Expected power calculation
    power <- sum(wtpdf * stats::pf(f_crit, df1, df2, ncp = ncp_vec, lower.tail = FALSE))
    
  } else {
    # For one-sided tests
    # Standard error accounting for covariate
    se_group <- sigma / sqrt(n * var_group)
    
    # Integration over t-distribution for covariate effect
    b <- n_cov / dfx
    tl <- stats::qt(dd, dfx)
    tu <- stats::qt(1 - dd, dfx)
    intl <- (tu - tl) / numint
    tvec <- tl + intl * (0:numint)
    wtpdf <- (intl / 3) * coevec * stats::dt(tvec, dfx)
    
    # Effective variance inflation due to covariate
    var_inflation <- 1 + b * tvec^2
    
    # Non-centrality for t-test at each integration point
    t_ncp <- mean_diff / (se_group * sqrt(var_inflation))
    
    if (alternative == "greater") {
      t_crit <- stats::qt(1 - alpha, df = df2)
      power_vec <- 1 - stats::pt(t_crit, df = df2, ncp = t_ncp)
    } else {
      t_crit <- stats::qt(alpha, df = df2)
      power_vec <- stats::pt(t_crit, df = df2, ncp = t_ncp)
    }
    
    # Integrate
    power <- sum(wtpdf * power_vec)
  }
  
  return(power)
}

#' Compare Power Calculation Methods
#'
#' Compares power calculations across different method combinations.
#'
#' @param n Integer. Total sample size.
#' @param d Numeric. Cohen's d.
#' @param beta_cov Numeric. Covariate coefficient.
#' @param sigma Numeric. Residual SD.
#' @param alpha Numeric. Significance level.
#'
#' @return Data frame comparing power across methods.
#'
#' @examples
#' \donttest{
#' compare_power_methods(n = 100, d = 0.5, beta_cov = 0.4, sigma = 1)
#' }
#' @importFrom pwr pwr.f2.test
#' @export
compare_power_methods <- function(n, d, beta_cov, sigma, alpha = 0.05) {
  # All combinations
  methods <- expand.grid(
    method = c("cohen", "theory"),
    covariate_method = c("fixed", "expected"),
    stringsAsFactors = FALSE
  )
  
  methods$power <- NA
  
  for (i in 1:nrow(methods)) {
    methods$power[i] <- analytical_power_ancova_cont_2arms(
      n = n,
      d = d,
      beta_cov = beta_cov,
      sigma = sigma,
      alpha = alpha,
      method = methods$method[i],
      covariate_method = methods$covariate_method[i]
    )
  }
  
  # Also compare with pwr package
  var_group <- 0.25  # equal groups
  beta_group <- d * sigma
  f2 <- (beta_group^2 * var_group) / sigma^2
  power_pwr <- pwr::pwr.f2.test(
    u = 1,
    v = n - 3,
    f2 = f2,
    sig.level = alpha
  )$power
  
  # Add pwr result
  methods <- rbind(
    methods,
    data.frame(
      method = "pwr.f2.test",
      covariate_method = "fixed",
      power = power_pwr
    )
  )
  
  # Add description
  methods$description <- paste(methods$method, methods$covariate_method, sep = " + ")
  
  return(methods[, c("description", "power", "method", "covariate_method")])
}

#' Compute Cohen's f-squared for Group Effect in Linear Model
#'
#' Calculates Cohen's f-squared for the group effect in a linear regression model of the form
#' \code{y ~ covariate + group}. Cohen's f-squared represents the proportion of variance
#' uniquely explained by the group variable relative to the unexplained variance.
#' This function is vectorized and can handle multiple values for the d parameter.
#'
#' @param d Numeric or vector of numerics. Cohen's d for the group effect (standardized mean difference).
#'   This represents the difference in group means divided by the pooled standard
#'   deviation, after adjusting for the covariate.
#' @param beta_cov Numeric. True coefficient for the covariate (assumes covariate ~ N(0,1)).
#' @param sigma Numeric. Residual standard deviation after fitting the full model.
#' @param equal_groups Logical. If TRUE (default), assumes equal group sizes (n1 = n2).
#'   If FALSE, requires p_group to be specified.
#' @param p_group Numeric. Proportion in group 1 (only used if equal_groups = FALSE).
#'
#' @return Numeric vector. Cohen's f-squared for the group effect(s).
#'
#' @details
#' Cohen's f-squared is calculated as the ratio of variance uniquely explained by the group
#' to the unexplained variance:
#' \deqn{f^{2} = \frac{R^{2}_{\text{full}} - R^{2}_{\text{reduced}}}{1 - R^{2}_{\text{full}}}}
#'
#' where \eqn{R^{2}_{\text{full}}} includes both predictors and \eqn{R^{2}_{\text{reduced}}} includes only the covariate.
#'
#' @examples
#' \donttest{
#' # Single value
#' f2_from_params_ancova_cont_2arms(d = 0.5, beta_cov = 0.4, sigma = 1)
#'
#' # Multiple d values (vectorized)
#' f2_from_params_ancova_cont_2arms(d = c(0.2, 0.5, 0.8), beta_cov = 0.4, sigma = 1)
#'
#' # Unequal groups (70/30 split)
#' f2_from_params_ancova_cont_2arms(d = 0.5, beta_cov = 0.4, sigma = 1,
#'                                   equal_groups = FALSE, p_group = 0.7)
#' }
#' @export
f2_from_params_ancova_cont_2arms <- function(d,
                                             beta_cov,
                                             sigma,
                                             equal_groups = TRUE,
                                             p_group = 0.5) {
  # Input validation
  if (!is.numeric(d)) {
    cli::cli_abort(c(
      "{.arg d} must be numeric",
      "x" = "You supplied {.type {d}}",
      "i" = "Provide Cohen's d as a number"
    ))
  }
  if (!is.numeric(beta_cov) || length(beta_cov) != 1) {
    cli::cli_abort(c(
      "{.arg beta_cov} must be a single numeric value",
      "x" = "You supplied {.val {beta_cov}}",
      "i" = "Provide a single covariate coefficient"
    ))
  }
  if (!is.numeric(sigma) || length(sigma) != 1 || sigma <= 0) {
    cli::cli_abort(c(
      "{.arg sigma} must be a single positive numeric value",
      "x" = "You supplied {.val {sigma}}",
      "i" = "Use a value > 0"
    ))
  }
  if (!is.logical(equal_groups) || length(equal_groups) != 1) {
    cli::cli_abort(c(
      "{.arg equal_groups} must be a single logical value",
      "x" = "You supplied {.type {equal_groups}}",
      "i" = "Use TRUE or FALSE"
    ))
  }
  if (!equal_groups && (
    is.null(p_group) || !is.numeric(p_group) ||
    length(p_group) != 1 || p_group <= 0 || p_group >= 1
  )) {
    cli::cli_abort(c(
      "When {.arg equal_groups} = FALSE, {.arg p_group} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {p_group}}",
      "i" = "Use a proportion like 0.5 or 0.7"
    ))
  }
  
  # Convert Cohen's d to regression coefficient (vectorized)
  beta_group <- d * sigma
  
  # Variance of group indicator
  if (equal_groups) {
    var_group <- 0.25  # p(1-p) with p = 0.5
  } else {
    var_group <- p_group * (1 - p_group)
  }
  
  # Direct calculation of Cohen's f^2 (vectorized for d/beta_group)
  # f^2 = (beta_group^2 * var_group) / sigma^2
  # Since beta_group = d * sigma, this becomes:
  # f^2 = (d^2 * sigma^2 * var_group) / sigma^2 = d^2 * var_group
  f2 <- d^2 * var_group
  
  return(f2)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/backend_bf.R"
# =============================================================================
# BAYESFLOW BACKEND
# =============================================================================
# All BayesFlow-specific posterior estimation and summarization.
# Uses fast vectorized matrix operations for maximum performance.
# Models are loaded via Python/reticulate for full BayesFlow compatibility.
#
# Entry points:
#   - estimate_single_bf(): Single analysis estimation (wraps batch of 1)
#   - estimate_sequential_bf(): Sequential/interim analysis estimation (batched)
#   - estimate_batch_bf(): Core batch estimation (single forward pass)
#
# Key functions:
#   - load_bf_model_python(): Load .keras model via Python
#   - summarize_post_bf(): Fast vectorized summarization (NOT compute_measures())
#   - prepare_single_as_batch_bf(): Convert single data.frame to batch format
#   - prepare_data_list_as_batch_bf(): Convert list of data.frames to batch format
#
# BayesFlow Model Types Supported:
#   - bf.BasicWorkflow object (has .sample() method)
#   - bf.approximators.Approximator object (has .sample() method)
#   - Raw Keras model (has .predict() method, returns distribution params)
#
# Output: All functions produce results compatible with summarize_sims()
#
# PRIORITY: Fast computation via vectorized matrix operations.
# This backend does NOT use compute_measures() or rvar operations.

# =============================================================================
# PYTHON ENVIRONMENT MANAGEMENT
# =============================================================================

# Package-level Python module cache
.bf_cache <- new.env(parent = emptyenv())


#' Check BayesFlow Availability
#'
#' Checks if Python, reticulate, and BayesFlow are available for use.
#' This is a fast check that can be called before attempting inference.
#'
#' @param silent If TRUE, return FALSE instead of error (default FALSE)
#'
#' @return TRUE if BayesFlow is available, FALSE or error otherwise
#' @export
#'
#' @examples
#' \dontrun{
#' if (check_bf_available(silent = TRUE)) {
#'   message("BayesFlow is available!")
#' }
#' }
check_bf_available <- function(silent = FALSE) {
  # Check reticulate package

  if (!requireNamespace("reticulate", quietly = TRUE)) {
    if (silent) return(FALSE)
    cli::cli_abort(c(
      "Package 'reticulate' is required for BayesFlow backend",
      "i" = "Install it with: install.packages('reticulate')"
    ))
  }

  # Check Python available
  if (!reticulate::py_available(initialize = TRUE)) {
    if (silent) return(FALSE)
    cli::cli_abort(c(
      "Python is not available",
      "i" = "Configure Python with: reticulate::use_python() or reticulate::use_condaenv()"
    ))
  }

  # Check BayesFlow module
  bf_available <- reticulate::py_module_available("bayesflow")
  if (!bf_available) {
    if (silent) return(FALSE)
    cli::cli_abort(c(
      "Python package 'bayesflow' is not installed",
      "i" = "Install it with: pip install bayesflow",
      "i" = "Or from R: reticulate::py_install('bayesflow')"
    ))
  }

  TRUE
}


#' Initialize BayesFlow Python Environment
#'
#' Imports BayesFlow, Keras, and NumPy modules, caching them for reuse.
#' Called automatically by estimation functions.
#'
#' This function:
#' 1. Returns cached modules if already initialized
#' 2. Declares Python dependencies via py_require() for automatic venv provisioning
#' 3. Sets KERAS_BACKEND=torch before importing keras (required for PyTorch backend)
#' 4. Imports modules with delay_load for CRAN compatibility
#'
#' @return List with bf, np, and keras Python modules
#' @export
#'
#' @examples
#' \dontrun{
#' py_mods <- init_bf_python()
#' py_mods$bf  # BayesFlow module
#' py_mods$keras  # Keras module
#' }
init_bf_python <- function() {
  # Return cached if available
  if (exists("bf", envir = .bf_cache) &&
      exists("np", envir = .bf_cache) &&
      exists("keras", envir = .bf_cache) &&
      !is.null(.bf_cache$bf)) {
    return(list(bf = .bf_cache$bf, np = .bf_cache$np, keras = .bf_cache$keras))
  }

  # Ensure availability
  check_bf_available(silent = FALSE)

  # Declare Python dependencies for automatic provisioning

  # py_require() triggers automatic virtual environment setup if needed
  tryCatch({
    reticulate::py_require(
      c("bayesflow>=2.0", "keras>=3.0", "torch", "numpy"),
      python_version = "3.12"
    )
  }, error = function(e) {
    cli::cli_warn(c(
      "Could not configure Python environment automatically",
      "i" = "You may need to install packages manually:",
      " " = "pip install bayesflow keras torch numpy"
    ))
  })

  # Set KERAS_BACKEND before importing keras (critical for PyTorch backend)
  # Only set if not already configured by user
  if (Sys.getenv("KERAS_BACKEND") == "") {
    os <- reticulate::import("os")
    os$environ$`__setitem__`("KERAS_BACKEND", "torch")
    cli::cli_alert_info("Set KERAS_BACKEND=torch for BayesFlow")
  }

  # Import modules with delay_load for CRAN compatibility
  .bf_cache$bf <- reticulate::import("bayesflow", delay_load = TRUE)
  .bf_cache$np <- reticulate::import("numpy", convert = FALSE, delay_load = TRUE)
  .bf_cache$keras <- reticulate::import("keras", delay_load = TRUE)

  list(bf = .bf_cache$bf, np = .bf_cache$np, keras = .bf_cache$keras)
}


# =============================================================================
# PYTHON MODEL LOADING
# =============================================================================

#' Load BayesFlow Model via Python
#'
#' Loads a BayesFlow model from file. Supports multiple formats:
#' - `.keras` files: Loaded via keras.saving.load_model()
#' - Pickled workflow/approximator objects
#'
#' @param model_path Path to model file (.keras or .pkl)
#'
#' @return Python model object (Keras model, BayesFlow Approximator, or Workflow)
#' @keywords internal
load_bf_model_python <- function(model_path) {
  if (!file.exists(model_path)) {
    cli::cli_abort("Model file not found: {.path {model_path}}")
  }

  # Initialize Python environment (includes cached keras)
  py_mods <- init_bf_python()

  ext <- tools::file_ext(model_path)

  if (ext == "keras") {
    # Load Keras model using cached module
    # Use keras.saving.load_model() with compile=TRUE, safe_mode=FALSE
    # for BayesFlow 2.0+ compatibility
    model <- tryCatch({
      py_mods$keras$saving$load_model(
        model_path,
        compile = TRUE,
        safe_mode = FALSE
      )
    }, error = function(e) {
      cli::cli_abort(c(
        "Failed to load BayesFlow model: {.path {model_path}}",
        "x" = conditionMessage(e),
        "i" = "Ensure model was saved with BayesFlow 2.0+ and Keras 3"
      ))
    })
  } else if (ext %in% c("pkl", "pickle")) {
    # Load pickled BayesFlow object
    pickle <- reticulate::import("pickle")
    builtins <- reticulate::import_builtins()
    f <- builtins$open(model_path, "rb")
    model <- pickle$load(f)
    f$close()
  } else {
    cli::cli_abort(c(
      "Unknown model file extension: {.val {ext}}",
      "i" = "Supported formats: .keras, .pkl"
    ))
  }

  model
}


#' Detect Function Type (R vs Python)
#'
#' Determines whether a function object is an R function or Python callable.
#'
#' @param fn Function or Python callable to check
#'
#' @return Character: "r", "python", or "unknown"
#' @keywords internal
detect_function_type <- function(fn) {
  if (is.function(fn)) return("r")
  if (inherits(fn, c("python.builtin.function", "python.builtin.method"))) return("python")
  if ("__call__" %in% names(fn)) return("python")
  return("unknown")
}


# =============================================================================
# FAST POSTERIOR SUMMARIZATION
# =============================================================================

#' Summarize Posterior Draws - BayesFlow Backend
#'
#' Computes all summary statistics directly from draws matrix using vectorized
#' operations. This is the fast path - does NOT use compute_measures() or rvars.
#'
#' Consistent API with summarize_post_brms() - both produce identical output schema.
#'
#' @param draws_mat Matrix of posterior draws (n_sims x n_post_draws)
#' @param target_param Parameter name (single string)
#' @param thr_scs Success threshold (numeric)
#' @param thr_ftl Futility threshold (numeric)
#' @param p_sig_scs Success probability threshold
#' @param p_sig_ftl Futility probability threshold
#' @param id_iter Vector of iteration IDs (length = n_sims)
#' @param id_cond Vector of condition IDs (length = n_sims)
#' @param id_look Analysis look ID (integer)
#' @param n_analyzed Sample size at this analysis (integer)
#'
#' @return Data frame with package output schema (n_sims rows)
#' @keywords internal
summarize_post_bf <- function(draws_mat, target_param,
                               thr_scs, thr_ftl, p_sig_scs, p_sig_ftl,
                               id_iter, id_cond, id_look = 1L, n_analyzed) {

  # Check matrixStats availability (for performance)
  if (!requireNamespace("matrixStats", quietly = TRUE)) {
    cli::cli_abort(c(
      "Package 'matrixStats' is required for BayesFlow backend",
      "i" = "Install it with: install.packages('matrixStats')"
    ))
  }

  n_sims <- nrow(draws_mat)

  # FAST vectorized operations using matrixStats
  # Probability of exceeding success threshold
  pr_scs <- rowMeans(draws_mat > thr_scs)
  # Probability below futility threshold
  pr_ftl <- rowMeans(draws_mat < thr_ftl)

  data.frame(
    par_name = rep(target_param, n_sims),
    thr_scs = rep(thr_scs, n_sims),
    thr_ftl = rep(thr_ftl, n_sims),
    p_sig_scs = rep(p_sig_scs, n_sims),
    p_sig_ftl = rep(p_sig_ftl, n_sims),
    pr_scs = pr_scs,
    pr_ftl = pr_ftl,
    dec_scs = as.integer(pr_scs >= p_sig_scs),
    dec_ftl = as.integer(pr_ftl >= p_sig_ftl),
    post_med = matrixStats::rowMedians(draws_mat),
    post_mad = matrixStats::rowMads(draws_mat),
    post_mn = rowMeans(draws_mat),
    post_sd = matrixStats::rowSds(draws_mat),
    rhat = rep(NA_real_, n_sims),       # Not applicable for BayesFlow
    ess_bulk = rep(NA_real_, n_sims),   # Not applicable for BayesFlow
    ess_tail = rep(NA_real_, n_sims),   # Not applicable for BayesFlow
    id_iter = id_iter,
    id_cond = id_cond,
    id_look = rep(as.integer(id_look), n_sims),
    n_analyzed = rep(as.integer(n_analyzed), n_sims),
    stopped = rep(FALSE, n_sims),
    stop_reason = rep(NA_character_, n_sims),
    converged = rep(1L, n_sims),
    error_msg = rep(NA_character_, n_sims),
    stringsAsFactors = FALSE
  )
}


# =============================================================================
# SUMMARY STATISTICS COMPUTATION
# =============================================================================

#' Compute Summary Statistics for ANCOVA Batch (BayesFlow Input)
#'
#' Extracts summary statistics from batch simulation data in the format
#' expected by BayesFlow trained models. The summary statistics must match
#' exactly what the BayesFlow model was trained on.
#'
#' For 2-arm ANCOVA, summaries are:
#' 1. mean_outcome_ctrl, 2. mean_outcome_treat,
#' 3. sd_outcome_ctrl, 4. sd_outcome_treat,
#' 5. n_ctrl, 6. n_treat,
#' 7. cor_outcome_covariate, 8. mean_covariate
#'
#' @param data_batch List with batch-formatted data from prepare_*_batch_bf()
#'   containing: outcome (matrix), covariate (matrix), group (matrix)
#'
#' @return Matrix of summary statistics (batch_size x n_summaries)
#' @keywords internal
compute_summaries_batch_ancova <- function(data_batch) {
  outcome <- data_batch$outcome
  covariate <- data_batch$covariate
  group <- data_batch$group

  batch_size <- nrow(outcome)
  n_summaries <- 8L

  summaries <- matrix(NA_real_, nrow = batch_size, ncol = n_summaries)

  for (i in seq_len(batch_size)) {
    ctrl_mask <- group[i, ] == 0
    treat_mask <- group[i, ] == 1

    # Handle edge cases where one arm might be empty
    mean_ctrl <- if (any(ctrl_mask)) mean(outcome[i, ctrl_mask]) else NA_real_
    mean_treat <- if (any(treat_mask)) mean(outcome[i, treat_mask]) else NA_real_
    sd_ctrl <- if (sum(ctrl_mask) > 1) stats::sd(outcome[i, ctrl_mask]) else NA_real_
    sd_treat <- if (sum(treat_mask) > 1) stats::sd(outcome[i, treat_mask]) else NA_real_

    summaries[i, ] <- c(
      mean_ctrl,
      mean_treat,
      sd_ctrl,
      sd_treat,
      sum(ctrl_mask),
      sum(treat_mask),
      stats::cor(outcome[i, ], covariate[i, ]),
      mean(covariate[i, ])
    )
  }

  summaries
}


# =============================================================================
# DATA PREPARATION
# =============================================================================

#' Prepare Single data.frame as Batch Format for BayesFlow
#'
#' Converts a single simulation's data.frame to the batch format expected
#' by BayesFlow models.
#'
#' @param data Single data.frame with columns: outcome, covariate, arm
#' @param backend_args List containing p_alloc and other settings
#'
#' @return List with batch-formatted arrays (n_sims = 1)
#' @keywords internal
prepare_single_as_batch_bf <- function(data, backend_args) {
  list(
    outcome = matrix(data$outcome, nrow = 1),
    covariate = matrix(data$covariate, nrow = 1),
    group = matrix(data$arm, nrow = 1),
    N = nrow(data),
    p_alloc = backend_args$p_alloc %||% 0.5
  )
}


#' Prepare List of data.frames as Batch Format for BayesFlow
#'
#' Converts a list of simulation data.frames to the batch format expected
#' by BayesFlow models.
#'
#' @param data_list List of data.frames, each with columns: outcome, covariate, arm
#' @param backend_args List containing p_alloc and other settings
#'
#' @return List with batch-formatted arrays
#' @keywords internal
prepare_data_list_as_batch_bf <- function(data_list, backend_args) {
  list(
    outcome = do.call(rbind, lapply(data_list, function(d) d$outcome)),
    covariate = do.call(rbind, lapply(data_list, function(d) d$covariate)),
    group = do.call(rbind, lapply(data_list, function(d) d$arm)),
    N = nrow(data_list[[1]]),
    p_alloc = backend_args$p_alloc %||% 0.5
  )
}


# =============================================================================
# BATCH ESTIMATION (CORE)
# =============================================================================

#' Detect BayesFlow Model Type
#'
#' Determines the type of BayesFlow model object for proper inference dispatch.
#'
#' @param bf_model Python model object
#'
#' @return Character: "workflow", "approximator", "keras", or "unknown"
#' @keywords internal
detect_bf_model_type <- function(bf_model) {
  # Check for BayesFlow BasicWorkflow
  if (reticulate::py_has_attr(bf_model, "sample") &&
      reticulate::py_has_attr(bf_model, "fit_online")) {
    return("workflow")
  }

  # Check for BayesFlow Approximator (has sample but not fit_online)
  if (reticulate::py_has_attr(bf_model, "sample") &&
      reticulate::py_has_attr(bf_model, "fit")) {
    return("approximator")
  }

  # Check for raw Keras model
  if (reticulate::py_has_attr(bf_model, "predict") &&
      reticulate::py_has_attr(bf_model, "compile")) {
    return("keras")
  }

  "unknown"
}


#' Sample from BayesFlow Model
#'
#' Dispatches to appropriate sampling method based on model type.
#' Handles BayesFlow Workflow, Approximator, and raw Keras models.
#'
#' @param bf_model Python BayesFlow model object
#' @param summaries_np NumPy array of summary statistics (batch_size x n_summaries)
#' @param n_samples Number of posterior samples to draw
#' @param target_param Name of target parameter (for workflow conditions key)
#'
#' @return NumPy array of posterior samples (batch_size x n_samples) or (batch_size x n_samples x n_params)
#' @keywords internal
sample_bf_model <- function(bf_model, summaries_np, n_samples, target_param = "b_arm_treat") {
  model_type <- detect_bf_model_type(bf_model)
  np <- reticulate::import("numpy")

  samples <- switch(model_type,
    workflow = {
      # BayesFlow BasicWorkflow: sample(conditions=dict, num_samples=int)
      conditions <- reticulate::dict(summaries = summaries_np)
      bf_model$sample(conditions = conditions, num_samples = as.integer(n_samples))
    },

    approximator = {
      # BayesFlow Approximator: sample(data=dict, num_samples=int)
      # The adapter determines which dict keys are expected
      data <- reticulate::dict(summary_variables = summaries_np)
      bf_model$sample(data = data, num_samples = as.integer(n_samples))
    },

    keras = {
      # Raw Keras model: predict() returns distribution parameters
      # We'll assume it's an NPE that outputs [batch, n_params * 2] for mean/std
      # Then sample from Gaussian
      output <- bf_model$predict(summaries_np)
      n_params <- as.integer(ncol(reticulate::py_to_r(output)) / 2L)

      # Extract mean and std (assuming first half is mean, second half is log_std)
      means <- output[, 1:n_params]
      log_stds <- output[, (n_params + 1):(2 * n_params)]
      stds <- np$exp(log_stds)

      # Sample from Gaussian: [batch, n_samples, n_params]
      batch_size <- nrow(reticulate::py_to_r(means))
      samples_shape <- c(as.integer(batch_size), as.integer(n_samples), as.integer(n_params))
      noise <- np$random$randn(samples_shape[[1]], samples_shape[[2]], samples_shape[[3]])

      # Broadcast: means[batch, 1, params] + stds[batch, 1, params] * noise[batch, samples, params]
      means_expanded <- np$expand_dims(means, 1L)
      stds_expanded <- np$expand_dims(stds, 1L)
      means_expanded + stds_expanded * noise
    },

    # Unknown model type
    {
      cli::cli_abort(c(
        "Unknown BayesFlow model type",
        "i" = "Model must be a BasicWorkflow, Approximator, or Keras model",
        "i" = "Model class: {class(bf_model)}"
      ))
    }
  )

  samples
}


#' Check if Mock Mode is Enabled
#'
#' Returns TRUE if BayesFlow mock mode is enabled via environment variable.
#' Used for testing R infrastructure without requiring Python/BayesFlow.
#'
#' @return Logical indicating if mock mode is enabled
#' @keywords internal
is_bf_mock_mode <- function() {
  Sys.getenv("RCTBP_MOCK_BF") == "TRUE"
}


#' Generate Mock BayesFlow Samples
#'
#' Generates fake posterior samples for testing purposes.
#' Samples are drawn from a normal distribution centered on a
#' reasonable treatment effect estimate.
#'
#' @param batch_size Number of simulations
#' @param n_samples Number of posterior samples per simulation
#' @param data_batch Optional data batch for data-driven mock
#'
#' @return Matrix (batch_size x n_samples) of mock posterior draws
#' @keywords internal
mock_bf_samples <- function(batch_size, n_samples, data_batch = NULL) {
  # Generate mock samples centered around 0.3 (typical effect size)
  # with some variation based on batch position
  samples <- matrix(
    stats::rnorm(batch_size * n_samples, mean = 0.3, sd = 0.15),
    nrow = batch_size,
    ncol = n_samples
  )

  # If data_batch provided, use summary stats to inform mock
  if (!is.null(data_batch) && all(c("outcome", "group") %in% names(data_batch))) {
    # Compute crude treatment effect estimates from data
    summaries <- compute_summaries_batch_ancova(data_batch)
    # Column 1 = mean_ctrl, Column 2 = mean_treat
    crude_effects <- summaries[, 2] - summaries[, 1]

    # Center mock samples around crude estimates
    for (i in seq_len(batch_size)) {
      samples[i, ] <- stats::rnorm(n_samples, mean = crude_effects[i], sd = 0.15)
    }
  }

  samples
}


#' Estimate Posterior Using BayesFlow (Batch)
#'
#' Processes a batch of simulations through BayesFlow in a single forward pass.
#' This is the core function for neural posterior estimation.
#'
#' Supports multiple BayesFlow model types:
#' - BasicWorkflow: Uses workflow.sample(conditions=dict, num_samples=int)
#' - Approximator: Uses approximator.sample(data=dict, num_samples=int)
#' - Raw Keras: Uses model.predict() + Gaussian sampling
#'
#' Mock Mode: Set environment variable RCTBP_MOCK_BF=TRUE to use mock samples
#' instead of actual BayesFlow inference. Useful for testing R infrastructure.
#'
#' @param data_batch List with batched simulation data from prepare_*_bf().
#'   Must contain: outcome (matrix), covariate (matrix), group (matrix), N (int)
#' @param bf_model BayesFlow/Keras model (Python object via reticulate)
#' @param backend_args List with n_posterior_samples (default 1000), etc.
#' @param target_params Character vector of parameter names (first used for single-param models)
#'
#' @return Matrix of posterior draws (batch_size x n_posterior_samples).
#'   For multi-parameter models, only the first target parameter is returned.
#' @keywords internal
estimate_batch_bf <- function(data_batch, bf_model, backend_args, target_params) {
  # Extract settings
  n_samples <- backend_args$n_posterior_samples %||% 1000L
  n_samples <- as.integer(n_samples)
  batch_size <- nrow(data_batch$outcome)

  # Mock mode for testing without Python
  if (is_bf_mock_mode()) {
    cli::cli_alert_info("Using BayesFlow mock mode (RCTBP_MOCK_BF=TRUE)")
    return(mock_bf_samples(batch_size, n_samples, data_batch))
  }

  # Initialize Python
  py_mods <- init_bf_python()
  np <- py_mods$np

  # Step 1: Compute summary statistics from data batch
  summaries <- compute_summaries_batch_ancova(data_batch)
  batch_size <- nrow(summaries)

  # Step 2: Convert to NumPy array
  summaries_np <- np$array(summaries, dtype = "float32")

  # Step 3: Call BayesFlow model for posterior samples
  samples_py <- tryCatch({
    sample_bf_model(
      bf_model = bf_model,
      summaries_np = summaries_np,
      n_samples = n_samples,
      target_param = target_params[1]
    )
  }, error = function(e) {
    cli::cli_abort(c(
      "BayesFlow sampling failed",
      "x" = e$message,
      "i" = "Check that the model is compatible with the summary statistics format"
    ))
  })

  # Step 4: Convert back to R matrix
  samples_r <- reticulate::py_to_r(samples_py)

  # Handle different output shapes
  # Expected shapes:
  # - [batch_size, n_samples] for single parameter

  # - [batch_size, n_samples, n_params] for multiple parameters
  # - dict with parameter names as keys
  if (is.list(samples_r) && !is.matrix(samples_r)) {
    # Dict output from workflow - extract target parameter
    if (target_params[1] %in% names(samples_r)) {
      samples_r <- samples_r[[target_params[1]]]
    } else if ("parameters" %in% names(samples_r)) {
      # Generic "parameters" key
      samples_r <- samples_r[["parameters"]]
    } else {
      # Use first element
      samples_r <- samples_r[[1]]
    }
  }

  # Ensure 2D matrix [batch_size, n_samples]
  if (length(dim(samples_r)) == 3) {
    # [batch, samples, params] -> select first param -> [batch, samples]
    samples_r <- samples_r[, , 1, drop = TRUE]
  }

  # Ensure proper dimensions
  if (length(dim(samples_r)) == 1 || is.null(dim(samples_r))) {
    # Single simulation - reshape to [1, n_samples]
    samples_r <- matrix(samples_r, nrow = 1)
  }

  # Verify dimensions
  if (nrow(samples_r) != batch_size) {
    cli::cli_warn(c(
      "BayesFlow output dimension mismatch",
      "Expected {batch_size} rows, got {nrow(samples_r)}",
      "i" = "Attempting to transpose"
    ))
    samples_r <- t(samples_r)
  }

  samples_r
}


# =============================================================================
# SINGLE ANALYSIS ESTIMATION
# =============================================================================

#' Estimate Single Analysis with BayesFlow Backend
#'
#' Performs single posterior estimation with no interim analyses using BayesFlow.
#' Supports both single data.frame (batch of 1) and list of data.frames (true batch).
#'
#' @param data Data frame with simulated observations OR list of data frames for batch
#' @param model BayesFlow/Keras model (Python object)
#' @param backend_args List of BayesFlow-specific arguments
#' @param target_params Character vector of parameter names
#' @param thresholds_success Numeric vector of success thresholds
#' @param thresholds_futility Numeric vector of futility thresholds
#' @param p_sig_scs Probability threshold for success
#' @param p_sig_ftl Probability threshold for futility
#' @param id_iter Iteration identifier (scalar or vector for batch)
#' @param id_cond Condition identifier (scalar or vector for batch)
#'
#' @return Data frame with results (1 row per simulation in batch)
#' @keywords internal
estimate_single_bf <- function(data, model, backend_args, target_params,
                                thresholds_success, thresholds_futility,
                                p_sig_scs, p_sig_ftl, id_iter, id_cond) {

  # Determine if this is a batch call (list of data.frames) or single call
  is_batch <- is.list(data) && !is.data.frame(data)

  if (is_batch) {
    # Batch processing: list of data.frames
    data_list <- data
    batch_size <- length(data_list)
    n_analyzed <- nrow(data_list[[1]])

    # Ensure id vectors are correct length
    if (length(id_iter) == 1) id_iter <- rep(id_iter, batch_size)
    if (length(id_cond) == 1) id_cond <- rep(id_cond, batch_size)

    # Prepare batch data
    data_batch <- prepare_data_list_as_batch_bf(data_list, backend_args)
  } else {
    # Single data.frame: wrap as batch of 1
    batch_size <- 1L
    n_analyzed <- nrow(data)

    # Prepare single as batch
    data_batch <- prepare_single_as_batch_bf(data, backend_args)
  }

  # Estimate posterior via BayesFlow
  draws_matrix <- tryCatch({
    estimate_batch_bf(data_batch, model, backend_args, target_params)
  }, error = function(e) {
    cli::cli_warn(c(
      "BayesFlow estimation failed",
      "x" = "Condition {id_cond[1]}, iteration {id_iter[1]}",
      "i" = "Error: {e$message}"
    ))
    return(NULL)
  })

  if (is.null(draws_matrix)) {
    # Create error results for all simulations in batch
    error_results <- lapply(seq_len(batch_size), function(i) {
      create_error_result(
        id_iter[i],
        id_cond[i],
        id_analysis = 0L,
        "BayesFlow estimation failed"
      )
    })
    return(dplyr::bind_rows(error_results))
  }

  # Resolve probability thresholds (info_frac = 1 for single-look designs)
  current_p_sig_scs <- resolve_threshold(p_sig_scs, 1)
  current_p_sig_ftl <- resolve_threshold(p_sig_ftl, 1)

  # Fast direct summarization (NOT through compute_measures)
  result <- summarize_post_bf(
    draws_mat = draws_matrix,
    target_param = target_params[1],  # Single param for now
    thr_scs = thresholds_success[1],
    thr_ftl = thresholds_futility[1],
    p_sig_scs = current_p_sig_scs,
    p_sig_ftl = current_p_sig_ftl,
    id_iter = id_iter,
    id_cond = id_cond,
    id_look = 0L,
    n_analyzed = n_analyzed
  )

  result
}


# =============================================================================
# SEQUENTIAL ANALYSIS ESTIMATION
# =============================================================================

#' Estimate Sequential Analysis with BayesFlow Backend
#'
#' Performs sequential interim analyses with BayesFlow backend using batch processing.
#' All simulations at the same interim step are processed together in a single
#' BayesFlow forward pass for efficiency.
#'
#' @param full_data_list List of complete datasets (one per simulation in batch)
#' @param model BayesFlow/Keras model (Python object)
#' @param backend_args List of BayesFlow-specific arguments
#' @param target_params Character vector of parameter names
#' @param thresholds_success Numeric vector of success thresholds
#' @param thresholds_futility Numeric vector of futility thresholds
#' @param p_sig_scs Probability threshold for success
#' @param p_sig_ftl Probability threshold for futility
#' @param analysis_at Vector of sample sizes for all analyses (including final)
#' @param interim_function Function to make interim decisions (optional)
#' @param id_iter Vector of iteration identifiers (one per sim in batch)
#' @param id_cond Vector of condition identifiers (one per sim in batch)
#'
#' @return Data frame with (batch_size x n_analyses) rows
#' @keywords internal
estimate_sequential_bf <- function(full_data_list, model, backend_args, target_params,
                                    thresholds_success, thresholds_futility,
                                    p_sig_scs, p_sig_ftl, analysis_at,
                                    interim_function, id_iter, id_cond) {

  batch_size <- length(full_data_list)
  n_total <- nrow(full_data_list[[1]])
  n_analyses <- length(analysis_at)

  # Initialize tracking for each sim
  stopped <- rep(FALSE, batch_size)
  stop_reason <- rep(NA_character_, batch_size)
  all_results <- vector("list", n_analyses)

  for (id_analysis in seq_along(analysis_at)) {
    current_n <- analysis_at[id_analysis]
    is_final <- id_analysis == n_analyses
    info_frac <- current_n / n_total

    # Resolve probability thresholds for this analysis
    current_p_sig_scs <- resolve_threshold(p_sig_scs, info_frac)
    current_p_sig_ftl <- resolve_threshold(p_sig_ftl, info_frac)

    # Identify active (non-stopped) simulations
    active_idx <- which(!stopped)
    if (length(active_idx) == 0 && !is_final) next

    # If all simulations have stopped but we're at final, still analyze for reporting
    if (length(active_idx) == 0) active_idx <- seq_len(batch_size)

    # Subset data to current analysis point for active simulations
    analysis_data_list <- lapply(full_data_list[active_idx], function(fd) fd[1:current_n, ])

    # Prepare batch data
    data_batch <- prepare_data_list_as_batch_bf(analysis_data_list, backend_args)

    # Single BayesFlow forward pass for all active simulations
    draws_matrix <- tryCatch({
      estimate_batch_bf(data_batch, model, backend_args, target_params)
    }, error = function(e) {
      cli::cli_warn(c(
        "BayesFlow batch estimation failed",
        "x" = "Analysis {id_analysis}",
        "i" = "Error: {e$message}"
      ))
      return(NULL)
    })

    if (is.null(draws_matrix)) {
      # Create error results for all active simulations
      error_results <- do.call(rbind, lapply(active_idx, function(i) {
        create_error_result(id_iter[i], id_cond[i], id_analysis, "BayesFlow estimation failed")
      }))
      all_results[[id_analysis]] <- error_results
      next
    }

    # Fast batch summarization
    batch_results <- summarize_post_bf(
      draws_mat = draws_matrix,
      target_param = target_params[1],
      thr_scs = thresholds_success[1],
      thr_ftl = thresholds_futility[1],
      p_sig_scs = current_p_sig_scs,
      p_sig_ftl = current_p_sig_ftl,
      id_iter = id_iter[active_idx],
      id_cond = id_cond[active_idx],
      id_look = id_analysis,
      n_analyzed = current_n
    )

    # Update stopping state (if not final)
    if (!is_final) {
      # Find newly stopped simulations
      new_stops_scs <- which(batch_results$dec_scs == 1)
      new_stops_ftl <- which(batch_results$dec_ftl == 1 & batch_results$dec_scs == 0)

      if (length(new_stops_scs) > 0) {
        stopped[active_idx[new_stops_scs]] <- TRUE
        stop_reason[active_idx[new_stops_scs]] <- "stop_success"
      }
      if (length(new_stops_ftl) > 0) {
        stopped[active_idx[new_stops_ftl]] <- TRUE
        stop_reason[active_idx[new_stops_ftl]] <- "stop_futility"
      }

      # Update batch_results with stopping information
      batch_results$stopped <- stopped[active_idx]
      batch_results$stop_reason <- stop_reason[active_idx]
    }

    all_results[[id_analysis]] <- batch_results
  }

  dplyr::bind_rows(all_results)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/backend_brms.R"
# =============================================================================
# BRMS BACKEND
# =============================================================================
# All brms-specific posterior estimation, extraction, and summarization.
# Provides complete brms pathway independent of other backends.
#
# Entry points:
#   - estimate_single_brms(): Single analysis estimation
#   - estimate_sequential_brms(): Sequential/interim analysis estimation
#
# Key functions:
#   - estimate_posterior_brms(): Fit brms model to data
#   - extract_posterior_rvars_brms(): Extract posterior as rvars
#   - summarize_post_brms(): Wrapper around compute_measures()
#
# Output: All functions produce results compatible with summarize_sims()

# =============================================================================
# POSTERIOR EXTRACTION
# =============================================================================

#' Extract Posterior Samples as rvars from brms Model
#'
#' Extracts posterior samples from a fitted brms model and converts them
#' to rvar format from the posterior package. This is the brms-specific
#' implementation of posterior extraction.
#'
#' @param brmsfit A fitted brmsfit object
#' @param target_params Character vector of parameter names to extract
#'
#' @return A named list of rvar objects, one per target parameter
#' @keywords internal
extract_posterior_rvars_brms <- function(brmsfit, target_params) {
  if (!inherits(brmsfit, "brmsfit")) {
    cli::cli_abort(c(
      "{.arg brmsfit} must be a fitted brms model object",
      "x" = "You supplied {.cls {class(brmsfit)}}",
      "i" = "Provide a fitted {.cls brmsfit} object"
    ))
  }

  # Extract all target parameters as rvars
  posterior_rvars <- brms::as_draws_rvars(brmsfit, variable = target_params)

  return(posterior_rvars)
}


# =============================================================================
# POSTERIOR ESTIMATION
# =============================================================================

#' Estimate Posterior Using brms Backend
#'
#' Fits a brms model to observed data by updating a pre-compiled template model.
#'
#' @param data Data frame containing the observed data
#' @param brms_model Pre-compiled brmsfit template model (chains = 0)
#' @param backend_args Named list of brms-specific arguments (chains, iter, cores, etc.)
#'
#' @return A fitted brmsfit object containing posterior samples
#' @keywords internal
estimate_posterior_brms <- function(data, brms_model, backend_args = list()) {
  if (!inherits(brms_model, "brmsfit")) {
    cli::cli_abort(c(
      "{.arg brms_model} must be a valid brmsfit object",
      "x" = "You supplied {.cls {class(brms_model)}}",
      "i" = "Provide a compiled {.cls brmsfit} template model"
    ))
  }

  # Merge with default brms args if not provided
  default_args <- list(
    object = brms_model,
    newdata = data
  )

  # User backend_args override defaults
  final_args <- modifyList(default_args, backend_args)

  # Fit model
  fitted_model <- do.call(stats::update, final_args)

  return(fitted_model)
}


# =============================================================================
# POSTERIOR SUMMARIZATION
# =============================================================================

#' Summarize Posterior - brms Backend
#'
#' Wrapper around [compute_measures()] for brms posterior output.
#' Provides consistent API with summarize_post_bf() (BayesFlow backend).
#'
#' @param posterior_rvars draws_rvars object from brms
#' @param target_params Character vector of parameter names
#' @param thresholds_success Numeric vector of success thresholds
#' @param thresholds_futility Numeric vector of futility thresholds
#' @param p_sig_scs Probability threshold for success
#' @param p_sig_ftl Probability threshold for futility
#'
#' @return Data frame with package output schema
#' @keywords internal
summarize_post_brms <- function(posterior_rvars, target_params,
                                 thresholds_success, thresholds_futility,
                                 p_sig_scs, p_sig_ftl) {
  # Delegate to existing compute_measures() function
  # which handles rvar operations
  compute_measures(
    posterior_rvars = posterior_rvars,
    target_params = target_params,
    thresholds_success = thresholds_success,
    thresholds_futility = thresholds_futility,
    p_sig_scs = p_sig_scs,
    p_sig_ftl = p_sig_ftl
  )
}


# =============================================================================
# SINGLE ANALYSIS ESTIMATION
# =============================================================================

#' Estimate Single Analysis with brms Backend
#'
#' Performs a single posterior estimation with no interim analyses using brms.
#' This is the simplest estimation strategy.
#'
#' @param data Data frame with simulated observations
#' @param model brmsfit template model
#' @param backend_args List of brms-specific arguments
#' @param target_params Character vector of parameter names
#' @param thresholds_success Numeric vector of success thresholds
#' @param thresholds_futility Numeric vector of futility thresholds
#' @param p_sig_scs Probability threshold for success
#' @param p_sig_ftl Probability threshold for futility
#' @param id_iter Iteration identifier
#' @param id_cond Condition identifier
#'
#' @return Data frame with 1 row containing measures and IDs
#' @keywords internal
estimate_single_brms <- function(data, model, backend_args, target_params,
                                 thresholds_success, thresholds_futility,
                                 p_sig_scs, p_sig_ftl,
                                 id_iter, id_cond) {

  # Estimate posterior
  estimation_result <- tryCatch({
    estimate_posterior_brms(
      data = data,
      brms_model = model,
      backend_args = backend_args
    )
  }, error = function(e) {
    cli::cli_warn(c(
      "brms estimation failed",
      "x" = "Condition {id_cond}, iteration {id_iter}",
      "i" = "Error: {e$message}"
    ))
    return(NULL)
  })

  if (is.null(estimation_result)) {
    return(create_error_result(id_iter, id_cond, id_analysis = 0L, "Estimation failed"))
  }

  # Extract posterior rvars
  posterior_rvars <- tryCatch({
    extract_posterior_rvars_brms(
      brmsfit = estimation_result,
      target_params = target_params
    )
  }, error = function(e) {
    cli::cli_warn(c(
      "Posterior extraction failed",
      "i" = "Error: {e$message}"
    ))
    return(NULL)
  })

  if (is.null(posterior_rvars)) {
    return(create_error_result(id_iter, id_cond, id_analysis = 0L, "Extraction failed"))
  }

  # Resolve probability thresholds (info_frac = 1 for single-look designs)
  current_p_sig_scs <- resolve_threshold(p_sig_scs, 1)
  current_p_sig_ftl <- resolve_threshold(p_sig_ftl, 1)

  # Compute measures using brms summarization
  result <- tryCatch({
    df <- summarize_post_brms(
      posterior_rvars = posterior_rvars,
      target_params = target_params,
      thresholds_success = thresholds_success,
      thresholds_futility = thresholds_futility,
      p_sig_scs = current_p_sig_scs,
      p_sig_ftl = current_p_sig_ftl
    ) |>
      dplyr::mutate(dplyr::across(-par_name, as.numeric))
    df |> dplyr::mutate(
      id_iter = id_iter,
      id_cond = id_cond,
      id_look = 0L,  # Single analysis
      converged = 1L,
      error_msg = NA_character_
    )
  }, error = function(e) {
    return(create_error_result(id_iter, id_cond, id_analysis = 0L, as.character(e)))
  })

  return(result)
}


# =============================================================================
# SEQUENTIAL ANALYSIS ESTIMATION
# =============================================================================

#' Estimate Sequential Analysis with brms Backend
#'
#' Performs sequential interim analyses with brms backend. Full dataset is simulated
#' once, then analyzed at multiple interim timepoints.
#'
#' @param full_data Complete simulated dataset (all n_total observations)
#' @param model brmsfit template model
#' @param backend_args List of brms-specific arguments
#' @param target_params Character vector of parameter names
#' @param thresholds_success Numeric vector of success thresholds
#' @param thresholds_futility Numeric vector of futility thresholds
#' @param p_sig_scs Probability threshold for success
#' @param p_sig_ftl Probability threshold for futility
#' @param analysis_at Vector of sample sizes for all analyses (including final at n_total)
#' @param interim_function Function to make interim decisions
#' @param id_iter Iteration identifier
#' @param id_cond Condition identifier
#'
#' @return Data frame with n_analyses rows (one per interim + final)
#' @importFrom dplyr if_else
#' @keywords internal
estimate_sequential_brms <- function(full_data, model, backend_args, target_params,
                                     thresholds_success, thresholds_futility,
                                     p_sig_scs, p_sig_ftl,
                                     analysis_at, interim_function,
                                     id_iter, id_cond) {

  n_total <- nrow(full_data)
  # analysis_at now includes the final analysis at n_total (last value = n_total)
  analysis_schedule <- analysis_at
  results_list <- list()
  stopped <- FALSE
  stop_reason <- NA_character_

  # Loop through each analysis timepoint sequentially
  for (id_analysis in seq_along(analysis_schedule)) {
    current_n <- analysis_schedule[id_analysis]
    is_final <- id_analysis == length(analysis_schedule)

    # Calculate information fraction for threshold resolution
    info_frac <- current_n / n_total

    # Resolve probability thresholds (handle function or numeric)
    current_p_sig_scs <- resolve_threshold(p_sig_scs, info_frac)
    current_p_sig_ftl <- resolve_threshold(p_sig_ftl, info_frac)

    # Subset data to current analysis point
    analysis_data <- full_data[1:current_n, ]

    # Estimate posterior
    estimation_result <- tryCatch({
      estimate_posterior_brms(
        data = analysis_data,
        brms_model = model,
        backend_args = backend_args
      )
    }, error = function(e) {
      cli::cli_warn(c(
        "brms estimation failed",
        "x" = "Analysis {id_analysis}",
        "i" = "Error: {e$message}"
      ))
      return(NULL)
    })

    if (is.null(estimation_result)) {
      results_list[[id_analysis]] <- create_error_result(
        id_iter, id_cond, id_analysis,
        "Estimation failed"
      )
      next
    }

    # Extract posterior rvars
    posterior_rvars <- tryCatch({
      extract_posterior_rvars_brms(
        brmsfit = estimation_result,
        target_params = target_params
      )
    }, error = function(e) {
      cli::cli_warn(c(
        "Posterior extraction failed",
        "i" = "Error: {e$message}"
      ))
      return(NULL)
    })

    if (is.null(posterior_rvars)) {
      results_list[[id_analysis]] <- create_error_result(
        id_iter, id_cond, id_analysis,
        "Extraction failed"
      )
      next
    }

    # Compute measures (using resolved probability thresholds)
    measures <- tryCatch({
      summarize_post_brms(
        posterior_rvars = posterior_rvars,
        target_params = target_params,
        thresholds_success = thresholds_success,
        thresholds_futility = thresholds_futility,
        p_sig_scs = current_p_sig_scs,
        p_sig_ftl = current_p_sig_ftl
      ) |>
        dplyr::mutate(dplyr::across(-par_name, as.numeric))
    }, error = function(e) {
      results_list[[id_analysis]] <- create_error_result(
        id_iter, id_cond, id_analysis,
        as.character(e)
      )
      return(NULL)
    })

    if (is.null(measures)) next

    # =============================================================================
    # STOPPING MECHANISM (Sequential Trial Early Stopping)
    # =============================================================================
    # Once stopped=TRUE, no further interim decisions are made, BUT analysis
    # continues to n_total to collect full data for safety/consistency metrics.
    # This design allows reporting: "would have stopped at n=100, final at n=200"
    #
    # Rationale: Complete data collection provides better understanding of trial
    # behavior even when stopping rule would have terminated early.
    #
    # Default stopping rule: dec_scs = 1 (success) or dec_ftl = 1 (futility)
    # Custom interim_function can override this with more complex logic.
    interim_decision <- NULL
    if (!is_final && !stopped) {
      if (!is.null(interim_function)) {
        # Custom stopping logic via interim_function
        interim_decision <- tryCatch({
          interim_function(
            interim_summaries = measures,
            current_n = current_n,
            analysis_at = current_n,
            n_total = n_total
          )
        }, error = function(e) {
          cli::cli_warn(c(
            "Interim function failed",
            "i" = "Error: {e$message}"
          ))
          list(decision = "continue", modified_params = NULL)
        })
      } else {
        # Default stopping rule: stop when dec_scs = 1 or dec_ftl = 1
        # Check the first target parameter (or union if multiple)
        dec_scs_val <- measures$dec_scs[1]
        dec_ftl_val <- measures$dec_ftl[1]

        if (!is.na(dec_scs_val) && dec_scs_val == 1) {
          interim_decision <- list(decision = "stop_success", modified_params = NULL)
        } else if (!is.na(dec_ftl_val) && dec_ftl_val == 1) {
          interim_decision <- list(decision = "stop_futility", modified_params = NULL)
        } else {
          interim_decision <- list(decision = "continue", modified_params = NULL)
        }
      }

      # Record stopping decision (loop continues but no more decisions made)
      if (interim_decision$decision %in% c("stop_success", "stop_futility")) {
        stopped <- TRUE
        stop_reason <- interim_decision$decision
      }
    }

    # Add IDs and interim information
    measures <- measures |>
      dplyr::mutate(
        id_iter = id_iter,
        id_cond = id_cond,
        id_look = id_analysis,
        n_analyzed = current_n,
        stopped = stopped,
        stop_reason = dplyr::if_else(stopped, stop_reason, NA_character_),
        interim_decision = if (!is.null(interim_decision)) list(interim_decision) else list(NULL),
        converged = 1L,
        error_msg = NA_character_
      )

    results_list[[id_analysis]] <- measures
  }

  # Combine all analyses
  dplyr::bind_rows(results_list)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/backends.R"
# =============================================================================
# BACKEND ABSTRACTION LAYER (DEPRECATED)
# =============================================================================
# DEPRECATION NOTICE: This file is deprecated as of 2025-11.
#
# All code has been moved to backend-specific files:
#   - R/backend_brms.R: All brms-specific estimation code
#   - R/backend_bf.R: All BayesFlow-specific estimation code
#   - R/model_cache.R: Model loading and caching
#   - R/utils_results.R: Shared utilities (create_error_result)
#
# This file is now empty and will be removed in a future version.
# =============================================================================
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/boundaries.R"
# =============================================================================
# STOPPING BOUNDARY FUNCTIONS
# =============================================================================
# Factory functions for creating look-dependent probability thresholds.
# These functions return boundary functions that take information fraction
# (current_n / n_total) and return the threshold to use at that look.
#
# Usage in build_design():
#   build_design(..., p_sig_scs = boundary_obf(0.975))

#' O'Brien-Fleming-like Bayesian Boundary
#'
#' Creates a boundary function that is stringent at early looks and relaxes
#' toward the final analysis. Approximates O'Brien-Fleming spending function
#' behavior in a Bayesian context.
#'
#' @param base Threshold at final analysis (info_frac = 1). Default 0.975.
#'
#' @return A function that takes information fraction and returns threshold.
#'
#' @details
#' Formula: `threshold = 1 - (1 - base) * sqrt(info_frac)`
#'
#' Example values for `base = 0.975`:
#' \itemize{
#'   \item info_frac = 0.25  threshold = 0.9875
#'   \item info_frac = 0.50  threshold = 0.9823
#'   \item info_frac = 0.75  threshold = 0.9783
#'
#'   \item info_frac = 1.00  threshold = 0.9750
#' }
#'
#' This boundary is very conservative at early looks (hard to stop early for
#' success) but relaxes to the nominal threshold at the final analysis.
#'
#' @export
#' @seealso [boundary_pocock()], [boundary_linear()], [boundary_power()]
#'
#' @examples
#' # Create OBF-style boundary
#' obf <- boundary_obf(0.975)
#'
#' # Evaluate at different information fractions
#' obf(0.25)  # Early look: ~0.9875
#' obf(0.50)  # Midpoint: ~0.9823
#' obf(1.00)  # Final: 0.975
#'
#' # Use in design
#' \dontrun{
#' design <- build_design(
#'   model = my_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = boundary_obf(0.975),
#'   p_sig_ftl = 0.90,
#'   analysis_at = c(0.5, 0.75)
#' )
#' }
boundary_obf <- function(base = 0.975) {
  force(base)

  if (!is.numeric(base) || length(base) != 1 || base < 0 || base > 1) {
    cli::cli_abort(c(
      "{.arg base} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {base}}"
    ))
  }

  f <- function(info_frac) {
    1 - (1 - base) * sqrt(info_frac)
  }

  # Add metadata for display

  attr(f, "boundary_type") <- "obf"
  attr(f, "boundary_params") <- list(base = base)
  class(f) <- c("boundary_function", "function")
  f
}


#' Pocock-like Boundary (Constant)
#'
#' Creates a constant boundary function that returns the same threshold
#' at all analysis timepoints. Equivalent to the Pocock spending function
#' approach.
#'
#' @param threshold Fixed threshold for all looks. Default 0.99.
#'
#' @return A function that takes information fraction and returns threshold.
#'
#' @details
#' Note: Using the same threshold at all interim analyses can inflate the
#' overall Type I error rate compared to a single-look design. Consider using
#' a higher threshold (e.g., 0.99) or [boundary_obf()] for better error control.
#'
#' @export
#' @seealso [boundary_obf()], [boundary_linear()], [boundary_power()]
#'
#' @examples
#' # Create constant boundary
#' pocock <- boundary_pocock(0.99)
#'
#' # Returns same value regardless of info_frac
#' pocock(0.25)  # 0.99
#' pocock(0.50)  # 0.99
#' pocock(1.00)  # 0.99
boundary_pocock <- function(threshold = 0.99) {
  force(threshold)

  if (!is.numeric(threshold) || length(threshold) != 1 ||
      threshold < 0 || threshold > 1) {
    cli::cli_abort(c(
      "{.arg threshold} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {threshold}}"
    ))
  }

  f <- function(info_frac) {
    threshold
  }

  # Add metadata for display
  attr(f, "boundary_type") <- "pocock"
  attr(f, "boundary_params") <- list(threshold = threshold)
  class(f) <- c("boundary_function", "function")
  f
}


#' Linear Boundary
#'
#' Creates a boundary function that interpolates linearly from a start value
#' at the first look to an end value at the final analysis.
#'
#' @param start Threshold at first analysis (info_frac approaching 0). Default 0.999.
#' @param end Threshold at final analysis (info_frac = 1). Default 0.975.
#'
#' @return A function that takes information fraction and returns threshold.
#'
#' @details
#' Formula: `threshold = start + (end - start) * info_frac`
#'
#' For success boundaries, typically `start > end` (stringent early, relaxed late).
#' For futility boundaries, typically `start < end` (lenient early, stringent late).
#'
#' @export
#' @seealso [boundary_obf()], [boundary_pocock()], [boundary_power()]
#'
#' @examples
#' # Success boundary: strict early, relaxed late
#' scs_boundary <- boundary_linear(start = 0.999, end = 0.975)
#' scs_boundary(0.5)  # 0.987
#'
#' # Futility boundary: lenient early, strict late
#' ftl_boundary <- boundary_linear(start = 0.70, end = 0.90)
#' ftl_boundary(0.5)  # 0.80
boundary_linear <- function(start = 0.999, end = 0.975) {
  force(start)
  force(end)

  if (!is.numeric(start) || length(start) != 1 || start < 0 || start > 1) {
    cli::cli_abort(c(
      "{.arg start} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {start}}"
    ))
  }
  if (!is.numeric(end) || length(end) != 1 || end < 0 || end > 1) {
    cli::cli_abort(c(
      "{.arg end} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {end}}"
    ))
  }

  f <- function(info_frac) {
    start + (end - start) * info_frac
  }

  # Add metadata for display
  attr(f, "boundary_type") <- "linear"
  attr(f, "boundary_params") <- list(start = start, end = end)
  class(f) <- c("boundary_function", "function")
  f
}


#' Power Family Boundary
#'
#' Creates a boundary function using a power transformation. Higher values of
#' `rho` create more O'Brien-Fleming-like behavior (conservative early).
#'
#' @param base Threshold at final analysis (info_frac = 1). Default 0.975.
#' @param rho Shape parameter controlling the boundary curve. Default 2.
#'   \itemize{
#'     \item `rho = 2`: Approximates O'Brien-Fleming
#'     \item `rho = 1`: Linear interpolation
#'     \item `rho = 0.5`: More Pocock-like (less conservative early)
#'   }
#'
#' @return A function that takes information fraction and returns threshold.
#'
#' @details
#' Formula: `threshold = 1 - (1 - base) * info_frac^(rho/2)`
#'
#' The `rho` parameter controls how quickly the boundary relaxes:
#' \itemize{
#'   \item Higher rho: More conservative early, faster relaxation later
#'   \item Lower rho: Less conservative early, slower relaxation
#' }
#'
#' @export
#' @seealso [boundary_obf()], [boundary_pocock()], [boundary_linear()]
#'
#' @examples
#' # Compare different rho values at info_frac = 0.5
#' boundary_power(0.975, rho = 3)(0.5)   # More conservative
#' boundary_power(0.975, rho = 2)(0.5)   # OBF-like
#' boundary_power(0.975, rho = 1)(0.5)   # Linear
#' boundary_power(0.975, rho = 0.5)(0.5) # Less conservative
boundary_power <- function(base = 0.975, rho = 2) {
  force(base)
  force(rho)

  if (!is.numeric(base) || length(base) != 1 || base < 0 || base > 1) {
    cli::cli_abort(c(
      "{.arg base} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {base}}"
    ))
  }
  if (!is.numeric(rho) || length(rho) != 1 || rho <= 0) {
    cli::cli_abort(c(
      "{.arg rho} must be a single positive numeric value",
      "x" = "You supplied {.val {rho}}"
    ))
  }

  f <- function(info_frac) {
    1 - (1 - base) * info_frac^(rho / 2)
  }

  # Add metadata for display
  attr(f, "boundary_type") <- "power"
  attr(f, "boundary_params") <- list(base = base, rho = rho)
  class(f) <- c("boundary_function", "function")
  f
}


#' Resolve Threshold Value
#'
#' Internal helper to resolve a threshold specification (numeric or function)
#' to a numeric value at a given information fraction.
#'
#' @param threshold Either a numeric value or a boundary function
#' @param info_frac Information fraction (current_n / n_total), between 0 and 1
#'
#' @return Numeric threshold value
#' @keywords internal
resolve_threshold <- function(threshold, info_frac) {
  if (is.function(threshold)) {
    threshold(info_frac)
  } else {
    threshold
  }
}


#' Resolve Boundary to Per-Look Vector
#'
#' Internal helper to resolve a boundary specification to a vector of
#' threshold values, one per analysis look.
#'
#' @param boundary Either NULL, a numeric value, a numeric vector, or a boundary function
#' @param look_info Data frame with id_look and n_analyzed columns
#' @param n_total Maximum planned sample size
#'
#' @return Numeric vector of thresholds, one per look
#' @keywords internal
resolve_boundary_vector <- function(boundary, look_info, n_total) {
  n_looks <- nrow(look_info)

  if (is.null(boundary)) {
    return(rep(NA_real_, n_looks))
  } else if (is.function(boundary)) {
    info_fracs <- look_info$n_analyzed / n_total
    sapply(info_fracs, boundary)
  } else if (length(boundary) == 1) {
    rep(boundary, n_looks)
  } else if (length(boundary) == n_looks) {
    boundary
  } else {
    cli::cli_abort(c(
      "Invalid boundary specification",
      "x" = "Got length {length(boundary)}, expected 1 or {n_looks}",
      "i" = "Provide: single value, vector of length {n_looks}, or boundary function"
    ))
  }
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/class_conditions.R"
# =============================================================================
# S7 CLASS DEFINITION: rctbp_conditions
# =============================================================================
# Stores parameter combinations for power analysis simulations. Creates an
# expanded grid of varying parameters combined with static values, then
# organizes arguments into simulation args (for data generation) and
# decision args (for analysis criteria).

#' @importFrom S7 new_class class_list class_any class_data.frame new_property
rctbp_conditions <- S7::new_class("rctbp_conditions",
  properties = list(
    conditions_grid = S7::class_data.frame,       # All parameter combinations
    condition_arguments = S7::class_list,         # Structured args per condition
    design = S7::class_any,                       # rctbp_design object
    condition_values = S7::class_list,            # User-specified varying params
    static_values = S7::class_list,               # User-specified constant params
    target_pwr = S7::new_property(                # Target power for optimal condition
      class = S7::class_any,                      # Allow NULL or numeric
      default = NULL
    )
  ),
  # Validator ensures grid and arguments are consistent
  validator = function(self) {
    # Validate conditions_grid has rows
    if (nrow(self@conditions_grid) == 0) {
      return("'conditions_grid' must have at least one row.")
    }

    # Validate one argument set per condition row
    if (length(self@condition_arguments) != nrow(self@conditions_grid)) {
      return("'condition_arguments' length must match 'conditions_grid' rows.")
    }

    # Validate design object
    if (!inherits(self@design, "rctbayespower::rctbp_design") && !inherits(self@design, "rctbp_design")) {
      return("'design' must be a valid rctbp_design object.")
    }

    # Validate target_pwr (NULL is allowed)
    if (!is.null(self@target_pwr)) {
      if (!is.numeric(self@target_pwr) || length(self@target_pwr) != 1 ||
          is.na(self@target_pwr) || self@target_pwr < 0 || self@target_pwr > 1) {
        return("'target_pwr' must be NULL or a single numeric value between 0 and 1.")
      }
    }

    NULL  # All validations passed
  }
)

# =============================================================================
# CONSTRUCTOR FUNCTION: build_conditions()
# =============================================================================
# Creates all parameter combinations from condition_values using expand_grid,
# combines with static_values, and separates into simulation and decision
# argument sets per condition.

#' Build Conditions for Power Analysis
#'
#' Creates a structured set of conditions and argument lists for power analysis
#' simulations. This function takes varying condition parameters and static
#' parameters, validates them against the design requirements, and creates
#' all necessary argument combinations for simulation runs.
#'
#' @param design An rctbp_design object that defines the study design
#' @param condition_values A named list where each element contains vectors of
#'   parameter values to vary across conditions. All combinations will be created.
#' @param static_values A named list of parameter values that remain constant
#'   across all conditions
#' @param target_pwr Target power level for identifying optimal conditions
#'   (default NULL). If NULL, the condition with highest power is shown.
#'   If specified (0 to 1), finds the smallest sample size achieving at
#'   least this power when displaying results.
#'
#' @return An S7 object of class "rctbp_conditions" containing:
#'   \item{conditions_grid}{A data.frame with all parameter combinations}
#'   \item{condition_arguments}{A list of argument lists for each condition,
#'     separated into simulation and interim analysis arguments}
#'   \item{design}{The original rctbp_design object}
#'   \item{condition_values}{The original condition_values list}
#'   \item{static_values}{The original static_values list}
#'   \item{target_pwr}{Target power level for optimal condition identification}
#'
#' @details The function performs several validation steps:
#' \itemize{
#'   \item Checks that condition_values and static_values don't have overlapping names
#'   \item Validates that all required parameters are provided
#'   \item Ensures p_alloc is properly specified as a list
#'   \item Creates expanded grid of all condition combinations
#' }
#'
#' \strong{Interim Analysis Inheritance:}
#' Interim analysis parameters (`analysis_at`, `interim_function`, `adaptive`) are
#' inherited from the design object as defaults. This means:
#' \itemize{
#'   \item If specified in `build_design()`, they apply to all conditions automatically
#'   \item Users can override per-condition via `condition_values` or `static_values`
#'   \item If not specified anywhere, defaults to single-look design (no interim analyses)
#' }
#' This allows specifying interim specs once at design level while still permitting
#' condition-specific overrides when needed.
#'
#' @examples
#' \dontrun{
#' # Create conditions for sample size and effect size analysis
#' conditions <- build_conditions(
#'   design = my_design,
#'   condition_values = list(
#'     n_total = c(100, 200, 300),
#'     effect_size = c(0.2, 0.5, 0.8)
#'   ),
#'   static_values = list(
#'     p_alloc = list(c(0.5, 0.5)),
#'     baseline_effect = 0.1
#'   )
#' )
#'
#' # Print the conditions
#' print(conditions)
#' }
#'
#' @export
build_conditions <- function(design, condition_values, static_values, target_pwr = NULL) {
  # validate design (allow both namespaced and non-namespaced class for testing)
  if (!inherits(design, "rctbayespower::rctbp_design") && !inherits(design, "rctbp_design")) {
    cli::cli_abort(c(
      "{.arg design} must be a valid rctbp_design object",
      "x" = "Got object of class {.cls {class(design)}}",
      "i" = "Use {.fn build_design} to create a valid design object"
    ))
  }

  # validate inputs
  if (!is.list(condition_values)) {
    cli::cli_abort(c(
      "{.arg condition_values} must be a list",
      "x" = "You supplied {.type {condition_values}}"
    ))
  }
  if (!is.list(static_values)) {
    cli::cli_abort(c(
      "{.arg static_values} must be a list",
      "x" = "You supplied {.type {static_values}}"
    ))
  }

  # validate target_pwr (NULL is allowed)
  if (!is.null(target_pwr)) {
    if (!is.numeric(target_pwr) || length(target_pwr) != 1 ||
        is.na(target_pwr) || target_pwr < 0 || target_pwr > 1) {
      cli::cli_abort(c(
        "{.arg target_pwr} must be NULL or a single numeric value between 0 and 1",
        "x" = "You supplied {.val {target_pwr}}"
      ))
    }
  }

  # gather provided parameter names
  params_given <- c(names(condition_values), names(static_values))

  # check for overlapping names between condition_values and static_values
  params_overlap <- intersect(names(condition_values), names(static_values))
  if (length(params_overlap) > 0) {
    cli::cli_abort(c(
      "Redundant parameters found in both {.arg condition_values} and {.arg static_values}",
      "x" = "Overlapping parameters: {.val {params_overlap}}",
      "i" = "Each parameter must appear in only one list"
    ))
  }
  # check for duplicated parameter names overall (within or across lists)
  params_redundant <- unique(params_given[duplicated(params_given)])
  if (length(params_redundant) > 0) {
    cli::cli_abort(c(
      "Duplicated parameter names detected",
      "x" = "Duplicated parameters: {.val {params_redundant}}",
      "i" = "Each parameter name must be unique"
    ))
  }

  # required parameters
  params_needed <- required_fn_args(design, print = FALSE)

  # Sensible defaults for non-sequential trial design
  # Parameters that have defaults and don't need to be specified by user
  params_with_defaults <- c("analysis_at", "interim_function", "adaptive")

  # Exclude parameters with defaults from required validation
  params_required <- setdiff(params_needed$params_all, params_with_defaults)

  # check for missing param values (excluding those with defaults)
  if (!all(params_required %in% params_given)) {
    missing_params <- setdiff(params_required, params_given)
    cli::cli_abort(c(
      "Missing required parameters",
      "x" = "The following parameters must be specified: {.val {missing_params}}",
      "i" = "Add these to {.arg condition_values} or {.arg static_values}"
    ))
  }

  # Merge inputs
  all_values <- c(condition_values, static_values)

  # Ensure p_alloc is wrapped in a list for proper grid expansion
  # Rationale: p_alloc is a vector (e.g., c(0.5, 0.5)), but expand_grid treats
  # vectors as multiple conditions. Wrapping as list(c(0.5, 0.5)) keeps it intact.
  if ("p_alloc" %in% names(condition_values) &&
      !is.list(all_values[["p_alloc"]])) {
    condition_values$p_alloc <- list(condition_values$p_alloc)
  }
  if ("p_alloc" %in% names(static_values) &&
      !is.list(all_values[["p_alloc"]])) {
    static_values$p_alloc <- list(static_values$p_alloc)
  }

  # =============================================================================
  # GRID EXPANSION & ARGUMENT ORGANIZATION
  # =============================================================================
  # Algorithm:
  # 1. Create all combinations of condition_values (Cartesian product)
  # 2. Add condition IDs for tracking
  # 3. For each condition row, extract varying params and merge with static
  # 4. Separate params into sim_args (data generation) and decision_args (analysis)
  # 5. Apply defaults for optional decision parameters

  # Create condition grid (Cartesian product of all condition_values)
  df_grid <- do.call(tidyr::expand_grid, condition_values)
  df_grid <- tibble::rowid_to_column(df_grid, var = "id_cond")

  # Convert each row to a list for processing
  condition_arguments_flat <- apply(df_grid, 1, as.list)

  # Combine simulation and decision arguments per condition
  condition_arguments <- lapply(condition_arguments_flat, function(condition) {
    # --- Simulation arguments ---
    sim_args <- list()
    for (param in params_needed$params_sim) {
      if (param %in% names(condition)) {
        sim_args[[param]] <- condition[[param]]
      } else if (param %in% names(static_values)) {
        sim_args[[param]] <- static_values[[param]]
      } else {
        cli::cli_abort(c(
          "Missing simulation parameter: {.val {param}}",
          "i" = "Add {.val {param}} to {.arg condition_values} or {.arg static_values}"
        ))
      }
    }

    # --- Decision arguments (per-condition) ---
    decision_args <- list()

    # Inherit interim parameters from design as defaults
    # Rationale: Design-level interim specs apply to all conditions unless overridden.
    # Users can override per-condition via condition_values or static_values.
    # If design doesn't specify, fall back to single-look defaults (NULL/FALSE).
    #
    # NOTE: interim_function = NULL is valid for sequential monitoring without stopping.
    # The estimation functions handle NULL by simply not making stopping decisions.
    decision_defaults <- list(
      analysis_at = design@analysis_at,           # Inherit from design (NULL = final only)
      interim_function = design@interim_function, # Inherit from design (NULL = no stopping rules)
      adaptive = design@adaptive                  # Inherit from design (FALSE = non-adaptive)
    )

    for (param in params_needed$params_decision) {
      if (param %in% names(condition)) {
        decision_args[[param]] <- condition[[param]]
      } else if (param %in% names(static_values)) {
        decision_args[[param]] <- static_values[[param]]
      } else if (param %in% names(decision_defaults)) {
        # Apply default for non-sequential parameters
        decision_args[[param]] <- decision_defaults[[param]]
      } else {
        cli::cli_abort(c(
          "Missing decision parameter: {.val {param}}",
          "i" = "Add {.val {param}} to {.arg condition_values} or {.arg static_values}"
        ))
      }
    }

    # Process analysis_at: convert proportions to sample sizes
    if (!is.null(decision_args$analysis_at)) {
      n_total <- sim_args$n_total
      if (is.null(n_total)) {
        cli::cli_abort(c(
          "'n_total' is required when 'analysis_at' is specified",
          "i" = "Add 'n_total' to {.arg condition_values} or {.arg static_values}"
        ))
      }

      # Convert proportions to integer sample sizes
      # analysis_at is validated as proportions in (0, 1] with last value = 1
      decision_args$analysis_at <- as.integer(round(decision_args$analysis_at * n_total))
    }

    # Return both sets of args
    list(
      id_cond = condition$id_cond,
      sim_args = sim_args,
      decision_args = decision_args
    )
  })


  # Create S7 object - validation happens automatically
  conditions_obj <- rctbp_conditions(
    conditions_grid = df_grid,
    condition_arguments = condition_arguments,
    design = design,
    condition_values = condition_values,
    static_values = static_values,
    target_pwr = target_pwr
  )

  return(conditions_obj)
}

# =============================================================================
# S7 METHOD: print()
# =============================================================================
# Displays conditions grid showing all parameter combinations and summary info.

#' Print Method for rctbp_conditions Objects
#'
#' Prints a formatted summary of condition grids created by [build_conditions()].
#' Shows the condition grid with all parameter combinations and provides
#' summary information about the number of conditions and parameters.
#'
#' @param x An S7 object of class "rctbp_conditions" created by [build_conditions()]
#' @param ... Additional arguments passed to [print()]
#'
#' @return Invisibly returns the input object
#' @importFrom S7 method
#' @name print.rctbp_conditions
#' @export
#'
#' @examples
#' \dontrun{
#' conditions <- build_conditions(design, condition_values, static_values)
#' print(conditions) # or just: conditions
#' }
#'
#' @export
S7::method(print, rctbp_conditions) <- function(x, ...) {
  report <- build_report.rctbp_conditions(x)
  render_report(report)
  invisible(x)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/class_design.R"
# =============================================================================
# S7 CLASS DEFINITION: rctbp_design
# =============================================================================
# Links a rctbp_model with global analysis configuration. Stores WHICH
# parameters to analyze and WHAT probability thresholds define success/futility.
# Note: Effect size thresholds (e.g., treatment effect > 0.2) are specified
# per-condition in build_conditions(), not here.

#' @importFrom S7 new_class class_character class_numeric class_integer class_function class_any new_union new_property
rctbp_design <- S7::new_class(
  "rctbp_design",
  properties = list(
    # Core design properties
    model = S7::class_any,                  # rctbp_model object
    target_params = S7::class_character,    # Parameters to analyze (must match model@parameter_names_brms)
    p_sig_scs = S7::class_numeric | S7::class_function,  # Probability threshold or boundary function
    p_sig_ftl = S7::class_numeric | S7::class_function,  # Probability threshold or boundary function
    design_name = S7::class_character | NULL,

    # Interim analysis properties (design-level defaults, can be overridden per-condition)
    analysis_at = S7::new_property(
      class = S7::class_numeric | NULL,
      default = NULL
    ),
    interim_function = S7::new_property(
      class = S7::class_function | NULL,
      default = NULL
    ),
    adaptive = S7::new_property(
      class = S7::class_logical,
      default = FALSE
    )
  ),
  # Validator ensures target_params are valid for the model and probability
  # thresholds are between 0 and 1
  validator = function(self) {
    # Validate model object type
    if (!inherits(self@model, "rctbayespower::rctbp_model")) {
      return("'model' must be a valid rctbp_model object.")
    }

    # Check required parameters are provided
    if (is.null(self@target_params) ||
        length(self@target_params) == 0) {
      return("'target_params' cannot be NULL or empty.")
    }
    if (is.null(self@p_sig_scs)) {
      return("'p_sig_scs' cannot be NULL.")
    }
    if (is.null(self@p_sig_ftl)) {
      return("'p_sig_ftl' cannot be NULL.")
    }

    # Validate target_params exist in brms model
    # This ensures we can extract posteriors for these parameters
    if (!all(self@target_params %in% self@model@parameter_names_brms)) {
      return(paste(
        "'target_params' must be a subset of the parameter names in the model:",
        paste(self@model@parameter_names_brms, collapse = ", ")
      ))
    }

    # Validate probability thresholds: numeric [0, 1] or boundary function
    if (is.function(self@p_sig_scs)) {
      # Validate function returns valid probability at test point
      test_val <- tryCatch(self@p_sig_scs(0.5), error = function(e) NA)
      if (is.na(test_val) || !is.numeric(test_val) || length(test_val) != 1 ||
          test_val < 0 || test_val > 1) {
        return("'p_sig_scs' function must return a single numeric value between 0 and 1")
      }
    } else if (!is.numeric(self@p_sig_scs) || length(self@p_sig_scs) != 1 ||
               self@p_sig_scs < 0 || self@p_sig_scs > 1) {
      return("'p_sig_scs' must be a numeric value between 0 and 1, or a boundary function")
    }

    if (is.function(self@p_sig_ftl)) {
      # Validate function returns valid probability at test point
      test_val <- tryCatch(self@p_sig_ftl(0.5), error = function(e) NA)
      if (is.na(test_val) || !is.numeric(test_val) || length(test_val) != 1 ||
          test_val < 0 || test_val > 1) {
        return("'p_sig_ftl' function must return a single numeric value between 0 and 1")
      }
    } else if (!is.numeric(self@p_sig_ftl) || length(self@p_sig_ftl) != 1 ||
               self@p_sig_ftl < 0 || self@p_sig_ftl > 1) {
      return("'p_sig_ftl' must be a numeric value between 0 and 1, or a boundary function")
    }

    # Validate interim analysis properties
    if (!is.null(self@analysis_at)) {
      # analysis_at must be monotonically increasing
      if (length(self@analysis_at) > 1 &&
          !all(diff(self@analysis_at) > 0)) {
        return("'analysis_at' must be monotonically increasing (each value greater than previous)")
      }

      # analysis_at must be proportions in (0, 1], with last value = 1
      # (1 is auto-appended in build_design() if missing)
      if (!all(self@analysis_at > 0 & self@analysis_at <= 1)) {
        return("'analysis_at' must be proportions in (0, 1] (e.g., c(0.5, 0.75, 1))")
      }

      # Last value must be 1 (final analysis at n_total)
      if (self@analysis_at[length(self@analysis_at)] != 1) {
        return("'analysis_at' must end with 1 (final analysis at n_total)")
      }
    }

    # Validate interim_function and analysis_at compliance
    # Rule: interim_function requires analysis_at (can't decide without knowing when)
    if (!is.null(self@interim_function) && is.null(self@analysis_at)) {
      return("'interim_function' requires 'analysis_at' to specify when interim analyses occur")
    }
    # Note: analysis_at without interim_function is allowed and valid.
    # This uses the default stopping rule: stop when dec_scs = 1 or dec_ftl = 1
    # (i.e., when posterior probabilities exceed p_sig thresholds).

    # Validate adaptive requires interim analysis setup
    if (isTRUE(self@adaptive) && is.null(self@analysis_at)) {
      return("'adaptive = TRUE' requires 'analysis_at' to specify interim timepoints")
    }

    # Validate adaptive is logical
    if (!is.logical(self@adaptive) || length(self@adaptive) != 1) {
      return("'adaptive' must be a single logical value (TRUE or FALSE)")
    }

    NULL  # All validations passed
  }
)

# =============================================================================
# CONSTRUCTOR FUNCTION: build_design()
# =============================================================================
# User-friendly interface to create rctbp_design objects with validation.

#' Constructor for rctbp_design Objects
#'
#' Creates an S7 rctbp_design object that combines a rctbp_model with analysis
#' configuration parameters for Bayesian power analysis. This constructor function
#' provides a user-friendly interface to the S7 class constructor.
#'
#' @param model An S7 object of class "rctbp_model" created by [build_model()]
#' @param target_params Character vector specifying which model parameters to
#'   analyze for power. Must be valid parameter names from the brms model.
#'   Use `model@parameter_names_brms` to discover available names. Required.
#' @param p_sig_scs Probability threshold for declaring success. Can be:
#'   \itemize{
#'     \item A numeric value between 0 and 1 (same threshold at all looks)
#'     \item A boundary function from [boundary_obf()], [boundary_linear()],
#'       [boundary_pocock()], or [boundary_power()] that takes information
#'       fraction (current_n / n_total) and returns the threshold
#'   }
#'   For sequential designs, function-valued thresholds enable look-dependent
#'   stopping boundaries (e.g., O'Brien-Fleming-style: stringent early, relaxed late).
#'   Typically 0.975 or 0.95 for fixed threshold. Required.
#' @param p_sig_ftl Probability threshold for declaring futility. Can be:
#'   \itemize{
#'     \item A numeric value between 0 and 1 (same threshold at all looks)
#'     \item A boundary function that takes information fraction and returns threshold
#'   }
#'   For sequential designs with futility monitoring, consider using
#'   [boundary_linear()] with increasing thresholds (lenient early, stringent late).
#'   Typically 0.5 for fixed threshold. Required.
#' @param design_name Optional character string providing a descriptive name for the
#'   design. Defaults to NULL.
#' @param analysis_at Optional numeric vector of proportions in (0, 1] specifying
#'   when analyses occur as fractions of n_total. Example: `c(0.5, 0.75)` for
#'   interim analyses at 50% and 75% of n_total. The final analysis at 1 (100%)
#'   is auto-appended if not included. Must be monotonically increasing.
#'   Converted to actual sample sizes in [build_conditions()] using n_total.
#'   Set to NULL (default) for single-look designs with no interim analyses.
#' @param interim_function Optional function for custom interim stopping decisions. If NULL
#'   (default), the default stopping rule is used: stop for success when `dec_scs = 1`
#'   (i.e., `pr_scs >= p_sig_scs`) or stop for futility when `dec_ftl = 1`
#'   (i.e., `pr_ftl >= p_sig_ftl`).
#'   If provided, must accept parameters: `interim_summaries`, `current_n`, `analysis_at`,
#'   `n_total`. Should return a list with `decision` ("continue", "stop_success",
#'   "stop_futility") and optionally `modified_params` (for adaptive designs).
#'   See [interim_futility_only()] and [interim_success_futility()] for helper factories
#'   that create custom stopping rules (e.g., different thresholds at different looks).
#' @param adaptive Logical indicating whether the design allows parameter modification
#'   between interim looks (default FALSE). When TRUE, the `interim_function` can return
#'   modified simulation parameters (e.g., updated allocation ratios). Requires
#'   `interim_function` to be specified.
#'
#' @details
#' The rctbp_design class combines model specifications with global analysis
#' configuration parameters. It defines WHICH parameters to analyze and WHAT
#' probability thresholds to use for decision-making.
#'
#' \strong{Important:} The actual success and futility thresholds (e.g., 0.2 for
#' treatment effect) are NOT stored in the design object. They are specified
#' per-condition when creating the conditions object using build_conditions().
#' The design only stores the probability thresholds (p_sig_scs, p_sig_ftl)
#' that determine how certain we need to be about exceeding those thresholds.
#'
#' \strong{Model Integration:} Links to the rctbp_model object containing data
#' simulation function and compiled brms model.
#'
#' \strong{Target Parameters:} Specifies which model parameters to analyze for power.
#' Parameter names are model-dependent. Use `model@parameter_names_brms` to discover
#' available parameters for your specific model.
#'
#' \strong{Probability Thresholds:} The p_sig_scs and p_sig_ftl parameters
#' control the certainty required for success/futility decisions. Higher values require
#' stronger evidence.
#'
#' \strong{Interim Analysis:} The `analysis_at`, `interim_function`, and `adaptive` parameters
#' specify design-level defaults for group sequential designs. These can be overridden
#' per-condition in [build_conditions()] via `condition_values` or `static_values`.
#'
#' \strong{Design Types:}
#' \itemize{
#'   \item \emph{Single-look:} `analysis_at = NULL` - analyze only at final n_total
#'   \item \emph{Sequential (default stopping):} `analysis_at` specified, `interim_function = NULL` -
#'     stops when `dec_scs = 1` or `dec_ftl = 1` (based on p_sig thresholds)
#'   \item \emph{Sequential (custom stopping):} `analysis_at` + `interim_function` - custom
#'     stopping rules (e.g., different thresholds at different looks)
#'   \item \emph{Adaptive:} `analysis_at` + `interim_function` + `adaptive = TRUE` - can
#'     modify simulation parameters between looks (e.g., response-adaptive randomization)
#' }
#'
#' \strong{Validation:} All parameters are validated for consistency with the
#' underlying model structure.
#'
#' @return An S7 object of class "rctbp_design" containing:
#' \describe{
#'   \item{model}{The rctbp_model object}
#'   \item{target_params}{Target parameters for analysis}
#'   \item{p_sig_scs}{Success probability threshold}
#'   \item{p_sig_ftl}{Futility probability threshold}
#'   \item{design_name}{Optional descriptive name}
#'   \item{analysis_at}{Interim analysis sample sizes (NULL for single-look)}
#'   \item{interim_function}{Interim decision function (NULL if none)}
#'   \item{adaptive}{Whether design allows parameter modification}
#' }
#'
#' @export
#' @seealso [build_model()], [build_conditions()]
#'
#' @examples
#' \dontrun{
#' # Create an ANCOVA model
#' ancova_model <- build_model("ancova_cont_2arms")
#'
#' # Simple design (no interim analysis)
#' simple_design <- build_design(
#'   model = ancova_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = 0.975,
#'   p_sig_ftl = 0.5
#' )
#'
#' # Design with interim analysis (1 is auto-appended for final analysis)
#' sequential_design <- build_design(
#'   model = ancova_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = 0.975,
#'   p_sig_ftl = 0.5,
#'   analysis_at = c(0.5, 0.75),  # Interim at 50%, 75%; final at 100% (auto-appended)
#'   interim_function = interim_futility_only(futility_threshold = 0.90)
#' )
#' }
build_design <- function(model,
                         target_params,
                         p_sig_scs,
                         p_sig_ftl,
                         design_name = NULL,
                         analysis_at = NULL,
                         interim_function = NULL,
                         adaptive = FALSE) {
  # Auto-append 1 to analysis_at if not present (final analysis at 100% of n_total)
  if (!is.null(analysis_at)) {
    if (analysis_at[length(analysis_at)] != 1) {
      analysis_at <- c(analysis_at, 1)
    }
  }

  # Use S7 constructor directly - all validation happens in the class validator
  rctbp_design(
    model = model,
    target_params = target_params,
    p_sig_scs = p_sig_scs,
    p_sig_ftl = p_sig_ftl,
    design_name = design_name,
    analysis_at = analysis_at,
    interim_function = interim_function,
    adaptive = adaptive
  )
}

# =============================================================================
# S7 METHOD: print()
# =============================================================================
# Formats rctbp_design objects showing both model specifications and
# analysis configuration parameters.

#' Print Method for rctbp_design Objects
#'
#' Displays a comprehensive summary of a rctbp_design object, showing
#' both model specifications and analysis configuration in an organized format.
#'
#' @param x An S7 object of class "rctbp_design"
#' @param ... Additional arguments (currently unused)
#'
#' @return Invisibly returns the input object. Used for side effects (printing).
#' @importFrom S7 method
#' @name print.rctbp_design
#' @export
S7::method(print, rctbp_design) <- function(x, ...) {
  report <- build_report.rctbp_design(x)
  render_report(report)
  invisible(x)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/class_interim.R"
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/class_model.R"
# =============================================================================
# S7 CLASS DEFINITION: rctbp_model
# =============================================================================
# Encapsulates all components needed for power analysis simulation including
# data simulation function, posterior estimation models (brms AND/OR BayesFlow),
# and trial metadata (endpoints, arms, repeated measures).
#
# Dual Backend Support:
#   - Models can have brms_model, bayesflow_model, or BOTH
#   - Single backend property: "brms" or "bf" (resolved at creation time)
#   - Use get_model() with backend="auto" for automatic selection

#' @importFrom S7 new_class class_character class_numeric class_integer class_function class_any new_union new_property
rctbp_model <- S7::new_class(
  "rctbp_model",
  properties = list(
    # Core components
    data_simulation_fn = S7::class_function,  # Generates trial data
    brms_model = S7::class_any | NULL,        # Template brmsfit object (backend = "brms")
    bayesflow_model = S7::class_any | NULL,   # Neural posterior model (backend = "bf")

    # Backend selection: "brms" or "bf" (resolved, not "auto")
    # Resolution happens in get_model() or build_model()
    backend = S7::new_property(
      class = S7::class_character,
      default = "brms",
      setter = function(self, value) {
        if (!value %in% c("brms", "bf")) {
          cli::cli_abort(c(
            "{.arg backend} must be 'brms' or 'bf'",
            "x" = "You supplied {.val {value}}",
            "i" = "Use {.fn get_model} with {.arg backend} = 'auto' for automatic selection"
          ))
        }
        self@backend <- value
        self
      }
    ),

    # Backend-specific arguments
    backend_args_brms = S7::new_property(S7::class_list, default = list()),
    backend_args_bf = S7::new_property(
      S7::class_list,
      default = list(batch_size = 64L, n_posterior_samples = 1000L)
    ),

    # Model metadata
    predefined_model = S7::new_union(S7::class_character, NULL),  # Name if using predefined
    model_name = S7::class_character,
    n_endpoints = S7::class_numeric,
    endpoint_types = S7::class_character,     # "continuous", "binary", or "count"
    n_arms = S7::class_numeric,
    n_repeated_measures = S7::class_numeric | NULL,

    # Computed property: simulation parameter names
    parameter_names_sim_fn = S7::new_property(
      class = S7::class_character,
      getter = function(self) {
        # Extract parameter names from data simulation function signature
        names(formals(self@data_simulation_fn))
      }
    ),

    # Computed property: brms parameter names
    parameter_names_brms = S7::new_property(
      class = S7::class_character,
      getter = function(self) {
        # Extract fixed effects parameter names from brms model (prefix "b_")
        if (!is.null(self@brms_model)) {
          stringr::str_subset(brms::variables(self@brms_model), pattern = "^b_")
        } else {
          character(0)
        }
      }
    )
  ),
  # Validator ensures object consistency and catches configuration errors early
  validator = function(self) {
    # Validate at least one backend model is provided
    has_brms <- !is.null(self@brms_model)
    has_bayesflow <- !is.null(self@bayesflow_model)

    if (!has_brms && !has_bayesflow) {
      return("At least one of 'brms_model' or 'bayesflow_model' must be provided.")
    }

    # Validate model type if provided
    if (has_brms && !inherits(self@brms_model, "brmsfit")) {
      return("'brms_model' must be a valid brmsfit object.")
    }

    # Validate backend_args are lists
    if (!is.list(self@backend_args_brms)) {
      return("'backend_args_brms' must be a list.")
    }
    if (!is.list(self@backend_args_bf)) {
      return("'backend_args_bf' must be a list.")
    }

    # Validate backend value (only "brms" or "bf" allowed, resolved at creation)
    if (!self@backend %in% c("brms", "bf")) {
      return("'backend' must be 'brms' or 'bf' (resolved at model creation).")
    }

    # Validate backend matches available model
    if (self@backend == "brms" && !has_brms) {
      return("Backend 'brms' selected but no brms_model available.")
    }
    if (self@backend == "bf" && !has_bayesflow) {
      return("Backend 'bf' selected but no bayesflow_model available.")
    }

    # Validate trial structure parameters
    if (length(self@n_endpoints) != 1 || self@n_endpoints <= 0) {
      return("'n_endpoints' must be a positive numeric value.")
    }
    if (length(self@endpoint_types) != self@n_endpoints ||
        any(!self@endpoint_types %in% c("continuous", "binary", "count"))) {
      return(
        "'endpoint_types' must be a character vector of length 'n_endpoints' with valid types."
      )
    }
    if (length(self@n_arms) != 1 || self@n_arms <= 0) {
      return("'n_arms' must be a positive numeric value.")
    }
    if (!is.null(self@n_repeated_measures) &&
        (length(self@n_repeated_measures) != 1 ||
         self@n_repeated_measures < 0)) {
      return("'n_repeated_measures' must be a non-negative numeric value.")
    }

    # Validate data_simulation_fn has required parameters
    required_params <- c("n_total", "p_alloc")
    if (!all(required_params %in% self@parameter_names_sim_fn)) {
      return("'data_simulation_fn' must have parameters 'n_total' and 'p_alloc'.")
    }

    NULL  # All validations passed
  }
)

# =============================================================================
# CONSTRUCTOR FUNCTIONS: get_model() and build_model()
# =============================================================================
# Two distinct functions for different use cases:
#   - get_model(): Retrieve predefined models with backend configuration
#   - build_model(): Create custom models from components (advanced users)

#' Get a Predefined Model
#'
#' Retrieves a predefined model by name with automatic backend configuration.
#' This is the recommended way to get models for standard analyses.
#'
#' @param name Character. Name of the predefined model. Use [list_predefined_models()]
#'   to see available options. Currently supported:
#'   \itemize{
#'     \item "ancova_cont_2arms" - ANCOVA for continuous outcomes, 2 arms
#'     \item "ancova_cont_3arms" - ANCOVA for continuous outcomes, 3 arms
#'   }
#' @param backend Character. Backend to use: "auto" (default), "brms", or "bf".
#'   \itemize{
#'     \item "auto": Try BayesFlow first, fall back to brms silently
#'     \item "brms": Use brms only (never loads BayesFlow)
#'     \item "bf": Try BayesFlow, fall back to brms with warning
#'   }
#'
#' @return An S7 object of class "rctbp_model" with only the active backend's model loaded
#'
#' @details
#' Backend resolution happens at call time, not at analysis time. The returned
#' model has a single `backend` property set to either "brms" or "bf" (never "auto"),
#' and only the active backend's model is loaded (not both).
#'
#' When `backend = "auto"`:
#' - Tries BayesFlow first; if available and model exists, uses BayesFlow
#' - Otherwise silently falls back to brms (no warning)
#'
#' When `backend = "bf"`:
#' - Tries BayesFlow first; if successful, uses BayesFlow
#' - If BayesFlow unavailable, falls back to brms with a warning message
#'
#' When `backend = "brms"`:
#' - Uses brms only; never attempts to load BayesFlow
#'
#' @export
#' @seealso [build_model()] for custom models, [list_predefined_models()],
#'   [add_bf_backend()], [check_bf_available()]
#'
#' @examples
#' # Get model with default backend (brms, or bf if available)
#' model <- get_model("ancova_cont_2arms")
#'
#' # Force brms backend
#' model <- get_model("ancova_cont_2arms", backend = "brms")
#'
#' \dontrun{
#' # Force BayesFlow backend (requires Python + trained model)
#' model <- get_model("ancova_cont_2arms", backend = "bf")
#' }
#'
#' # Check which backend is active
#' model@backend
get_model <- function(name, backend = c("auto", "brms", "bf")) {
  # Validate name
  if (missing(name) || !is.character(name) || length(name) != 1) {
    cli::cli_abort(c(
      "{.arg name} must be a single character string",
      "i" = "Use {.fn list_predefined_models} to see available models"
    ))
  }

  backend <- match.arg(backend)

  # Get base model from registry
  model <- get_predefined_model(name)

  # Resolve and set backend
  model <- resolve_backend(model, backend, model_name = name)

  model
}


#' Build a Custom Model
#'
#' Creates a custom rctbp_model from user-supplied components. For most users,
#' [get_model()] with predefined models is recommended instead.
#'
#' @param data_simulation_fn A function that simulates trial data. Must accept
#'   parameters `n_total` and `p_alloc` at minimum.
#' @param brms_model A compiled brmsfit object serving as the estimation template.
#'   Posterior draws are stripped automatically (chains = 0).
#' @param bayesflow_model A trained BayesFlow model (Python object via reticulate).
#' @param backend Which backend to use: "brms" or "bf". Unlike [get_model()],
#'   "auto" is not supported - you must specify which backend to use.
#' @param backend_args_brms List of brms-specific arguments (chains, iter, etc.)
#' @param backend_args_bf List of BayesFlow-specific arguments.
#'   Default: list(batch_size = 64L, n_posterior_samples = 1000L)
#' @param n_endpoints Number of endpoints (positive integer)
#' @param endpoint_types Character vector of endpoint types ("continuous", "binary", "count")
#' @param n_arms Number of arms including control (positive integer)
#' @param n_repeated_measures Number of repeated measures (NULL or 0 for single timepoint)
#' @param model_name Descriptive name for the model
#'
#' @return An S7 object of class "rctbp_model"
#'
#' @export
#' @seealso [get_model()] for predefined models, [add_bf_backend()], [add_brms_backend()]
#'
#' @examples
#' \dontrun{
#' # Create custom model with brms backend
#' model <- build_model(
#'   data_simulation_fn = my_sim_function,
#'   brms_model = my_compiled_brms,
#'   backend = "brms",
#'   n_endpoints = 1,
#'   endpoint_types = "continuous",
#'   n_arms = 2
#' )
#' }
build_model <- function(data_simulation_fn,
                        brms_model = NULL,
                        bayesflow_model = NULL,
                        backend = c("brms", "bf"),
                        backend_args_brms = list(),
                        backend_args_bf = list(
                          batch_size = 64L,
                          n_posterior_samples = 1000L
                        ),
                        n_endpoints = NULL,
                        endpoint_types = NULL,
                        n_arms = NULL,
                        n_repeated_measures = NULL,
                        model_name = NULL) {

  backend <- match.arg(backend)

  # Early validation
  if (!is.function(data_simulation_fn)) {
    cli::cli_abort(c(
      "{.arg data_simulation_fn} must be a function",
      "x" = "You supplied {.type {data_simulation_fn}}"
    ))
  }

  if (!is.null(model_name) && !is.character(model_name)) {
    cli::cli_abort(c(
      "{.arg model_name} must be a character string or NULL",
      "x" = "You supplied {.type {model_name}}"
    ))
  }

  # Validate at least one model is provided
  if (is.null(brms_model) && is.null(bayesflow_model)) {
    cli::cli_abort(c(
      "At least one of {.arg brms_model} or {.arg bayesflow_model} must be provided",
      "x" = "Both are NULL"
    ))
  }

  # Validate backend matches provided models
  if (backend == "brms" && is.null(brms_model)) {
    cli::cli_abort(c(
      "Backend 'brms' selected but {.arg brms_model} is NULL",
      "i" = "Provide a brms model or use backend = 'bf'"
    ))
  }
  if (backend == "bf" && is.null(bayesflow_model)) {
    cli::cli_abort(c(
      "Backend 'bf' selected but {.arg bayesflow_model} is NULL",
      "i" = "Provide a BayesFlow model or use backend = 'brms'"
    ))
  }

  # Strip brms model posterior draws (template only)
  if (!is.null(brms_model)) {
    brms_model <- suppressMessages(stats::update(brms_model, chains = 0, silent = 2))
  }

  # Set default model_name
  if (is.null(model_name)) {
    backends_present <- c()
    if (!is.null(brms_model)) backends_present <- c(backends_present, "brms")
    if (!is.null(bayesflow_model)) backends_present <- c(backends_present, "bf")
    backend_str <- paste(backends_present, collapse = "+")
    model_name <- paste0("Custom Model (", backend_str, ")")
  }

  # Create S7 object
  model_obj <- rctbp_model(
    predefined_model = NULL,
    data_simulation_fn = data_simulation_fn,
    brms_model = brms_model,
    bayesflow_model = bayesflow_model,
    backend = backend,
    backend_args_brms = backend_args_brms,
    backend_args_bf = backend_args_bf,
    model_name = model_name,
    n_endpoints = n_endpoints,
    endpoint_types = endpoint_types,
    n_arms = n_arms,
    n_repeated_measures = n_repeated_measures
  )

  model_obj
}


#' Resolve Backend Selection
#'
#' Internal helper to resolve "auto" backend to actual backend.
#' For "bf", attempts to load pre-trained model if not present.
#'
#' @param model rctbp_model object
#' @param backend "auto", "brms", or "bf"
#' @param model_name Name for loading BayesFlow model (optional)
#'
#' @return Modified model with resolved backend
#' @keywords internal
resolve_backend <- function(model, backend, model_name = NULL) {

  has_brms <- !is.null(model@brms_model)
  has_bf <- !is.null(model@bayesflow_model)

  if (backend == "brms") {
    # -------------------------------------------------------------------------
    # "brms" - Only load brms, never try BayesFlow
    # -------------------------------------------------------------------------
    if (!has_brms) {
      if (!is.null(model_name)) {
        tryCatch({
          brms_model <- load_brms_model(model_name)
          model@brms_model <- brms_model
        }, error = function(e) {
          cli::cli_abort(c(
            "Backend 'brms' requested but brms model not available",
            "i" = "Model '{model_name}' not found in cache or GitHub releases",
            "x" = conditionMessage(e)
          ))
        })
      } else {
        cli::cli_abort("Backend 'brms' requested but no brms model available")
      }
    }
    model@backend <- "brms"
    model@bayesflow_model <- NULL  # Clear BayesFlow if present

  } else if (backend == "bf") {
    # -------------------------------------------------------------------------
    # "bf" - Try BayesFlow, fall back to brms with warning if unavailable
    # -------------------------------------------------------------------------
    bf_loaded <- FALSE

    if (has_bf) {
      bf_loaded <- TRUE
    } else if (!is.null(model_name) && check_bf_available(silent = TRUE)) {
      tryCatch({
        bf_model <- load_bf_model(model_name)
        model@bayesflow_model <- bf_model
        bf_loaded <- TRUE
      }, error = function(e) {
        # Will fall back to brms below
      })
    }

    if (bf_loaded) {
      model@backend <- "bf"
      model@brms_model <- NULL  # Clear brms
      cli::cli_alert_success("Using BayesFlow backend")
    } else {
      # Fallback to brms with warning
      if (!has_brms && !is.null(model_name)) {
        tryCatch({
          brms_model <- load_brms_model(model_name)
          model@brms_model <- brms_model
          has_brms <- TRUE
        }, error = function(e) {
          cli::cli_abort(c(
            "BayesFlow unavailable and brms fallback failed",
            "x" = conditionMessage(e)
          ))
        })
      }

      if (has_brms || !is.null(model@brms_model)) {
        model@backend <- "brms"
        model@bayesflow_model <- NULL
        cli::cli_warn(c(
          "BayesFlow backend unavailable, falling back to brms",
          "i" = "To use BayesFlow: install Python, bayesflow package, and upload trained model"
        ))
      } else {
        cli::cli_abort("Neither BayesFlow nor brms backend available")
      }
    }

  } else {
    # -------------------------------------------------------------------------
    # "auto" - Try BayesFlow first (silent), fall back to brms (silent)
    # -------------------------------------------------------------------------
    bf_loaded <- FALSE

    if (has_bf) {
      bf_loaded <- TRUE
    } else if (!is.null(model_name) && check_bf_available(silent = TRUE)) {
      tryCatch({
        bf_model <- load_bf_model(model_name)
        model@bayesflow_model <- bf_model
        bf_loaded <- TRUE
      }, error = function(e) {
        # Silent - will try brms
      })
    }

    if (bf_loaded) {
      model@backend <- "bf"
      model@brms_model <- NULL  # Clear brms
      cli::cli_alert_info("Using BayesFlow backend (auto-selected)")
    } else if (has_brms) {
      model@backend <- "brms"
      model@bayesflow_model <- NULL
      cli::cli_alert_info("Using brms backend (auto-selected)")
    } else if (!is.null(model_name)) {
      # Try to load brms
      tryCatch({
        brms_model <- load_brms_model(model_name)
        model@brms_model <- brms_model
        model@backend <- "brms"
        model@bayesflow_model <- NULL
        cli::cli_alert_info("Using brms backend (auto-selected)")
      }, error = function(e) {
        cli::cli_abort(c(
          "No backend available",
          "x" = "Neither BayesFlow nor brms models could be loaded"
        ))
      })
    } else {
      cli::cli_abort("No backend available - provide brms_model or bayesflow_model")
    }
  }

  model
}

# =============================================================================
# S7 METHOD: print()
# =============================================================================
# Formats rctbp_model objects for console display showing model specifications
# and parameter information.

#' Print Method for rctbp_model Objects
#'
#' Displays a summary of a rctbp_model object, including model specifications,
#' parameter information, and function details.
#'
#' @param x An S7 object of class "rctbp_model"
#' @param ... Additional arguments (currently unused)
#'
#' @return Invisibly returns the input object. Used for side effects (printing).
#' @importFrom S7 method
#' @name print.rctbp_model
#' @export
S7::method(print, rctbp_model) <- function(x, ...) {
  report <- build_report.rctbp_model(x)
  render_report(report)
  invisible(x)
}

# =============================================================================
# UTILITY FUNCTIONS: Predefined Model Registry
# =============================================================================
# Functions to discover and access models stored in package internal environment
# (sysdata.rda). Predefined models provide ready-made configurations for
# common trial designs.

#' List available predefined models
#'
#' This function inspects the package's internal environment and returns the names
#' of all predefined model objects that inherit from the `rctbp_model` class.
#' These models are prebuilt and stored internally in the package via `sysdata.rda`
#' and are not exported to users directly. This function allows discovery of
#' available predefined models programmatically.
#'
#' @param filter_string Optional character string for filtering model names.
#'   If provided, only models whose names match this pattern (via [base::grepl()])
#'   will be returned. Use this to find specific types of models (e.g., "ancova").
#'
#' @details
#' The returned model names can be used directly with [get_model()]:
#'
#' `model <- get_model("model_name")`
#'
#' This provides a convenient way to discover and use prebuilt models without
#' needing to specify all model parameters manually.
#'
#' @return A character vector of object names corresponding to predefined models.
#'   Returns an empty character vector if no models are found or if the filter
#'   excludes all available models.
#'
#' @examples
#' # List all available predefined models
#' list_predefined_models()
#'
#' # Filter for ANCOVA models only
#' list_predefined_models(filter_string = "ancova")
#'
#' # Use discovered model with get_model()
#' available_models <- list_predefined_models()
#' if (length(available_models) > 0) {
#'   model <- get_model(available_models[1])
#' }
#'
#' @export
list_predefined_models <- function(filter_string = NULL) {
  ns <- asNamespace("rctbayespower")

  all_objs <- ls(envir = ns, all.names = TRUE)
  matches <- vapply(all_objs, function(obj_name) {
    obj <- get(obj_name, envir = ns)
    inherits(obj, "rctbayespower::rctbp_model")
  }, logical(1L))

  # filter matching strings
  if (!is.null(filter_string)) {
    matches <- matches & grepl(filter_string, all_objs)
  }

  names(matches[matches])
}


#' Get a predefined model by name
#'
#' Access an internal predefined model by name.
#'
#' @param model_name Character scalar, name of the predefined model.
#' @return The model object.
#' @export
get_predefined_model <- function(model_name) {
  ns <- asNamespace("rctbayespower")
  if (!model_name %in% ls(envir = ns)) {
    cli::cli_abort(c(
      "Model {.val {model_name}} not found",
      "i" = "Use {.fn list_predefined_models} to see available models"
    ), call = FALSE)
  }
  model <- get(model_name, envir = ns)
  if (!inherits(model, "rctbayespower::rctbp_model")) {
    cli::cli_abort(c(
      "Object {.val {model_name}} is not an rctbp_model",
      "x" = "Got {.cls {class(model)}}"
    ), call = FALSE)
  }
  model
}


# =============================================================================
# UTILITY FUNCTIONS: Backend Management
# =============================================================================

#' Add BayesFlow Backend to Existing Model
#'
#' Adds a BayesFlow model to an existing rctbp_model that only has brms.
#' This enables dual-backend support, allowing switching to the faster
#' BayesFlow backend.
#'
#' @param model Existing rctbp_model object
#' @param bayesflow_model BayesFlow/Keras model (Python object via reticulate).
#'   Load using [load_bf_model()] or Python's keras.models.load_model().
#' @param backend_args_bf BayesFlow configuration (batch_size, n_posterior_samples)
#' @param set_active Whether to switch to BayesFlow backend (default TRUE)
#'
#' @return The modified model with BayesFlow backend added
#'
#' @export
#' @seealso [get_model()], [build_model()], [load_bf_model()]
#'
#' @examples
#' \dontrun{
#' # Start with brms-only model
#' model <- get_model("ancova_cont_2arms", backend = "brms")
#' model@backend  # "brms"
#'
#' # Add BayesFlow backend
#' bf_model <- load_bf_model("ancova_cont_2arms")
#' model <- add_bf_backend(model, bf_model)
#' model@backend  # "bf" (set_active = TRUE by default)
#'
#' # Switch back to brms
#' model@backend <- "brms"
#' }
add_bf_backend <- function(model, bayesflow_model,
                            backend_args_bf = list(
                              batch_size = 64L,
                              n_posterior_samples = 1000L
                            ),
                            set_active = TRUE) {

  # Validate input
  if (!inherits(model, "rctbayespower::rctbp_model") &&
      !inherits(model, "rctbp_model")) {
    cli::cli_abort(c(
      "{.arg model} must be an rctbp_model object",
      "x" = "Got {.cls {class(model)}}"
    ))
  }

  if (is.null(bayesflow_model)) {
    cli::cli_abort("{.arg bayesflow_model} cannot be NULL")
  }

  # Add BayesFlow model and args
  model@bayesflow_model <- bayesflow_model
  model@backend_args_bf <- backend_args_bf

  # Optionally switch to BayesFlow backend
  if (set_active) {
    model@backend <- "bf"
  }

  model
}


#' Add brms Backend to Existing Model
#'
#' Adds a brms model to an existing rctbp_model that only has BayesFlow.
#' This enables dual-backend support.
#'
#' @param model Existing rctbp_model object
#' @param brms_model Compiled brmsfit template model
#' @param backend_args_brms brms configuration (chains, iter, etc.)
#' @param set_active Whether to switch to brms backend (default FALSE)
#'
#' @return The modified model with brms backend added
#'
#' @export
#' @seealso [build_model()], [load_brms_model()]
add_brms_backend <- function(model, brms_model,
                              backend_args_brms = list(),
                              set_active = FALSE) {

  # Validate input
  if (!inherits(model, "rctbayespower::rctbp_model") &&
      !inherits(model, "rctbp_model")) {
    cli::cli_abort(c(
      "{.arg model} must be an rctbp_model object",
      "x" = "Got {.cls {class(model)}}"
    ))
  }

  if (is.null(brms_model)) {
    cli::cli_abort("{.arg brms_model} cannot be NULL")
  }

  if (!inherits(brms_model, "brmsfit")) {
    cli::cli_abort(c(
      "{.arg brms_model} must be a brmsfit object",
      "x" = "Got {.cls {class(brms_model)}}"
    ))
  }

  # Strip posterior draws for efficiency
  brms_model <- suppressMessages(stats::update(brms_model, chains = 0, silent = 2))

  # Add brms model and args
  model@brms_model <- brms_model
  model@backend_args_brms <- backend_args_brms

  # Optionally switch to brms backend
  if (set_active) {
    model@backend <- "brms"
  }

  model
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/class_power_analysis.R"
# =============================================================================
# S7 CLASS DEFINITION: rctbp_power_analysis
# =============================================================================
# Main power analysis configuration and results container. Stores simulation
# parameters (n_sims, n_cores), conditions object, and placeholders for results.

#' @importFrom S7 new_class class_any class_numeric class_logical class_list class_character

rctbp_power_analysis <- S7::new_class(
  "rctbp_power_analysis",
  properties = list(
    # Simulation control parameters
    n_sims = S7::new_property(S7::class_numeric, default = 1),
    n_cores = S7::new_property(S7::class_numeric, default = 1),
    verbosity = S7::new_property(class = S7::class_numeric, default = 1),
    brms_args = S7::class_list | NULL,
    bf_args = S7::class_list | NULL,  # BayesFlow args: n_posterior_samples, batch_size
    design_prior = S7::new_property(S7::class_character |
                                      S7::class_function | NULL, default = NULL),

    # Configuration objects
    conditions = S7::class_any,  # rctbp_conditions object
    design = S7::new_property(getter = function(self) self@conditions@design),

    # Results (populated after run())
    results_conditions = S7::class_data.frame,
    results_interim = S7::class_data.frame,
    results_raw = S7::class_data.frame,
    elapsed_time = S7::new_property(class = S7::class_numeric, default = NA_real_),

    # Computed property for interim analysis detection
    has_interim = S7::new_property(
      getter = function(self) nrow(self@results_interim) > 0
    )
  ),
  validator = function(self) {
    # Validate conditions object
    if (!inherits(self@conditions, "rctbayespower::rctbp_conditions") &&
        !inherits(self@conditions, "rctbp_conditions")) {
      cli::cli_abort(c(
        "{.arg conditions} must be a valid rctbp_conditions object",
        "x" = "Got object of class {.cls {class(self@conditions)}}",
        "i" = "Use {.fn build_conditions} to create a valid conditions object"
      ))
    }

    # Validate n_sims, positive whole number
    if (!is.numeric(self@n_sims) ||
        length(self@n_sims) != 1 ||
        self@n_sims <= 0 || self@n_sims != round(self@n_sims)) {
      cli::cli_abort(c(
        "{.arg n_sims} must be a positive whole number",
        "x" = "You supplied {.val {self@n_sims}}",
        "i" = "Use an integer >= 1"
      ))
    }

    # Validate n_cores, positive whole number, < available cores
    if (!is.numeric(self@n_cores) ||
        length(self@n_cores) != 1 ||
        self@n_cores <= 0 ||
        self@n_cores != round(self@n_cores)) {
      cli::cli_abort(c(
        "{.arg n_cores} must be a positive whole number",
        "x" = "You supplied {.val {self@n_cores}}",
        "i" = "Use an integer >= 1"
      ))
    }
    # Validate n_cores <= parallel::detectCores()
    if (self@n_cores > parallel::detectCores()) {
      cli::cli_abort(c(
        "{.arg n_cores} must not exceed available cores",
        "x" = "You requested {self@n_cores} cores, but only {parallel::detectCores()} are available",
        "i" = "We recommend using at most {.code parallel::detectCores() - 1} cores"
      ))
    }

    # Validate verbosity
    if (!is.numeric(self@verbosity) ||
        length(self@verbosity) != 1 ||
        !self@verbosity %in% c(0, 1, 2)) {
      cli::cli_abort(c(
        "{.arg verbosity} must be 0, 1, or 2",
        "x" = "You supplied {.val {self@verbosity}}",
        "i" = "Use 0 (quiet), 1 (normal), or 2 (verbose)"
      ))
    }

    # Validate brms_args
    if (!is.list(self@brms_args)) {
      cli::cli_abort(c(
        "{.arg brms_args} must be a list",
        "x" = "You supplied {.type {self@brms_args}}"
      ))
    }

    # If all validations pass, return NULL
    NULL
  }
)

# =============================================================================
# CONSTRUCTOR FUNCTION: power_analysis()
# =============================================================================
# Creates power analysis configuration object and optionally executes it.
# Validates all parameters and prepares for parallel execution.

#' Build Power Analysis Configuration
#'
#' Creates a power analysis configuration object that specifies all parameters
#' needed to conduct Bayesian power analysis for randomized controlled trials.
#' This function creates an S7 object that serves as the main interface for
#' configuring and executing power analysis simulations.
#'
#' @param run Logical indicating whether to immediately execute the analysis
#'   after creating the configuration object (default TRUE)
#' @param ... Arguments passed to the rctbp_power_analysis constructor, including:
#'   \itemize{
#'     \item conditions: An rctbp_conditions object containing the experimental
#'       conditions and design parameters for the power analysis
#'     \item n_sims: Number of simulations to run per condition (default 100)
#'     \item n_cores: Number of CPU cores to use for parallel execution (default 1).
#'       Must not exceed the number of available cores
#'     \item verbosity: Verbosity level controlling output detail (default 1).
#'       0 = quiet (minimal output), 1 = normal (standard output), 2 = verbose (detailed debug output).
#'       Can also be set globally with options(rctbayespower.verbosity = level).
#'     \item brms_args: List of brms arguments (chains, iter, warmup, cores).
#'       Overrides model defaults for brms backend.
#'     \item bf_args: List of BayesFlow arguments. Overrides model defaults:
#'       \itemize{
#'         \item n_posterior_samples: Number of posterior samples (default 1000)
#'         \item batch_size: Simulations per batch for GPU efficiency (default 64)
#'       }
#'     \item design_prior: Prior specification for design parameters. Can be NULL
#'       (no prior), a string with brms syntax, or a function for custom priors
#'   }
#'
#' @return An S7 object of class "rctbp_power_analysis" containing:
#'   \itemize{
#'     \item All input parameters for simulation control
#'     \item Access to design and model specifications via properties
#'     \item Placeholder slots for results (filled after running analysis)
#'   }
#'
#' @details
#' This function validates input parameters and creates a configuration object
#' that can be executed using [run()]. The resulting object provides access
#' to design and model information through S7 properties:
#'
#' \strong{Key Properties:}
#' \itemize{
#'   \item `design`: Access to the experimental design configuration
#'   \item `model`: Access to the underlying Bayesian model specification
#'   \item `conditions`: The condition grid for analysis
#' }
#'
#' \strong{Parallel Processing:}
#' When `n_cores > 1`, simulations are distributed across multiple cores
#' for improved performance. The function validates that `n_cores` does
#' not exceed available system cores.
#'
#' @seealso [build_conditions()], [build_design()], [build_model()], [run()]
#'
#' @export
#' @examples
#' \dontrun{
#' # Create conditions for power analysis
#' conditions <- build_conditions(design, n_total = c(100, 200))
#'
#' # Basic power analysis (brms backend)
#' result <- power_analysis(conditions = conditions, n_sims = 100)
#'
#' # With custom brms arguments
#' result <- power_analysis(
#'   conditions = conditions,
#'   n_sims = 1000,
#'   n_cores = 4,
#'   brms_args = list(chains = 4, iter = 2000)
#' )
#'
#' # BayesFlow backend with custom settings
#' result <- power_analysis(
#'   conditions = conditions,
#'   n_sims = 1000,
#'   bf_args = list(n_posterior_samples = 2000, batch_size = 128)
#' )
#' }
power_analysis <- function(run = TRUE, ...) {
  power_object <- rctbp_power_analysis(...)
  
  # Overwrite object parameters with dots if provided and run = FALSE
  # Avoids updating twice, since run() also updates S7 object with dots
  if (!run){
    if (length(list(...)) > 0) {
      # Recreate the S7 object with updated parameters
      power_object <- update_S7_with_dots(power_object, ...)
    }
  }

  # Run the power analysis immediately if requested)
  if (run) {
    power_object <- run(x = power_object)
  }
  
  return(power_object)
}


#' Run Analysis Objects
#'
#' Generic function for executing analysis objects. This function provides a
#' unified interface for running different types of analysis configurations.
#'
#' @param x An S7 object to run (e.g., rctbp_power_analysis)
#' @param ... Additional arguments passed to specific methods
#'
#' @details
#' This generic function dispatches to appropriate methods based on the class
#' of the input object. Currently supported:
#' \itemize{
#'   \item \code{rctbp_power_analysis}: Executes Bayesian power analysis
#' }
#'
#' @return The result depends on the specific method called. For power analysis
#'   objects, returns the modified object with results stored in the
#'   \code{results_conditions}, \code{results_interim}, and \code{results_raw} properties.
#'
#' @export
#' @importFrom S7 method new_generic
#' @examples
#' \dontrun{
#' # Create and run power analysis
#' power_config <- rctbp_power_analysis(conditions = conditions, n_sims = 100)
#' power_config <- run(power_config)
#' }
run <- S7::new_generic("run", "x")

#' Run Power Analysis
#'
#' Executes the Bayesian power analysis using the configuration specified in
#' the rctbp_power_analysis object. This method contains the core simulation
#' logic and returns comprehensive results.
#'
#' @param x An S7 object of class "rctbp_power_analysis"
#' @param ... Additional arguments (currently unused)
#'
#' @details
#' This method implements the core power analysis algorithm:
#'
#' \strong{Parallel Execution:} When n_cores > 1, simulations are distributed
#' across multiple cores for optimal performance.
#'
#' \strong{Progress Tracking:} Progress is shown through pbapply when available,
#' with fallback to basic progress reporting.
#'
#' \strong{Error Handling:} Robust error handling ensures partial results are
#' preserved even if some simulations fail.
#'
#' \strong{Result Aggregation:} Individual simulation results are aggregated
#' into power curves and summary statistics.
#'
#' @return A list of class "rctbayespower_sim_result" containing:
#' \itemize{
#'   \item results_df: Aggregated power analysis results
#'   \item results_df_raw: Raw simulation results from all runs
#'   \item design: The design object used for analysis
#'   \item conditions: The condition specifications used
#'   \item n_sims: Number of simulations per condition
#'   \item elapsed_time: Total analysis runtime
#' }
#'
#' @export
#' @importFrom parallel detectCores makeCluster stopCluster parLapply clusterEvalQ clusterExport
#' @importFrom utils modifyList
#' @name run.rctbp_power_analysis

# =============================================================================
# S7 METHOD: run() - Main Power Analysis Execution
# =============================================================================
# Executes power analysis simulations across all conditions using parallel
# processing. Handles S7 serialization, worker setup, batching strategy,
# and result aggregation.

S7::method(run, rctbp_power_analysis) <- function(x, ...) {
  # Time start
  start_time <- Sys.time()

  # Overwrite object parameters with dots if provided
  if (length(list(...)) > 0) {
    # Recreate the S7 object with updated parameters
    x <- update_S7_with_dots(x, ...)
  }

  # Set verbosity for this analysis run
  old_verbosity <- set_verbosity(x@verbosity)
  on.exit(set_verbosity(old_verbosity), add = TRUE)


  # Early status message - show immediately before any slow operations
  design <- x@conditions@design
  backend <- design@model@backend
  n_conditions <- nrow(x@conditions@conditions_grid)
  total_work_units <- x@n_sims * n_conditions

 # Calculate total model fits (including interim analyses)
  # analysis_at can vary per condition, so check first condition as representative
  n_analyses <- 1L
  if (!is.null(design@analysis_at)) {
    n_analyses <- length(design@analysis_at)
  }
  total_model_fits <- total_work_units * n_analyses

  if (should_show(1)) {
    cli::cli_h3("Power Analysis Configuration")
    cli::cli_text("")
    if (should_show(2)) {
      print(x@conditions@conditions_grid)
      cli::cli_text("")
    }

    config_items <- c(
      "Backend" = backend,
      "Conditions" = n_conditions,
      "Simulations per condition" = x@n_sims
    )
    if (n_analyses > 1) {
      config_items <- c(config_items,
        "Analyses per simulation" = n_analyses,
        "Total model fits" = total_model_fits
      )
    } else {
      config_items <- c(config_items,
        "Total simulations" = total_work_units
      )
    }
    cli::cli_dl(config_items)
    cli::cli_text("")
  }

  # Configure brms model with MCMC settings ----------------------------------
  # This updates the compiled model template with the desired sampling settings.
  # Workers will use this pre-configured model when fitting simulated data.

  if (backend == "brms") {
    # Set default brms args
    default_brms_args <- list(
      chains = 4,
      iter = 450,
      warmup = 200,
      cores = 1
    )
    # Merge with user-provided brms_args
    final_brms_args <- utils::modifyList(default_brms_args, x@brms_args)
    # Assign back to object for reference
    x@brms_args <- final_brms_args

    # Warn if cores > 1
    if (x@brms_args$cores > 1) {
      cli::cli_warn(c(
        "Parallel configuration issue detected",
        "x" = "Multiple cores specified for brms when running simulations in parallel",
        "i" = "Set brms cores to 1 when using parallel simulations to avoid resource conflicts"
      ))
    }

    # Update brms model with MCMC settings
    # Note: This runs a quick fit on the template data to configure the model.
    # Warnings about R-hat/ESS are expected and suppressed since this is just
    # for configuration - actual inference happens in workers with simulated data.
    # Changing chains/iter/warmup does NOT trigger recompilation (Stan code unchanged).
    # See: https://github.com/paul-buerkner/brms/blob/master/R/update.R
    if (should_show(1)) {
      cli::cli_alert_info("Updating brms model with MCMC settings (chains={final_brms_args$chains}, iter={final_brms_args$iter}, warmup={final_brms_args$warmup})")
    }
    x@conditions@design@model@brms_model <- suppressWarnings(
      do.call(function(...) {
        stats::update(object = x@conditions@design@model@brms_model, ...)
      }, x@brms_args)
    )
    if (should_show(1)) {
      cli::cli_alert_success("Model configured")
      cli::cli_text("")
    }

    # Also store in backend_args for workers
    x@conditions@design@model@backend_args_brms <- final_brms_args

  } else if (backend == "bf") {
    # For BayesFlow backend, merge user-provided bf_args with model defaults
    default_bf_args <- list(
      n_posterior_samples = 1000L,
      batch_size = 64L
    )
    # Get model defaults, then override with user-provided args
    model_bf_args <- design@model@backend_args_bf
    if (length(model_bf_args) > 0) {
      default_bf_args <- utils::modifyList(default_bf_args, model_bf_args)
    }
    if (!is.null(x@bf_args) && length(x@bf_args) > 0) {
      default_bf_args <- utils::modifyList(default_bf_args, x@bf_args)
    }
    final_bf_args <- default_bf_args
    x@bf_args <- final_bf_args

    # Store in model for workers
    x@conditions@design@model@backend_args_bf <- final_bf_args

    if (should_show(1)) {
      cli::cli_alert_info("Using BayesFlow backend (n_posterior_samples={final_bf_args$n_posterior_samples}, batch_size={final_bf_args$batch_size})")
    }
  }



  # Extract variables from object
  n_cores <- x@n_cores
  n_sims <- x@n_sims
  
  # Extract parameters from the object
  conditions <- x@conditions
  design <- x@conditions@design
  design_prior <- x@design_prior

  # =============================================================================
  # S7 SERIALIZATION WORKAROUND
  # =============================================================================
  # Problem: S7 objects contain environments that don't serialize across
  # parallel cluster boundaries (PSOCK clusters). When passed to workers,
  # S7 objects lose their class information and methods fail.
  #
  # Solution: prepare_design_for_workers() flattens S7 objects to nested lists
  # that serialize correctly. Workers access fields using list syntax ($)
  # instead of S7 property syntax (@).
  #
  # Trade-off: Worker code must handle both S7 and list formats, adding
  # complexity, but enables parallel execution without serialization failures.
  design_serialized <- prepare_design_for_workers(design)

  # Detect backend and batching strategy
  backend <- design@model@backend
  batch_size <- if (backend == "bf" && !is.null(design@model@backend_args_bf$batch_size)) {
    design@model@backend_args_bf$batch_size
  } else {
    1L  # No batching for brms or when batch_size not specified
  }

  # Create work units: conditions x iterations (NOT interims!)
  # Work units are (id_cond, id_iter) pairs
  work_units <- lapply(seq_len(n_conditions), function(id_cond) {
    lapply(seq_len(n_sims), function(id_iter) {
      list(
        id_cond = id_cond,
        id_iter = id_iter,
        condition_args = conditions@condition_arguments[[id_cond]]
      )
    })
  })
  work_units <- unlist(work_units, recursive = FALSE)

  # Execute simulations
  if (requireNamespace("pbapply", quietly = TRUE)) {
    # set cl to NULL for default sequential execution
    cl <- NULL
    if (n_cores > 1) {
      if (should_show(1)) {
        cli::cli_alert_info("Setting up parallel cluster with {n_cores} cores")
      }
      # Set up cluster
      cl <- parallel::makeCluster(n_cores, type = "PSOCK")
      # Ensure cleanup on exit
      
      # Load required packages on workers first
      parallel::clusterEvalQ(cl, {
        library(brms)
        library(dplyr)
        library(progressr)
        library(posterior)
        library(purrr)
        library(stats)
        library(utils)
        library(S7)
        library(stringr)
        
        # For development with devtools/pkgload, try to load the package
        tryCatch({
          if (requireNamespace("rctbayespower", quietly = TRUE)) {
            library(rctbayespower)
          }
        }, error = function(e) {
          # Package might not be properly installed, will rely on explicit exports
        })
      })
      
      # Export required objects to cluster
      parallel::clusterExport(
        cl,
        varlist = c("work_units", "design_serialized", "batch_size", "backend"),
        envir = environment()
      )

      # =============================================================================
      # WORKER EXPORT STRATEGY (Multi-tier fallback)
      # =============================================================================
      # Algorithm: Try multiple export methods to handle different package states
      #
      # Method 1: Namespace export (installed package mode)
      #   - Fastest and most reliable
      #   - Works when package is installed normally (R CMD INSTALL)
      #   - Functions accessible via asNamespace("rctbayespower")
      #
      # Method 2: Environment export (development mode - NOT IMPLEMENTED)
      #   - Fallback for devtools::load_all() workflow
      #   - devtools doesn't register namespace, so Method 1 fails
      #   - Would extract from parent environment
      #
      # Method 3: Graceful degradation
      #   - Proceed with warning if export fails
      #   - Workers may fail at runtime if functions unavailable
      #
      # Rationale: devtools::load_all() doesn't register package namespace like
      # standard installation, requiring flexible export strategy to support
      # both development and production modes.

      functions_to_export <- c(
        "worker_process_single",
        "worker_process_batch",
        "prepare_design_for_workers",
        "estimate_single_brms",
        "estimate_single_npe",
        "estimate_sequential_brms",
        "estimate_sequential_npe",
        "create_error_result",
        "estimate_posterior",
        "estimate_posterior_brms",
        "estimate_posterior_npe",
        "extract_posterior_rvars",
        "extract_posterior_rvars_brms",
        "extract_posterior_rvars_npe",
        "compute_measures",
        "compute_measures_brmsfit",
        "calculate_mcse_power",
        "calculate_mcse_mean",
        "calculate_mcse_integrated_power"
      )

      export_success <- FALSE

      # Try Method 1: Namespace export
      tryCatch({
        ns <- asNamespace("rctbayespower")
        all_exist <- all(sapply(functions_to_export, exists, envir = ns))
        if (all_exist) {
          parallel::clusterExport(cl, varlist = functions_to_export, envir = ns)
          export_success <- TRUE
          if (should_show(2)) {
            cli::cli_alert_success("Functions exported from package namespace")
          }
        }
      }, error = function(e) {
        if (should_show(2)) {
          cli::cli_alert_warning("Namespace export failed: {e$message}")
        }
      })

      # Method 3: Graceful degradation
      if (!export_success && should_show(1)) {
        cli::cli_alert_warning("Could not export all required functions to workers")
      }
    }

    # =============================================================================
    # BATCHING STRATEGY
    # =============================================================================
    # Algorithm: Choose processing strategy based on backend and batch_size
    #
    # batch_size = 1 (brms typical):
    #   - Each worker processes one (condition, iteration) pair
    #   - brms fits are independent --> no batching benefit
    #   - Simple parallelization: distribute work units across cores
    #
    # batch_size > 1 (NPE typical):
    #   - Group multiple work units into batches
    #   - NPE processes batch in single forward pass --> efficient
    #   - Trade-off: Larger batches = fewer parallel tasks but faster per-batch
    #
    # Rationale: NPE models can vectorize across multiple simulations (batch
    # dimension), making batched processing significantly faster. brms fits each
    # simulation independently, so batching adds no benefit.

    if (batch_size == 1L) {
      # Single processing: one work unit per worker call
      results_raw_list <-
        pbapply::pblapply(cl = cl, seq_along(work_units), function(i) {
          tryCatch({
            wu <- work_units[[i]]
            worker_process_single(
              id_cond = wu$id_cond,
              id_iter = wu$id_iter,
              condition_args = wu$condition_args,
              design = design_serialized
            )
          }, error = function(e) {
            create_error_result(
              id_iter = work_units[[i]]$id_iter,
              id_cond = work_units[[i]]$id_cond,
              id_analysis = 0L,
              error_msg = paste("Worker error:", e$message)
            )
          })
        })
    } else {
      # Batch processing: group work units into batches
      n_batches <- ceiling(total_work_units / batch_size)
      batches <- split(work_units, ceiling(seq_along(work_units) / batch_size))

      results_raw_list <-
        pbapply::pblapply(cl = cl, batches, function(batch) {
          tryCatch({
            worker_process_batch(
              work_units = batch,
              design = design_serialized
            )
          }, error = function(e) {
            # Return errors for all work units in failed batch
            lapply(batch, function(wu) {
              create_error_result(
                id_iter = wu$id_iter,
                id_cond = wu$id_cond,
                id_analysis = 0L,
                error_msg = paste("Batch worker error:", e$message)
              )
            }) |> dplyr::bind_rows()
          })
        })
    }
    
    if (n_cores > 1) {
      # Stop cluster after use
      parallel::stopCluster(cl)
    }

  } else {
    # Fallback: no progress bar if pbapply not available
    if (should_show(1)) {
      cli::cli_alert_info("Running {length(work_units)} simulations (pbapply not available, no progress bar)")
    }

    cl <- NULL
    if (n_cores > 1) {
      # Set up cluster
      cl <- parallel::makeCluster(n_cores, type = "PSOCK")

      # Load required packages on workers first
      parallel::clusterEvalQ(cl, {
        library(brms)
        library(dplyr)
        library(progressr)
        library(posterior)
        library(purrr)
        library(stats)
        library(utils)
        library(S7)
        library(stringr)

        # For development with devtools/pkgload, try to load the package
        tryCatch({
          if (requireNamespace("rctbayespower", quietly = TRUE)) {
            library(rctbayespower)
          }
        }, error = function(e) {
          # Package might not be properly installed, will rely on explicit exports
        })
      })

      # Export required objects to cluster
      parallel::clusterExport(
        cl,
        varlist = c("work_units", "design_serialized", "batch_size", "backend"),
        envir = environment()
      )

      functions_to_export <- c(
        "worker_process_single",
        "worker_process_batch",
        "prepare_design_for_workers",
        "estimate_single_brms",
        "estimate_single_npe",
        "estimate_sequential_brms",
        "estimate_sequential_npe",
        "create_error_result",
        "estimate_posterior",
        "estimate_posterior_brms",
        "estimate_posterior_npe",
        "extract_posterior_rvars",
        "extract_posterior_rvars_brms",
        "extract_posterior_rvars_npe",
        "compute_measures",
        "compute_measures_brmsfit",
        "calculate_mcse_power",
        "calculate_mcse_mean",
        "calculate_mcse_integrated_power"
      )

      export_success <- FALSE

      # Try Method 1: Namespace export
      tryCatch({
        ns <- asNamespace("rctbayespower")
        all_exist <- all(sapply(functions_to_export, exists, envir = ns))
        if (all_exist) {
          parallel::clusterExport(cl, varlist = functions_to_export, envir = ns)
          export_success <- TRUE
          if (should_show(2)) {
            cli::cli_alert_success("Functions exported from package namespace")
          }
        }
      }, error = function(e) {
        if (should_show(2)) {
          cli::cli_alert_warning("Namespace export failed: {e$message}")
        }
      })

      # Method 3: Graceful degradation
      if (!export_success && should_show(1)) {
        cli::cli_alert_warning("Could not export all required functions to workers")
      }
    }

    if (batch_size == 1L) {
      # Single processing
      if (n_cores > 1) {
        results_raw_list <- parallel::parLapply(cl, seq_along(work_units), function(i) {
          tryCatch({
            wu <- work_units[[i]]
            worker_process_single(
              id_cond = wu$id_cond,
              id_iter = wu$id_iter,
              condition_args = wu$condition_args,
              design = design_serialized
            )
          }, error = function(e) {
            create_error_result(
              id_iter = work_units[[i]]$id_iter,
              id_cond = work_units[[i]]$id_cond,
              id_analysis = 0L,
              error_msg = paste("Worker error:", e$message)
            )
          })
        })
      } else {
        results_raw_list <- lapply(work_units, function(wu) {
          tryCatch({
            worker_process_single(
              id_cond = wu$id_cond,
              id_iter = wu$id_iter,
              condition_args = wu$condition_args,
              design = design_serialized
            )
          }, error = function(e) {
            create_error_result(
              id_iter = wu$id_iter,
              id_cond = wu$id_cond,
              id_analysis = 0L,
              error_msg = paste("Worker error:", e$message)
            )
          })
        })
      }
    } else {
      # Batch processing
      batches <- split(work_units, ceiling(seq_along(work_units) / batch_size))
      if (n_cores > 1) {
        results_raw_list <- parallel::parLapply(cl, batches, function(batch) {
          tryCatch({
            worker_process_batch(
              work_units = batch,
              design = design_serialized
            )
          }, error = function(e) {
            lapply(batch, function(wu) {
              create_error_result(
                id_iter = wu$id_iter,
                id_cond = wu$id_cond,
                id_analysis = 0L,
                error_msg = paste("Batch worker error:", e$message)
              )
            }) |> dplyr::bind_rows()
          })
        })
      } else {
        results_raw_list <- lapply(batches, function(batch) {
          tryCatch({
            worker_process_batch(
              work_units = batch,
              design = design_serialized
            )
          }, error = function(e) {
            lapply(batch, function(wu) {
              create_error_result(
                id_iter = wu$id_iter,
                id_cond = wu$id_cond,
                id_analysis = 0L,
                error_msg = paste("Batch worker error:", e$message)
              )
            }) |> dplyr::bind_rows()
          })
        })
      }
    }

    # Clean up cluster
    if (n_cores > 1) {
      parallel::stopCluster(cl)
    }
  }
  
  # Filter out NULL results before combining
  results_raw_list <- results_raw_list[!sapply(results_raw_list, is.null)]
  
  if (length(results_raw_list) == 0) {
    stop("All simulations failed. Check your model and data simulation parameters.")
  }
  
  # Check for and report any error messages
  # Always show errors for debugging
  error_results <- dplyr::bind_rows(results_raw_list)
  error_rows <- !is.na(error_results$error_msg)
  if (any(error_rows)) {
    error_msgs <- error_results[error_rows, "error_msg"]
    cat("Simulation errors found:\n")
    cat(paste(unique(error_msgs), collapse = "\n"), "\n")
    cat("Number of errors:", sum(error_rows), "\n")
  }
  
  # Combine results - use bind_rows for robustness
  results_df_raw <- dplyr::bind_rows(results_raw_list)

  # Brief simulation summary (shown at verbosity >= 1)
  if (should_show(1)) {
    n_conds <- length(unique(results_df_raw$id_cond))
    total_sims <- nrow(results_df_raw)
    has_interim <- "id_look" %in% colnames(results_df_raw)

    cli::cli_text("")
    cli::cli_alert_info("Simulations complete: {n_conds} condition{?s}, {n_sims} iteration{?s} each")
  }

  # Average across simulation runs
  results_summary <- summarize_sims(results_df_raw, n_sims)

  # =============================================================================

  # HANDLE INTERIM VS STANDARD RESULT STRUCTURE
  # =============================================================================
  # summarize_sims() returns either:
  # - A data frame (standard single-look analysis)
  # - A list with $by_look and $overall (interim/sequential analysis)
  #
  # Results storage:
  # - results_conditions: per-condition summary (single-look) or overall stopping stats (sequential)
  # - results_interim: per-look metrics (sequential only, empty for single-look)
  # - results_raw: raw per-simulation data
  if (is.list(results_summary) && !is.data.frame(results_summary)) {
    # Sequential analysis results
    # results_interim = per-look metrics joined with conditions
    results_interim_df <- dplyr::full_join(
      conditions@conditions_grid,
      results_summary$by_look,
      by = "id_cond"
    )

    # results_conditions = overall stopping stats joined with conditions
    results_conditions_df <- dplyr::full_join(
      conditions@conditions_grid,
      results_summary$overall,
      by = "id_cond"
    )
  } else {
    # Standard single-look results
    results_conditions_df <- dplyr::full_join(
      conditions@conditions_grid,
      results_summary,
      by = "id_cond"
    )
    results_interim_df <- data.frame()
  }

  # End time

  elapsed_time <- difftime(Sys.time(), start_time, units = "mins")

  # Print results summary (shown at verbosity >= 1)
  if (should_show(1)) {
    cli::cli_alert_success("Analysis complete in {round(as.numeric(elapsed_time), 2)} minutes")

    # Show power range (check both results_conditions and results_interim for pwr_scs)
    pwr_source <- if ("pwr_scs" %in% names(results_conditions_df)) {
      results_conditions_df
    } else if (nrow(results_interim_df) > 0 && "pwr_scs" %in% names(results_interim_df)) {
      results_interim_df
    } else {
      NULL
    }

    if (!is.null(pwr_source)) {
      pwr_range <- range(pwr_source$pwr_scs, na.rm = TRUE)
      cli::cli_alert_info("Power range: {round(pwr_range[1] * 100, 1)}% - {round(pwr_range[2] * 100, 1)}%")

      # Show best condition (highest power)
      best_idx <- which.max(pwr_source$pwr_scs)
      if (length(best_idx) > 0) {
        best_pwr <- round(pwr_source$pwr_scs[best_idx] * 100, 1)
        best_n <- pwr_source$n_total[best_idx]
        cli::cli_alert_success("Highest power: {best_pwr}% at n_total = {best_n}")
      }
    }

    # Show early stopping summary if sequential analysis
    if (nrow(results_interim_df) > 0 && "prop_stp_early" %in% names(results_conditions_df)) {
      avg_stopped <- mean(results_conditions_df$prop_stp_early, na.rm = TRUE)
      avg_n_mn <- mean(results_conditions_df$n_mn, na.rm = TRUE)
      avg_n_planned <- mean(results_conditions_df$n_planned, na.rm = TRUE)
      cli::cli_alert_info("Early stopping: {round(avg_stopped * 100, 1)}% stopped early, avg N = {round(avg_n_mn, 0)} of {round(avg_n_planned, 0)} planned")
    }

    cli::cli_text("")
    cli::cli_text("Use {.code print(result)} for detailed summary or {.code plot(result)} for visualizations")
  }

  # Update the S7 object with results
  x@results_conditions <- results_conditions_df
  x@results_interim <- results_interim_df
  x@results_raw <- results_df_raw
  x@elapsed_time <- as.numeric(elapsed_time)

  # Show full compact summary at verbosity >= 2
  if (should_show(2)) {
    cli::cli_text("")
    print(x, target_pwr = x@conditions@target_pwr)
  }

  # Return the updated object
  return(x)
  
}


# S7 Method for Print

#' Print Method for rctbp_power_analysis Objects
#'
#' Displays a compact summary of a power analysis object. When results are
#' available, shows decision rates, early stopping statistics, and the
#' optimal/highest power condition.
#'
#' @param x An S7 object of class "rctbp_power_analysis"
#' @param ... Additional arguments:
#'   \describe{
#'     \item{target_pwr}{Target power for finding optimal condition. If NULL
#'       (default), shows the condition with highest achieved power.}
#'   }
#'
#' @return Invisibly returns the input object. Used for side effects (printing).
#' @importFrom S7 method
#' @name print.rctbp_power_analysis
#' @export
S7::method(print, rctbp_power_analysis) <- function(x, ...) {

  dots <- list(...)
  target_pwr <- if (!is.null(dots$target_pwr)) {
    dots$target_pwr
  } else {
    x@conditions@target_pwr
  }

  design <- x@conditions@design
  has_results <- nrow(x@results_conditions) > 0 || nrow(x@results_raw) > 0

  # Note: fmt_range() and fmt_params() are defined in report_builders.R

  # Helper to safely get range (handles missing/empty columns)
  safe_range <- function(col, pct = FALSE, digits = 1) {
    if (is.null(col)) return("N/A")
    vals <- col[!is.na(col)]
    if (length(vals) == 0) return("N/A")
    fmt_range(vals, pct = pct, digits = digits)
  }

  # Header
  cli::cli_h1("Power Analysis Results")

  if (has_results) {
    n_conditions <- nrow(x@conditions@conditions_grid)
    has_interim <- x@has_interim
    # For sequential: power metrics in results_interim, overall stats in results_conditions
    # For single-look: power metrics in results_conditions
    results_df <- if (has_interim) x@results_interim else x@results_conditions
    interim_overall <- if (has_interim) x@results_conditions else NULL
    n_looks <- if (has_interim) length(unique(results_df$id_look)) else 1

    # Status line
    runtime <- if (!is.na(x@elapsed_time)) round(x@elapsed_time, 1) else NA
    design_type <- if (has_interim) paste0("Sequential (", n_looks, " looks)") else "Single-look"

    cli::cli_alert_success("Completed in {runtime} min | {n_conditions} conditions x {x@n_sims} sims | {design_type}")

    # Design section
    cli::cli_h2("Design")
    target_params_str <- paste(design@target_params, collapse = ", ")
    p_scs_str <- format_boundary(design@p_sig_scs)
    p_ftl_str <- format_boundary(design@p_sig_ftl)
    cli::cli_text("Target: {.field {target_params_str}} | P(scs) >= {p_scs_str}, P(ftl) >= {p_ftl_str}")
    if (!is.null(target_pwr)) {
      cli::cli_text("Target power: {.strong {round(target_pwr * 100, 0)}%}")
    }

    # Decision Rates section
    cli::cli_h2("Decision Rates {.emph (range across conditions)}")
    pwr_scs_range <- safe_range(results_df$pwr_scs, pct = TRUE)
    pwr_ftl_range <- safe_range(results_df$pwr_ftl, pct = TRUE)
    cli::cli_bullets(c(
      "*" = "Success: {pwr_scs_range}",
      "*" = "Futility: {pwr_ftl_range}"
    ))
    if (has_interim && !is.null(interim_overall) && "prop_no_dec" %in% names(interim_overall)) {
      no_dec_range <- safe_range(interim_overall$prop_no_dec, pct = TRUE)
      cli::cli_bullets(c("*" = "No decision: {no_dec_range}"))
    }

    # Early Stopping section (only for sequential)
    if (has_interim && !is.null(interim_overall)) {
      cli::cli_h2("Early Stopping {.emph (range across conditions)}")
      bullets <- character()
      if ("prop_stp_early" %in% names(interim_overall)) {
        stp_range <- safe_range(interim_overall$prop_stp_early, pct = TRUE, digits = 0)
        bullets <- c(bullets, "*" = "Stopped early: {stp_range}")
      }
      if ("n_mn" %in% names(interim_overall) && "n_planned" %in% names(interim_overall)) {
        n_mn_range <- safe_range(interim_overall$n_mn, digits = 0)
        n_planned_range <- safe_range(interim_overall$n_planned, digits = 0)
        bullets <- c(bullets, "*" = "Sample size: {n_mn_range} (of {n_planned_range} planned)")
      }
      if (length(bullets) > 0) {
        cli::cli_bullets(bullets)
      }
    }

    # Find optimal condition
    optimal <- find_optimal_condition(
      results_summ = results_df,
      conditions_grid = x@conditions@conditions_grid,
      target_pwr = target_pwr,
      interim_overall = interim_overall,
      power_col = "pwr_scs"
    )

    # Optimal/Highest condition section
    if (optimal$found || optimal$mode == "highest") {
      if (optimal$mode == "highest") {
        cli::cli_h2("Highest Power Condition {.emph (id: {optimal$condition_id})}")
      } else {
        cli::cli_h2("Optimal Condition for {round(target_pwr * 100, 0)}% Power {.emph (id: {optimal$condition_id})}")
      }

      param_str <- fmt_params(optimal$condition_params)
      pwr_val <- round(optimal$achieved_pwr * 100, 1)
      cli::cli_bullets(c("*" = "Power: {.strong {pwr_val}%} | {param_str}"))

      if (!is.null(optimal$interim)) {
        int <- optimal$interim
        cli::cli_bullets(c(
          "*" = "Stopped early: {round(int$prop_stp_early * 100, 1)}% | N: mean={round(int$n_mn, 0)}, median={round(int$n_mdn, 0)}, mode={round(int$n_mode, 0)}"
        ))
      }
    } else if (!is.null(optimal$closest)) {
      cli::cli_alert_warning("No condition achieves {round(target_pwr * 100, 0)}% power")
      cli::cli_h2("Closest Condition {.emph (id: {optimal$closest$condition_id})}")

      param_str <- fmt_params(optimal$closest$condition_params)
      pwr_val <- round(optimal$closest$achieved_pwr * 100, 1)
      cli::cli_bullets(c("*" = "Power: {.strong {pwr_val}%} | {param_str}"))

      if (!is.null(optimal$closest$interim)) {
        int <- optimal$closest$interim
        cli::cli_bullets(c(
          "*" = "Stopped early: {round(int$prop_stp_early * 100, 1)}% | N: mean={round(int$n_mn, 0)}, median={round(int$n_mdn, 0)}, mode={round(int$n_mode, 0)}"
        ))
      }
    }
    # Hints
    cli::cli_text("")
    cli::cli_rule()
    if (is.null(target_pwr)) {
      cli::cli_alert_info("Find optimal N for targeted power: {.code print(x, target_pwr = 0.8)}")
    }
    cli::cli_alert_info("Visualize: {.code plot(x)}")

  } else {
    # Pending analysis
    n_conditions <- nrow(x@conditions@conditions_grid)
    cli::cli_alert_warning("Analysis not yet run")

    cli::cli_h2("Configuration")
    cli::cli_bullets(c(
      "*" = "Conditions: {n_conditions}",
      "*" = "Simulations: {x@n_sims} per condition",
      "*" = "Cores: {x@n_cores}"
    ))
    cli::cli_text("")
    cli::cli_rule()
    cli::cli_alert_info("Run: {.code run(x)}")
  }

  invisible(x)
}


#' Summary Method for rctbp_power_analysis Objects
#'
#' Displays a comprehensive summary of power analysis results including all
#' conditions and detailed statistics. For a compact overview, use [print()].
#'
#' @param object An S7 object of class "rctbp_power_analysis"
#' @param ... Additional arguments:
#'   \describe{
#'     \item{target_pwr}{Target power for finding optimal condition. If NULL
#'       (default), shows the condition with highest achieved power.}
#'   }
#'
#' @return Invisibly returns the input object. Used for side effects (printing).
#'
#' @details
#' The summary follows the same structure as [print()] but provides more detail:
#' \itemize{
#'   \item Design specifications with full boundary descriptions
#'   \item Results summary with power ranges
#'   \item Optimal/highest power condition with full details
#'   \item Per-condition results table
#'   \item Per-look results (for sequential designs)
#' }
#'
#' For topic-specific reports, see [report()].
#'
#' @seealso [print.rctbp_power_analysis()], [report()], [report_early_stopping()],
#'   [report_conditions()]
#'
#' @importFrom S7 method
#' @name summary.rctbp_power_analysis
#' @export
S7::method(summary, rctbp_power_analysis) <- function(object, ...) {
  dots <- list(...)
  target_pwr <- if (!is.null(dots$target_pwr)) {
    dots$target_pwr
  } else {
    object@conditions@target_pwr
  }

  design <- object@conditions@design
  has_results <- nrow(object@results_conditions) > 0 || nrow(object@results_raw) > 0

  # Helper to safely get range (handles missing/empty columns)
  safe_range <- function(x, pct = FALSE, digits = 1) {
    x <- x[!is.na(x)]
    if (length(x) == 0) return("N/A")
    fmt_range(x, pct = pct, digits = digits)
  }

  # Header
  cli::cli_h1("Power Analysis Summary")

  if (has_results) {
    n_conditions <- nrow(object@conditions@conditions_grid)
    has_interim <- object@has_interim
    results_df <- if (has_interim) object@results_interim else object@results_conditions
    interim_overall <- if (has_interim) object@results_conditions else NULL
    n_looks <- if (has_interim) length(unique(results_df$id_look)) else 1

    # Status line
    runtime <- if (!is.na(object@elapsed_time)) round(object@elapsed_time, 1) else NA
    design_type <- if (has_interim) paste0("Sequential (", n_looks, " looks)") else "Single-look"
    cli::cli_alert_success("Completed in {runtime} min | {n_conditions} conditions x {object@n_sims} sims | {design_type}")

    # Design section (more verbose than print)
    cli::cli_h2("Design")
    target_params_str <- paste(design@target_params, collapse = ", ")
    p_scs_str <- format_boundary(design@p_sig_scs)
    p_ftl_str <- format_boundary(design@p_sig_ftl)

    cli::cli_bullets(c(
      "*" = "Target parameters: {.field {target_params_str}}",
      "*" = paste0("Success probability threshold: ", p_scs_str),
      "*" = paste0("Futility probability threshold: ", p_ftl_str)
    ))

    if (!is.null(design@analysis_at)) {
      # Don't add "1.0 (final)" if 1 is already in the list
      if (1 %in% design@analysis_at) {
        analysis_pts <- paste(design@analysis_at, collapse = ", ")
      } else {
        analysis_pts <- paste(c(design@analysis_at, "1.0 (final)"), collapse = ", ")
      }
      cli::cli_bullets(c("*" = "Analysis timepoints: {analysis_pts}"))
    }

    if (!is.null(target_pwr)) {
      cli::cli_bullets(c("*" = "Target power: {.strong {round(target_pwr * 100, 0)}%}"))
    }

    # Decision Rates section
    cli::cli_h2("Decision Rates")
    pwr_scs_range <- safe_range(results_df$pwr_scs, pct = TRUE)
    pwr_ftl_range <- safe_range(results_df$pwr_ftl, pct = TRUE)
    cli::cli_bullets(c(
      "*" = "Success rate (range): {pwr_scs_range}",
      "*" = "Futility rate (range): {pwr_ftl_range}"
    ))
    if (has_interim && !is.null(interim_overall) && "prop_no_dec" %in% names(interim_overall)) {
      no_dec_range <- safe_range(interim_overall$prop_no_dec, pct = TRUE)
      cli::cli_bullets(c("*" = "No decision rate (range): {no_dec_range}"))
    }

    # Early Stopping section (for sequential, more verbose)
    if (has_interim && !is.null(interim_overall)) {
      cli::cli_h2("Early Stopping")
      bullets <- character()

      if ("prop_stp_early" %in% names(interim_overall)) {
        stp_range <- safe_range(interim_overall$prop_stp_early, pct = TRUE)
        bullets <- c(bullets, "*" = "Stopped early (total): {stp_range}")
      }
      if ("prop_stp_scs" %in% names(interim_overall)) {
        stp_scs_range <- safe_range(interim_overall$prop_stp_scs, pct = TRUE)
        bullets <- c(bullets, "*" = "Stopped for success: {stp_scs_range}")
      }
      if ("prop_stp_ftl" %in% names(interim_overall)) {
        stp_ftl_range <- safe_range(interim_overall$prop_stp_ftl, pct = TRUE)
        bullets <- c(bullets, "*" = "Stopped for futility: {stp_ftl_range}")
      }
      if ("n_mn" %in% names(interim_overall) && "n_planned" %in% names(interim_overall)) {
        n_mn_range <- safe_range(interim_overall$n_mn, digits = 0)
        n_planned_range <- safe_range(interim_overall$n_planned, digits = 0)
        bullets <- c(bullets, "*" = "Mean sample size: {n_mn_range} (of {n_planned_range} planned)")
      }
      if ("n_mdn" %in% names(interim_overall)) {
        n_mdn_range <- safe_range(interim_overall$n_mdn, digits = 0)
        bullets <- c(bullets, "*" = "Median sample size: {n_mdn_range}")
      }

      if (length(bullets) > 0) {
        cli::cli_bullets(bullets)
      }
    }

    # Find optimal condition
    optimal <- find_optimal_condition(
      results_summ = results_df,
      conditions_grid = object@conditions@conditions_grid,
      target_pwr = target_pwr,
      interim_overall = interim_overall,
      power_col = "pwr_scs"
    )

    # Optimal/Highest condition section (more verbose than print)
    if (optimal$found || optimal$mode == "highest") {
      if (optimal$mode == "highest") {
        cli::cli_h2("Highest Power Condition")
      } else {
        cli::cli_h2("Optimal Condition for {round(target_pwr * 100, 0)}% Power")
      }

      cli::cli_bullets(c(
        "*" = "Condition ID: {.val {optimal$condition_id}}",
        "*" = "Achieved power: {.strong {round(optimal$achieved_pwr * 100, 1)}%}",
        "*" = "Sample size (n_total): {.val {optimal$n_total}}"
      ))

      # Show all condition parameters (skip n_total since already shown)
      for (pname in names(optimal$condition_params)) {
        if (pname == "n_total") next
        pval <- optimal$condition_params[[pname]]
        if (is.numeric(pval)) pval <- round(pval, 3)
        cli::cli_bullets(c("*" = "{pname}: {.val {pval}}"))
      }

      if (!is.null(optimal$interim)) {
        int <- optimal$interim
        cli::cli_text("")
        cli::cli_text("{.emph Early stopping (this condition):}")
        bullets <- c("*" = "Stopped early: {round(int$prop_stp_early * 100, 1)}%")
        if (!is.null(int$prop_stp_scs) && !is.na(int$prop_stp_scs)) {
          bullets <- c(bullets, "*" = "Stopped for success: {round(int$prop_stp_scs * 100, 1)}%")
        }
        if (!is.null(int$prop_stp_ftl) && !is.na(int$prop_stp_ftl)) {
          bullets <- c(bullets, "*" = "Stopped for futility: {round(int$prop_stp_ftl * 100, 1)}%")
        }
        bullets <- c(bullets, "*" = "Mean N: {round(int$n_mn, 0)}, Median N: {round(int$n_mdn, 0)}, Mode N: {round(int$n_mode, 0)}")
        cli::cli_bullets(bullets)
      }
    } else if (!is.null(optimal$closest)) {
      cli::cli_alert_warning("No condition achieves {round(target_pwr * 100, 0)}% power")
      cli::cli_h2("Closest Condition")

      cli::cli_bullets(c(
        "*" = "Condition ID: {.val {optimal$closest$condition_id}}",
        "*" = "Achieved power: {.strong {round(optimal$closest$achieved_pwr * 100, 1)}%}",
        "*" = "Sample size (n_total): {.val {optimal$closest$n_total}}"
      ))

      for (pname in names(optimal$closest$condition_params)) {
        if (pname == "n_total") next
        pval <- optimal$closest$condition_params[[pname]]
        if (is.numeric(pval)) pval <- round(pval, 3)
        cli::cli_bullets(c("*" = "{pname}: {.val {pval}}"))
      }

      if (!is.null(optimal$closest$interim)) {
        int <- optimal$closest$interim
        cli::cli_text("")
        cli::cli_text("{.emph Early stopping (this condition):}")
        bullets <- c("*" = "Stopped early: {round(int$prop_stp_early * 100, 1)}%")
        if (!is.null(int$prop_stp_scs) && !is.na(int$prop_stp_scs)) {
          bullets <- c(bullets, "*" = "Stopped for success: {round(int$prop_stp_scs * 100, 1)}%")
        }
        if (!is.null(int$prop_stp_ftl) && !is.na(int$prop_stp_ftl)) {
          bullets <- c(bullets, "*" = "Stopped for futility: {round(int$prop_stp_ftl * 100, 1)}%")
        }
        bullets <- c(bullets, "*" = "Mean N: {round(int$n_mn, 0)}, Median N: {round(int$n_mdn, 0)}, Mode N: {round(int$n_mode, 0)}")
        cli::cli_bullets(bullets)
      }
    }

    # Per-condition results table
    cli::cli_h2("All Conditions")
    if (has_interim) {
      # For sequential: get power from final look of results_interim, merge with interim_overall
      final_look <- max(results_df$id_look)
      final_power <- results_df[results_df$id_look == final_look,
                                c("id_cond", "pwr_scs", "pwr_ftl"), drop = FALSE]

      # Merge with interim_overall stats
      if (!is.null(interim_overall) && nrow(interim_overall) > 0) {
        cond_table <- merge(
          interim_overall[, intersect(c("id_cond", "n_total", "prop_stp_early", "n_mn"),
                                      names(interim_overall)), drop = FALSE],
          final_power,
          by = "id_cond",
          all.x = TRUE
        )
        # Reorder columns
        col_order <- c("id_cond", "n_total", "pwr_scs", "pwr_ftl", "prop_stp_early", "n_mn")
        col_order <- intersect(col_order, names(cond_table))
        cond_table <- cond_table[, col_order, drop = FALSE]
        # Sort by power
        if ("pwr_scs" %in% names(cond_table)) {
          cond_table <- cond_table[order(-cond_table$pwr_scs), , drop = FALSE]
        }
      } else {
        cond_table <- final_power
      }
    } else {
      # Single-look results
      cols <- c("id_cond", "n_total", "pwr_scs", "pwr_ftl")
      cols_available <- intersect(cols, names(results_df))
      cond_table <- results_df[, cols_available, drop = FALSE]
      if ("pwr_scs" %in% names(cond_table)) {
        cond_table <- cond_table[order(-cond_table$pwr_scs), , drop = FALSE]
      }
    }
    table_lines <- format_table_cli(cond_table)
    cat(paste(table_lines, collapse = "\n"), "\n")

    # Hints
    cli::cli_text("")
    cli::cli_rule()
    if (is.null(target_pwr)) {
      cli::cli_alert_info("Find optimal N: {.code print(x, target_pwr = 0.8)}")
    }
    cli::cli_alert_info("Visualize: {.code plot(x)}")
    if (has_interim) {
      cli::cli_alert_info("Early stopping details: {.code report(x, 'early_stopping')}")
    }

  } else {
    # Pending analysis
    n_conditions <- nrow(object@conditions@conditions_grid)
    cli::cli_alert_warning("Analysis not yet run")

    cli::cli_h2("Configuration")
    cli::cli_bullets(c(
      "*" = "Conditions: {n_conditions}",
      "*" = "Simulations per condition: {object@n_sims}",
      "*" = "Cores: {object@n_cores}",
      "*" = "Verbose: {object@verbose}"
    ))

    # Show design details
    cli::cli_h2("Design")
    target_params_str <- paste(design@target_params, collapse = ", ")
    p_scs_str <- format_boundary(design@p_sig_scs)
    p_ftl_str <- format_boundary(design@p_sig_ftl)
    cli::cli_bullets(c(
      "*" = "Target parameters: {.field {target_params_str}}",
      "*" = paste0("Success threshold: ", p_scs_str),
      "*" = paste0("Futility threshold: ", p_ftl_str)
    ))

    if (!is.null(design@analysis_at)) {
      if (1 %in% design@analysis_at) {
        analysis_pts <- paste(design@analysis_at, collapse = ", ")
      } else {
        analysis_pts <- paste(c(design@analysis_at, "1.0 (final)"), collapse = ", ")
      }
      cli::cli_bullets(c("*" = "Analysis timepoints: {analysis_pts}"))
    }

    # Show conditions grid preview
    cli::cli_h2("Conditions Grid")
    grid_preview <- object@conditions@conditions_grid
    table_lines <- format_table_cli(grid_preview)
    cat(paste(table_lines, collapse = "\n"), "\n")

    cli::cli_text("")
    cli::cli_rule()
    cli::cli_alert_info("Run analysis: {.code run(x)}")
  }

  invisible(object)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/compute_measures.R"
#' Compute Power Measures from Posterior rvars (Backend-Agnostic)
#'
#' This function computes power analysis measures from posterior samples in rvar format.
#' It works with posteriors from any backend (brms, NPE, etc.) as long as they are
#' converted to rvar format first.
#'
#' @param posterior_rvars A draws_rvars object containing posterior samples for target parameters
#' @param target_params Character vector of parameter names to analyze
#' @param thresholds_success Numeric vector of success thresholds (one per parameter)
#' @param thresholds_futility Numeric vector of futility thresholds (one per parameter)
#' @param p_sig_scs Probability threshold for declaring success
#' @param p_sig_ftl Probability threshold for declaring futility
#'
#' @return A data frame containing power analysis measures
#' @importFrom stats median
#' @keywords internal
compute_measures <- function(posterior_rvars, target_params, thresholds_success,
                             thresholds_futility, p_sig_scs, p_sig_ftl) {

  # Compute measures for each parameter
  measures_list <- purrr::map(target_params, function(param) {
    # Extract rvar for this parameter
    param_rvar <- posterior_rvars[[param]]

    # extract thresholds
    if (length(target_params) > 1) {
      threshold_success <- thresholds_success[which(target_params == param)]
      # if the threshold is NA, use the first one
      if (is.na(threshold_success)) {
        threshold_success <- thresholds_success[1]
      }
      threshold_futility <- thresholds_futility[which(target_params == param)]
      # if the threshold is NA, use the first one
      if (is.na(threshold_futility)) {
        threshold_futility <- thresholds_futility[1]
      }
    } else {
      threshold_success <- thresholds_success
      threshold_futility <- thresholds_futility
    }
    # calculate the probability of success / futility
    success_prob <- posterior::Pr(param_rvar > threshold_success)
    futility_prob <- posterior::Pr(param_rvar < threshold_futility)
    # significance
    sig_success <- as.numeric(success_prob >= p_sig_scs, na.rm = TRUE)
    sig_futility <- as.numeric(futility_prob >= p_sig_ftl, na.rm = TRUE)
    # parameter estimates
    est_median <- stats::median(param_rvar)
    est_mad <- posterior::mad(param_rvar)
    est_mean <- mean(param_rvar)
    est_sd <- posterior::sd(param_rvar)
    # convergence metrics
    rhat <- posterior::rhat(param_rvar)
    ess_bulk <- posterior::ess_bulk(param_rvar)
    ess_tail <- posterior::ess_tail(param_rvar)

    # combine results into a list
    out_list <- list(
      par_name = param,
      thr_scs = threshold_success,
      thr_ftl = threshold_futility,
      p_sig_scs = p_sig_scs,
      p_sig_ftl = p_sig_ftl,
      pr_scs = success_prob,
      pr_ftl = futility_prob,
      dec_scs = sig_success,
      dec_ftl = sig_futility,
      post_med = est_median,
      post_mad = est_mad,
      post_mn = est_mean,
      post_sd = est_sd,
      rhat = rhat,
      ess_bulk = ess_bulk,
      ess_tail = ess_tail
    )

    return(out_list)
  })

  # compute combined probabilities and powers
  if (length(target_params) > 1) {
    # Convert rvars to matrix for combined calculations
    # Extract draws as matrix: each column is a parameter
    posterior_matrix_list <- lapply(target_params, function(param) {
      as.vector(posterior::draws_of(posterior_rvars[[param]]))
    })
    posterior_samples <- do.call(cbind, posterior_matrix_list)

    # extract thresholds and broadcast if necessary
    if (length(target_params) > length(thresholds_success)) {
      thresholds_success_combined <- rep(thresholds_success[1], length(target_params))
    } else {
      thresholds_success_combined <- thresholds_success
    }
    if (length(target_params) > length(thresholds_futility)) {
      thresholds_futility_combined <- rep(thresholds_futility[1], length(target_params))
    } else {
      thresholds_futility_combined <- thresholds_futility
    }

    # Combined probability algorithm (AND decision rule for multiple parameters)
    # All parameters must simultaneously exceed threshold (conjunctive logic)
    # Algorithm:
    #   1. Convert each parameter to binary: exceeds threshold? (1/0)
    #   2. Take minimum across parameters for each draw (AND operation)
    #   3. Average across draws to get probability
    # Example: P(param1 > 0.2 AND param2 > 0.3 AND param3 > 0.1)
    combined_success_prob <- mean(apply(ifelse(
      posterior_samples > thresholds_success_combined, 1, 0
    ), 1, min))
    combined_futility_prob <- mean(apply(
      ifelse(posterior_samples < thresholds_futility_combined, 1, 0),
      1,
      min
    ))

    # calculate combined significance
    combined_sig_success <- as.numeric(combined_success_prob >= p_sig_scs, na.rm = TRUE)
    combined_sig_futility <- as.numeric(combined_futility_prob >= p_sig_ftl, na.rm = TRUE)

    # combine results into a list
    measures_list_combined <- list(
      par_name = "union",
      thr_scs = NA,
      thr_ftl = NA,
      p_sig_scs = p_sig_scs,
      p_sig_ftl = p_sig_ftl,
      pr_scs = combined_success_prob,
      pr_ftl = combined_futility_prob,
      dec_scs = combined_sig_success,
      dec_ftl = combined_sig_futility,
      post_med = NA,
      post_mad = NA,
      post_mn = NA,
      post_sd = NA,
      rhat = NA,
      ess_bulk = NA,
      ess_tail = NA
    )
  }

  # remove success_samples and futility_samples from the list
  measures_list <- purrr::map(measures_list, function(x) {
    x$success_samples <- NULL
    x$futility_samples <- NULL
    return(x)
  })

  # make data.frames and rbind()
  measures_df <- do.call(rbind, measures_list)

  if (length(target_params) > 1) {
    # create a data frame for combined measures
    measures_df_combined <- do.call(cbind, measures_list_combined)
    # add combined measures
    measures_df <- as.data.frame(rbind(measures_df, measures_df_combined))
  } else {
    measures_df <- as.data.frame(measures_df)
  }

  return(measures_df)
}


#' Compute Power Measures from brms Model Fit (Wrapper for Backward Compatibility)
#'
#' This function extracts posterior samples from a fitted brms model and computes
#' various power analysis measures. It is a wrapper around compute_measures() that
#' first extracts rvars from the brmsfit object.
#'
#' @param brmsfit A fitted brms model object containing posterior samples
#' @param design A list or S7 design object containing the experimental design specification
#'
#' @return A data frame containing power analysis measures
#' @seealso [compute_measures()]
#' @keywords internal
compute_measures_brmsfit <- function(brmsfit, design) {
  # Validate inputs
  if (!inherits(brmsfit, "brmsfit")) {
    cli::cli_abort(c(
      "{.arg brmsfit} must be a fitted brms model object",
      "x" = "You supplied {.cls {class(brmsfit)}}",
      "i" = "Provide a fitted {.cls brmsfit} object"
    ))
  }

  # Extract target parameters from design
  if (inherits(design, "rctbayespower::rctbp_design") || inherits(design, "rctbp_design")) {
    target_params <- design@target_params
  } else if (is.list(design)) {
    target_params <- design$target_params
  } else {
    cli::cli_abort(c(
      "Invalid design object",
      "i" = "This is an internal error - please report"
    ))
  }

  # Extract posterior rvars from brms model
  posterior_rvars <- brms::as_draws_rvars(brmsfit, variable = target_params)

  # Call backend-agnostic compute_measures
  compute_measures(posterior_rvars, design)
}


#' Summarize Power Analysis Simulation Results
#'
#' This function aggregates raw simulation results across multiple runs to compute
#' summary statistics including power estimates, parameter estimates, convergence
#' metrics, and Monte Carlo standard errors.
#'
#' @param results_df_raw A data frame containing raw simulation results with columns:
#'   \itemize{
#'     \item `id_cond`: Condition identifier
#'     \item `par_name`: Parameter name
#'     \item `thr_scs`: Success threshold for the parameter
#'     \item `thr_ftl`: Futility threshold for the parameter
#'     \item `pr_scs`: Probability of success for each simulation
#'     \item `pr_ftl`: Probability of futility for each simulation
#'     \item `dec_scs`: Binary success decision indicator
#'     \item `dec_ftl`: Binary futility decision indicator
#'     \item `post_med`: Posterior median estimates
#'     \item `post_mad`: Posterior median absolute deviation
#'     \item `post_mn`: Posterior mean estimates
#'     \item `post_sd`: Posterior standard deviation
#'     \item `rhat`: R-hat convergence diagnostic
#'     \item `ess_bulk`: Bulk effective sample size
#'     \item `ess_tail`: Tail effective sample size
#'     \item `converged`: Convergence status indicator
#'     \item `error_msg`: Error messages (if any)
#'   }
#' @param n_sims Integer specifying the total number of simulations run
#'
#' @return A data frame with summarized results grouped by condition and parameter,
#'   containing mean estimates and Monte Carlo standard errors (MCSE) for all metrics:
#'   \itemize{
#'     \item Probability estimates: `pr_scs_mean`, `pr_ftl_mean` with `*_mcse`
#'     \item Power estimates: `pwr_scs_mean`, `pwr_ftl_mean` with `*_mcse`
#'     \item Parameter estimates: `post_med_mean`, `post_mn_mean`, `post_mad_mean`, `post_sd_mean`
#'     \item Convergence metrics: `rhat_mean`, `ess_bulk_mean`, `ess_tail_mean`, `conv_rate_mean`
#'   }
#'   Each metric includes corresponding `*_mcse` columns with Monte Carlo standard errors.
#'
#' @details
#' The function groups results by condition ID, parameter, and thresholds, then computes:
#' \itemize{
#'   \item Mean values across simulations for all continuous metrics
#'   \item Power as the proportion of simulations meeting significance criteria
#'   \item Convergence rate as the proportion of successfully converged simulations
#'   \item Monte Carlo standard errors for uncertainty quantification
#'   \item Concatenated error messages for debugging purposes
#' }
#'
#' @seealso [compute_measures_brmsfit()], [summarize_sims_with_interim()]
#' @keywords internal
summarize_sims <- function(results_df_raw, n_sims) {
  # Validate input
  if (!is.data.frame(results_df_raw) || nrow(results_df_raw) == 0) {
    cli::cli_abort(c(
      "{.arg results_df_raw} must be a non-empty data frame",
      "x" = "You supplied {.type {results_df_raw}} with {.val {nrow(results_df_raw)}} rows",
      "i" = "This is an internal error - please report"
    ))
  }

  # =============================================================================
  # INTERIM ANALYSIS DETECTION
  # =============================================================================
  # Detect if results contain interim analyses by checking:

  # 1. Required interim columns exist (id_look, n_analyzed, stopped, stop_reason)
  # 2. More than one unique analysis index exists (multiple looks per simulation)
  #
  # Single-look designs have id_look = 1 for all rows, so we only dispatch to
  # interim summarization when there are actually multiple analysis timepoints.
  has_interim_cols <- all(c("id_look", "n_analyzed", "stopped", "stop_reason") %in%
                            names(results_df_raw))
  has_multiple_analyses <- has_interim_cols &&
    length(unique(results_df_raw$id_look)) > 1

  if (has_multiple_analyses) {
    return(summarize_sims_with_interim(results_df_raw, n_sims))
  }

  # =============================================================================
  # STANDARD (SINGLE-LOOK) SUMMARIZATION
  # =============================================================================
  # remove rows with NA in id_cond or par_name
  results_df_raw <- results_df_raw |>
    dplyr::filter(!is.na(id_cond) & !is.na(par_name))


  results_summarized <- results_df_raw |>
    dplyr::group_by(id_cond, par_name, thr_scs, thr_ftl, p_sig_scs, p_sig_ftl) |>
    dplyr::summarise(
      pr_scs_mean = mean(pr_scs, na.rm = TRUE),
      pr_scs_mcse = calculate_mcse_mean(pr_scs, n_sims),
      pr_ftl_mean = mean(pr_ftl, na.rm = TRUE),
      pr_ftl_mcse = calculate_mcse_mean(pr_ftl, n_sims),
      pwr_scs_mean = mean(dec_scs, na.rm = TRUE),
      pwr_scs_mcse = calculate_mcse_power(dec_scs, n_sims),
      pwr_ftl_mean = mean(dec_ftl, na.rm = TRUE),
      pwr_ftl_mcse = calculate_mcse_power(dec_ftl, n_sims),
      post_med_mean = mean(.data$post_med, na.rm = TRUE),
      post_med_mcse = calculate_mcse_mean(.data$post_med, n_sims),
      post_mad_mean = mean(.data$post_mad, na.rm = TRUE),
      post_mad_mcse = calculate_mcse_mean(.data$post_mad, n_sims),
      post_mn_mean = mean(.data$post_mn, na.rm = TRUE),
      post_mn_mcse = calculate_mcse_mean(.data$post_mn, n_sims),
      post_sd_mean = mean(.data$post_sd, na.rm = TRUE),
      post_sd_mcse = calculate_mcse_mean(.data$post_sd, n_sims),
      rhat_mean = mean(rhat, na.rm = TRUE),
      rhat_mcse = calculate_mcse_mean(rhat, n_sims),
      ess_bulk_mean = mean(ess_bulk, na.rm = TRUE),
      ess_bulk_mcse = calculate_mcse_mean(ess_bulk, n_sims),
      ess_tail_mean = mean(ess_tail, na.rm = TRUE),
      ess_tail_mcse = calculate_mcse_mean(ess_tail, n_sims),
      conv_rate_mean = mean(converged, na.rm = TRUE),
      conv_rate_mcse = calculate_mcse_power(converged, n_sims),
      .groups = "drop"
    ) |>
    # make shorter names
    dplyr::rename(
      pr_scs = pr_scs_mean,
      se_pr_scs = pr_scs_mcse,
      pr_ftl = pr_ftl_mean,
      se_pr_ftl = pr_ftl_mcse,
      pwr_scs = pwr_scs_mean,
      se_pwr_scs = pwr_scs_mcse,
      pwr_ftl = pwr_ftl_mean,
      se_pwr_ftl = pwr_ftl_mcse,
      post_med = post_med_mean,
      se_post_med = post_med_mcse,
      post_mad = post_mad_mean,
      se_post_mad = post_mad_mcse,
      post_mn = post_mn_mean,
      se_post_mn = post_mn_mcse,
      post_sd = post_sd_mean,
      se_post_sd = post_sd_mcse,
      rhat = rhat_mean,
      se_rhat = rhat_mcse,
      ess_bulk = ess_bulk_mean,
      se_ess_bulk = ess_bulk_mcse,
      ess_tail = ess_tail_mean,
      se_ess_tail = ess_tail_mcse,
      conv_rate = conv_rate_mean,
      se_conv_rate = conv_rate_mcse
    )

  return(results_summarized)
}


#' Summarize Power Analysis Results with Interim Analyses
#'
#' Aggregates raw simulation results for sequential designs with interim analyses.
#' Provides both per-look summaries and overall trial outcome statistics.
#'
#' @param results_df_raw A data frame containing raw simulation results including
#'   interim analysis columns (`id_look`, `n_analyzed`, `stopped`, `stop_reason`)
#' @param n_sims Integer specifying the total number of simulations run
#'
#' @return A list with two data frames:
#'   \describe{
#'     \item{by_look}{Summary statistics grouped by condition, parameter, and interim look}
#'     \item{overall}{Overall trial outcome statistics including stopping rates and sample size}
#'   }
#'
#' @details
#' This function is called automatically by [summarize_sims()] when interim analysis
#' results are detected. It computes:
#'
#' \strong{Per-Look Metrics (by_look):}
#' \itemize{
#'   \item Power metrics: `pr_scs`, `pr_ftl`, `pwr_scs`, `pwr_ftl` (with SEs)
#'   \item Posterior estimates: `post_med`, `post_mn`, `post_sd` (with SEs)
#'   \item Convergence: `rhat`, `ess_bulk`, `conv_rate`
#'   \item Stopping at this look: `prop_stp_look`, `prop_scs_look`, `prop_ftl_look`
#'   \item Cumulative stopping: `cumul_stp`
#' }
#'
#' \strong{Overall Metrics (overall):}
#' \itemize{
#'   \item `n_planned`: Maximum planned sample size
#'   \item `n_mn`, `se_n_mn`: Mean sample size with standard error
#'   \item `n_mdn`: Robust median (always an observed value, uses low median for ties)
#'   \item `n_mode`: Modal sample size (most frequent stopping point)
#'   \item `prop_at_mode`: Proportion of trials at modal N
#'   \item `prop_stp_early`: Proportion stopped before final look
#'   \item `prop_stp_scs`: Proportion stopped for success
#'   \item `prop_stp_ftl`: Proportion stopped for futility
#'   \item `prop_no_dec`: Proportion with no decision (= 1 - prop_stp_scs - prop_stp_ftl)
#' }
#'
#' @seealso [summarize_sims()]
#' @keywords internal
summarize_sims_with_interim <- function(results_df_raw, n_sims) {
  # Validate input
  if (!is.data.frame(results_df_raw) || nrow(results_df_raw) == 0) {
    cli::cli_abort(c(
      "{.arg results_df_raw} must be a non-empty data frame",
      "x" = "You supplied {.type {results_df_raw}} with {.val {nrow(results_df_raw)}} rows"
    ))
  }

  # Check required interim columns exist
 required_cols <- c("id_look", "n_analyzed", "stopped", "stop_reason")
  missing_cols <- setdiff(required_cols, names(results_df_raw))
  if (length(missing_cols) > 0) {
    cli::cli_abort(c(
      "Missing required interim analysis columns",
      "x" = "Missing: {.val {missing_cols}}",
      "i" = "This function requires results from sequential estimation"
    ))
  }

  # Remove rows with NA in key identifiers
  results_df_raw <- results_df_raw |>
    dplyr::filter(!is.na(id_cond) & !is.na(par_name))

  # =============================================================================
  # PER-LOOK SUMMARY
  # =============================================================================

  # First compute per-look stopping stats (condition  look level)
  # These will be joined to by_look (redundant across parameters but keeps it simple)
  n_total_sims <- results_df_raw |>
    dplyr::group_by(id_cond) |>
    dplyr::summarise(n_total_sims = dplyr::n_distinct(id_iter), .groups = "drop")

  # Count stops at each look
  stopping_by_look <- results_df_raw |>
    dplyr::filter(!is.na(stop_reason)) |>
    dplyr::group_by(id_cond, id_look, n_analyzed) |>
    dplyr::summarise(
      n_stp_look = dplyr::n_distinct(id_iter),
      n_scs_look = sum(stop_reason == "stop_success", na.rm = TRUE),
      n_ftl_look = sum(stop_reason == "stop_futility", na.rm = TRUE),
      .groups = "drop"
    ) |>
    dplyr::left_join(n_total_sims, by = "id_cond") |>
    dplyr::mutate(
      prop_stp_look = n_stp_look / n_total_sims,
      prop_scs_look = n_scs_look / n_total_sims,
      prop_ftl_look = n_ftl_look / n_total_sims
    ) |>
    dplyr::group_by(id_cond) |>
    dplyr::arrange(id_look) |>
    dplyr::mutate(cumul_stp = cumsum(prop_stp_look)) |>
    dplyr::ungroup() |>
    dplyr::select(id_cond, id_look, prop_stp_look, prop_scs_look, prop_ftl_look, cumul_stp)

  # Group by condition, parameter, threshold, AND analysis look for power metrics
  by_look <- results_df_raw |>
    dplyr::group_by(id_cond, par_name, thr_scs, thr_ftl, p_sig_scs, p_sig_ftl, id_look, n_analyzed) |>
    dplyr::summarise(
      # Standard metrics
      pr_scs = mean(pr_scs, na.rm = TRUE),
      se_pr_scs = calculate_mcse_mean(pr_scs, n_sims),
      pr_ftl = mean(pr_ftl, na.rm = TRUE),
      se_pr_ftl = calculate_mcse_mean(pr_ftl, n_sims),
      pwr_scs = mean(dec_scs, na.rm = TRUE),
      se_pwr_scs = calculate_mcse_power(dec_scs, n_sims),
      pwr_ftl = mean(dec_ftl, na.rm = TRUE),
      se_pwr_ftl = calculate_mcse_power(dec_ftl, n_sims),
      post_med = mean(.data$post_med, na.rm = TRUE),
      se_post_med = calculate_mcse_mean(.data$post_med, n_sims),
      post_mn = mean(.data$post_mn, na.rm = TRUE),
      se_post_mn = calculate_mcse_mean(.data$post_mn, n_sims),
      post_sd = mean(.data$post_sd, na.rm = TRUE),
      se_post_sd = calculate_mcse_mean(.data$post_sd, n_sims),
      # Convergence
      rhat = mean(rhat, na.rm = TRUE),
      ess_bulk = mean(ess_bulk, na.rm = TRUE),
      conv_rate = mean(converged, na.rm = TRUE),
      .groups = "drop"
    ) |>
    # Join per-look stopping stats (redundant across parameters)
    dplyr::left_join(stopping_by_look, by = c("id_cond", "id_look")) |>
    # Fill NA stopping stats with 0 (looks where no trials stopped)
    dplyr::mutate(
      prop_stp_look = dplyr::if_else(is.na(prop_stp_look), 0, prop_stp_look),
      prop_scs_look = dplyr::if_else(is.na(prop_scs_look), 0, prop_scs_look),
      prop_ftl_look = dplyr::if_else(is.na(prop_ftl_look), 0, prop_ftl_look),
      cumul_stp = dplyr::if_else(is.na(cumul_stp), 0, cumul_stp)
    )

  # =============================================================================
  # OVERALL TRIAL SUMMARY (per condition)
  # =============================================================================
  # For overall metrics, we need to look at the final state of each simulation
  # Get final analysis for each simulation (last id_look per id_iter/id_cond)
  # Also need to track where stopping actually occurred

  # Get the analysis where stopping occurred (if any) for each simulation
  stopping_info <- results_df_raw |>
    dplyr::filter(!is.na(stop_reason)) |>
    dplyr::group_by(id_cond, id_iter) |>
    dplyr::slice_min(id_look, n = 1, with_ties = FALSE) |>
    dplyr::ungroup() |>
    dplyr::select(id_cond, id_iter, stop_n = n_analyzed, stop_reason)

  # Get all unique simulation runs
  all_sims <- results_df_raw |>
    dplyr::select(id_cond, id_iter) |>
    dplyr::distinct()

  # Join to get stopping info for each sim (NA if no stop)
  sim_outcomes <- all_sims |>
    dplyr::left_join(stopping_info, by = c("id_cond", "id_iter"))

  # Get planned n_total (max n_analyzed per condition)
  planned_n <- results_df_raw |>
    dplyr::group_by(id_cond) |>
    dplyr::summarise(n_planned = max(n_analyzed, na.rm = TRUE), .groups = "drop")

  sim_outcomes <- sim_outcomes |>
    dplyr::left_join(planned_n, by = "id_cond") |>
    dplyr::mutate(
      # If no stop, the effective N is n_planned
      effective_n = dplyr::if_else(is.na(stop_n), n_planned, stop_n),
      stopped_early = !is.na(stop_n)
    )


  # Helper to compute mode (most frequent value) and proportion at mode
  calc_mode <- function(x) {
    x <- x[!is.na(x)]
    if (length(x) == 0) return(NA_real_)
    freq_table <- table(x)
    as.numeric(names(freq_table)[which.max(freq_table)])
  }

  calc_prop_at_mode <- function(x) {
    x <- x[!is.na(x)]
    if (length(x) == 0) return(NA_real_)
    freq_table <- table(x)
    max(freq_table) / length(x)
  }

  # Robust median: returns actual observed value (low median for even n)
  # Unlike stats::median which averages two middle values, this always
  # returns a value that was actually observed in the data
  calc_robust_median <- function(x) {
    x <- sort(x[!is.na(x)])
    n <- length(x)
    if (n == 0) return(NA_real_)
    # For odd n: middle value; for even n: lower of two middle values
    x[(n + 1) %/% 2]
  }

  # Compute overall summaries by condition
  overall <- sim_outcomes |>
    dplyr::group_by(id_cond) |>
    dplyr::summarise(
      n_planned = dplyr::first(n_planned),
      n_mn = mean(effective_n, na.rm = TRUE),
      se_n_mn = stats::sd(effective_n, na.rm = TRUE) / sqrt(dplyr::n()),
      n_mdn = calc_robust_median(effective_n),
      n_mode = calc_mode(effective_n),
      prop_at_mode = calc_prop_at_mode(effective_n),
      prop_stp_early = mean(stopped_early, na.rm = TRUE),
      prop_stp_scs = mean(stop_reason == "stop_success", na.rm = TRUE),
      prop_stp_ftl = mean(stop_reason == "stop_futility", na.rm = TRUE),
      prop_no_dec = 1 - prop_stp_scs - prop_stp_ftl,
      .groups = "drop"
    )

  # Add power metrics from final look to overall
  final_look_id <- max(by_look$id_look)
  final_power <- by_look |>
    dplyr::filter(.data$id_look == final_look_id) |>
    dplyr::select("id_cond", "pwr_scs", "pwr_ftl", "se_pwr_scs", "se_pwr_ftl")

  # Add n_total (max n_analyzed per condition)
  n_total_df <- results_df_raw |>
    dplyr::group_by(.data$id_cond) |>
    dplyr::summarise(n_total = max(.data$n_analyzed, na.rm = TRUE), .groups = "drop")

  # Merge and reorder columns logically
  overall <- overall |>
    dplyr::left_join(final_power, by = "id_cond") |>
    dplyr::left_join(n_total_df, by = "id_cond") |>
    dplyr::select(
      "id_cond",
      # Sample size metrics
      "n_total", "n_planned", "n_mn", "n_mdn", "n_mode", "se_n_mn", "prop_at_mode",
      # Power metrics
      "pwr_scs", "pwr_ftl", "se_pwr_scs", "se_pwr_ftl",
      # Stopping proportions
      "prop_stp_early", "prop_stp_scs", "prop_stp_ftl", "prop_no_dec"
    )

  return(list(
    by_look = by_look,
    overall = overall
  ))
}


# =============================================================================
# POST-HOC BOUNDARY RE-ANALYSIS
# =============================================================================

#' Re-summarize Results with Different Boundaries
#'
#' Apply different stopping boundaries to existing simulation results and return
#' a new power_analysis object with updated results. Useful for comparing boundary
#' configurations without re-running simulations.
#'
#' @param power_result An rctbp_power_analysis object with completed results
#' @param p_sig_scs New success boundary specification. Can be:
#'   \itemize{
#'     \item NULL to keep original thresholds
#'     \item A single numeric value (same threshold at all looks)
#'     \item A numeric vector (one threshold per look)
#'     \item A boundary function from [boundary_obf()], [boundary_linear()], etc.
#'   }
#' @param p_sig_ftl New futility boundary specification (same format as p_sig_scs)
#'
#' @return A new rctbp_power_analysis object with recomputed:
#'   \itemize{
#'     \item `results_raw`: Updated decisions and stopping based on new boundaries
#'     \item `results_interim`: Re-summarized by-look statistics
#'     \item `results_conditions`: Re-summarized overall statistics
#'   }
#'
#' @details
#' This function takes the raw posterior probabilities (`pr_scs`, `pr_ftl`) from
#' existing simulation results and recomputes the binary decisions (`dec_scs`,
#' `dec_ftl`) and stopping behavior using new probability thresholds.
#'
#' Since posterior probabilities are stored in results_raw, different threshold
#' configurations can be explored without re-running the computationally expensive
#' posterior estimation.
#'
#' @export
#' @seealso [compare_boundaries()], [boundary_obf()], [boundary_linear()]
#'
#' @examples
#' \dontrun{
#' # Run simulation once
#' result <- power_analysis(conditions, n_sims = 500, analysis_at = c(0.5, 0.75))
#'
#' # Re-analyze with O'Brien-Fleming-style boundaries
#' result_obf <- resummarize_boundaries(
#'   result,
#'   p_sig_scs = boundary_obf(0.975),
#'   p_sig_ftl = boundary_linear(0.70, 0.90)
#' )
#'
#' # Compare results
#' print(result)      # Original boundaries
#' print(result_obf)  # OBF boundaries
#' }
resummarize_boundaries <- function(power_result,
                                    p_sig_scs = NULL,
                                    p_sig_ftl = NULL) {

  # Validate input
 if (!inherits(power_result, "rctbayespower::rctbp_power_analysis") &&
      !inherits(power_result, "rctbp_power_analysis")) {
    cli::cli_abort(c(
      "{.arg power_result} must be an rctbp_power_analysis object",
      "x" = "Got object of class {.cls {class(power_result)}}"
    ))
  }

  results_raw <- power_result@results_raw
  n_sims <- power_result@n_sims

  # Check if this is a sequential design (has multiple looks)
  if (!"id_look" %in% names(results_raw) ||
      length(unique(results_raw$id_look)) <= 1) {
    cli::cli_abort(c(
      "Cannot re-analyze boundaries for single-look designs",
      "i" = "This function requires sequential designs with analysis_at specified"
    ))
  }

  # Get look structure for resolving functions
  look_info <- results_raw |>
    dplyr::select(id_look, n_analyzed) |>
    dplyr::distinct() |>
    dplyr::arrange(id_look)

  n_total <- max(look_info$n_analyzed)

  # Use original thresholds if not specified
  if (is.null(p_sig_scs)) p_sig_scs <- power_result@design@p_sig_scs
  if (is.null(p_sig_ftl)) p_sig_ftl <- power_result@design@p_sig_ftl

  # Resolve boundaries to per-look values
  scs_thresholds <- resolve_boundary_vector(p_sig_scs, look_info, n_total)
  ftl_thresholds <- resolve_boundary_vector(p_sig_ftl, look_info, n_total)

  # Create threshold lookup
  threshold_df <- look_info |>
    dplyr::mutate(
      p_sig_scs_new = scs_thresholds,
      p_sig_ftl_new = ftl_thresholds
    )

  # Recompute decisions with new thresholds
  results_recomputed <- results_raw |>
    dplyr::left_join(threshold_df, by = c("id_look", "n_analyzed")) |>
    dplyr::group_by(.data$id_cond, .data$id_iter, .data$par_name) |>
    dplyr::arrange(.data$id_look) |>
    dplyr::mutate(
      # Recompute binary decisions
      dec_scs = as.integer(.data$pr_scs >= .data$p_sig_scs_new),
      dec_ftl = as.integer(.data$pr_ftl >= .data$p_sig_ftl_new),
      # Update p_sig columns to show what was applied
      p_sig_scs = .data$p_sig_scs_new,
      p_sig_ftl = .data$p_sig_ftl_new,
      # Recompute stopping
      triggers_stop = (.data$dec_scs == 1) | (.data$dec_ftl == 1 & .data$dec_scs == 0),
      already_stopped = cumsum(dplyr::lag(.data$triggers_stop, default = FALSE)) > 0,
      is_stop_point = .data$triggers_stop & !.data$already_stopped,
      stopped = cumsum(.data$is_stop_point) > 0,
      stop_reason = dplyr::case_when(
        .data$is_stop_point & .data$dec_scs == 1 ~ "stop_success",
        .data$is_stop_point & .data$dec_ftl == 1 ~ "stop_futility",
        TRUE ~ NA_character_
      )
    ) |>
    dplyr::ungroup() |>
    dplyr::select(-"p_sig_scs_new", -"p_sig_ftl_new", -"triggers_stop",
                  -"already_stopped", -"is_stop_point")

  # Summarize with existing function
  summary_result <- summarize_sims_with_interim(results_recomputed, n_sims)

  # Create new power_analysis object with updated results
  new_result <- power_result
  new_result@results_raw <- results_recomputed
  new_result@results_interim <- summary_result$by_look
  new_result@results_conditions <- summary_result$overall

  new_result
}


#' Compare Multiple Boundary Configurations
#'
#' Evaluate the same simulation results under multiple stopping boundary
#' specifications. Returns a summary comparing operating characteristics
#' across all boundary configurations.
#'
#' @param power_result An rctbp_power_analysis object with completed results
#' @param boundaries Named list of boundary specifications. Each element should
#'   be a list with `success` and/or `futility` components specifying the
#'   boundary for that configuration. Components can be numeric values or
#'   boundary functions.
#'
#' @return A data frame with one row per boundary configuration per condition,
#'   containing operating characteristics:
#'   \itemize{
#'     \item `boundary`: Name of the boundary configuration
#'     \item `id_cond`: Condition identifier
#'     \item `n_planned`, `n_mn`, `n_mdn`: Sample size statistics
#'     \item `prop_stp_early`, `prop_stp_scs`, `prop_stp_ftl`: Stopping proportions
#'   }
#'
#' @export
#' @seealso [resummarize_boundaries()], [boundary_obf()], [boundary_linear()]
#'
#' @examples
#' \dontrun{
#' # Run simulation once
#' result <- power_analysis(conditions, n_sims = 500, analysis_at = c(0.5, 0.75))
#'
#' # Compare different boundary configurations
#' comparison <- compare_boundaries(result, list(
#'   "Fixed 0.975" = list(success = 0.975, futility = 0.90),
#'   "OBF-style" = list(success = boundary_obf(0.975), futility = 0.90),
#'   "Stringent" = list(success = 0.99, futility = 0.95),
#'   "Linear" = list(
#'     success = boundary_linear(0.999, 0.975),
#'     futility = boundary_linear(0.70, 0.90)
#'   )
#' ))
#'
#' print(comparison)
#' }
compare_boundaries <- function(power_result, boundaries) {

  # Validate input
  if (!is.list(boundaries) || length(boundaries) == 0) {
    cli::cli_abort(c(
      "{.arg boundaries} must be a non-empty named list",
      "i" = "Each element should have 'success' and/or 'futility' components"
    ))
  }

  if (is.null(names(boundaries)) || any(names(boundaries) == "")) {
    cli::cli_abort(c(
      "{.arg boundaries} must be a named list",
      "i" = "Provide names for each boundary configuration"
    ))
  }

  # Process each boundary configuration
  purrr::map_dfr(names(boundaries), function(name) {
    b <- boundaries[[name]]

    reanalyzed <- resummarize_boundaries(
      power_result,
      p_sig_scs = b$success,
      p_sig_ftl = b$futility
    )

    # Extract overall summary and add boundary name
    reanalyzed@results_conditions |>
      dplyr::mutate(boundary = name, .before = 1)
  })
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/design_prior.R"
#' Parse and Validate Design Prior
#'
#' Internal helper function to parse design prior specifications and create
#' weight and quantile functions for design prior integration.
#'
#' @param design_prior Design prior specification (string or function)
#' @param effect_sizes Vector of effect sizes for coverage checking
#' @param verbose Whether to print parsing information (default: TRUE)
#' @return List containing weight_fn, quantile_fn, and weight_type
#' @importFrom stats sd approx
#' @keywords internal
parse_design_prior <- function(design_prior, effect_sizes, verbose = TRUE) {
  weight_fn <- NULL
  weight_type <- "none"
  quantile_fn <- NULL

  if (is.null(design_prior)) {
    return(list(
      weight_fn = weight_fn,
      quantile_fn = quantile_fn,
      weight_type = weight_type
    ))
  }

  if (is.character(design_prior)) {
    # Use brms density functions with generic evaluation
    weight_type <- "brms"
    tryCatch(
      {
        if (!requireNamespace("brms", quietly = TRUE)) {
          cli::cli_abort(c(
            "Package {.pkg brms} is required for parsing brms design prior syntax",
            "i" = "Install it with {.code install.packages('brms')}"
          ))
        }

        # Validate basic syntax
        if (!grepl("^[a-zA-Z_][a-zA-Z0-9_]*\\(.*\\)$", design_prior)) {
          cli::cli_abort(c(
            "Invalid brms prior syntax",
            "x" = "You supplied {.val {design_prior}}",
            "i" = "Expected format: {.code distribution_name(param1, param2, ...)}"
          ))
        }

        # Extract distribution name
        dist_name <- gsub("\\(.*\\)", "", design_prior)

        # Create density and quantile function calls
        # Priority: stats package first, then brms, then error
        density_call <- NULL
        quantile_call <- NULL
        source_package <- NULL

        # =============================================================================
        # PRIORITY HIERARCHY for Prior Evaluation (stats  brms  custom)
        # =============================================================================
        # 1. stats package (norm, t, gamma) - Fastest, most stable, no dependencies
        # 2. brms package (student_t, lognormal) - Specialized distributions
        # 3. custom functions - Maximum flexibility, user-defined
        #
        # Rationale: Attempt lightweight solutions before heavy dependencies.
        # stats functions are base R and very fast. brms functions add overhead
        # but support more distributions. Custom functions allow full flexibility.
        brms_to_stats_map <- list(
          "normal" = "norm",
          "student_t" = "t",
          "gamma" = "gamma",
          "beta" = "beta",
          "exponential" = "exp",
          "uniform" = "unif",
          "poisson" = "pois",
          "binomial" = "binom",
          "cauchy" = "cauchy",
          "chi_square" = "chisq",
          "f" = "f",
          "lognormal" = "lnorm",
          "logistic" = "logis",
          "weibull" = "weibull",
          "wilcox" = "wilcox"
        )

        # Try Method 1: stats package first
        stats_name <- brms_to_stats_map[[dist_name]]
        if (!is.null(stats_name)) {
          # Test if stats function exists and works
          stats_call <- sub(dist_name, stats_name, design_prior)
          test_density_call <- paste0("stats::d", stats_call)
          test_quantile_call <- paste0("stats::q", stats_call)

          # Test the stats functions
          stats_works <- tryCatch(
            {
              test_call <- gsub("\\(", "(0.5,", test_density_call)
              result <- eval(parse(text = test_call))
              is.numeric(result) &&
                length(result) == 1 && !is.na(result)
            },
            error = function(e) {
              FALSE
            }
          )

          if (stats_works) {
            density_call <- test_density_call
            quantile_call <- test_quantile_call
            source_package <- "stats"
          } else {
            stats_works <- FALSE
          }
        } else {
          stats_works <- FALSE
        }

        # If stats doesn't work, try brms
        if (!stats_works) {
          density_call <- paste0("brms::d", design_prior)
          quantile_call <- paste0("brms::q", design_prior)
          source_package <- "brms"

          # Test brms functions
          brms_works <- tryCatch(
            {
              test_call <- gsub("\\(", "(0.5,", density_call)
              result <- eval(parse(text = test_call))
              is.numeric(result) &&
                length(result) == 1 && !is.na(result)
            },
            error = function(e) {
              FALSE
            }
          )

          if (!brms_works) {
            cli::cli_abort(c(
              "Distribution {.val {dist_name}} not available",
              "x" = "Distribution not found in either {.pkg stats} or {.pkg brms} packages",
              "i" = "Use a supported distribution or provide a custom function"
            ))
          }
        }

        # Create weight function
        weight_fn <- function(x) {
          # Replace the first parameter (or add x as first parameter)
          modified_call <- gsub("\\(", paste0("(", x, ","), density_call)
          tryCatch(
            {
              eval(parse(text = modified_call))
            },
            error = function(e) {
              cli::cli_abort(c(
                "Error evaluating {.pkg {source_package}} density function",
                "x" = "{e$message}",
                "i" = "Check your distribution parameters"
              ))
            }
          )
        }

        # Try to create quantile function
        quantile_fn <- NULL
        test_quantile_call <- gsub("\\(", "(0.5,", quantile_call)
        quantile_available <- tryCatch(
          {
            test_result <- eval(parse(text = test_quantile_call))
            if (is.numeric(test_result) &&
              length(test_result) == 1 && !is.na(test_result)) {
              # Create actual quantile function
              quantile_fn <<- function(p) {
                modified_call <- gsub("\\(", paste0("(", p, ","), quantile_call)
                tryCatch(
                  {
                    eval(parse(text = modified_call))
                  },
                  error = function(e) {
                    cli::cli_abort(c(
                      "Error evaluating {.pkg {source_package}} quantile function",
                      "x" = "{e$message}",
                      "i" = "Check your distribution parameters"
                    ))
                  }
                )
              }
              TRUE
            } else {
              FALSE
            }
          },
          error = function(e) {
            FALSE
          }
        )

        if (!quantile_available) {
          cli::cli_warn(c(
            "Quantile function not available",
            "x" = "Function '{quantile_call}' could not be created",
            "i" = "Coverage checking will be disabled"
          ))
        }

        if (verbose) {
          cat("Successfully parsed design prior:", design_prior, "\n")
          cat("  Distribution:", dist_name, "\n")
          cat(
            "  Density function:",
            density_call,
            "(using",
            source_package,
            "package)\n"
          )
          if (quantile_available) {
            cat(
              "  Quantile function:",
              quantile_call,
              "(using",
              source_package,
              "package)\n"
            )
          } else {
            cat("  Quantile function: Not available (coverage checking disabled)\n")
          }
        }
      },
      error = function(e) {
        cli::cli_abort(c(
          "Error parsing design prior",
          "x" = "{e$message}",
          "i" = "Check your prior specification syntax"
        ))
      }
    )
  } else if (is.function(design_prior)) {
    # Validate R function
    weight_type <- "function"
    tryCatch(
      {
        test_val <- design_prior(0.5)
        if (!is.numeric(test_val) || length(test_val) != 1) {
          cli::cli_abort(c(
            "Design prior function must return a single numeric value",
            "x" = "Your function returned {.type {test_val}} with length {.val {length(test_val)}}",
            "i" = "Ensure your function returns a single numeric value"
          ))
        }
        weight_fn <- design_prior
        # For custom functions, we'll estimate quantiles numerically
        quantile_fn <- function(p) {
          # Create a grid of values and find approximate quantiles
          test_range <- seq(
            min(effect_sizes) - 2 * sd(effect_sizes),
            max(effect_sizes) + 2 * sd(effect_sizes),
            length.out = 1000
          )
          weights <- sapply(test_range, weight_fn)
          weights <- weights / sum(weights)
          cumulative <- cumsum(weights)
          approx(cumulative, test_range, xout = p)$y
        }
      },
      error = function(e) {
        cli::cli_abort(c(
          "Error testing design prior function",
          "x" = "{e$message}",
          "i" = "Ensure your function accepts a numeric input and returns a numeric output"
        ))
      }
    )
  } else {
    cli::cli_abort(c(
      "{.arg design_prior} must be a character string or R function",
      "x" = "You supplied {.type {design_prior}}",
      "i" = "Use brms syntax (e.g., {.val normal(0.5, 0.2)}) or provide a function"
    ))
  }

  # Compute quantiles and check coverage
  if (!is.null(quantile_fn)) {
    tryCatch(
      {
        q10 <- quantile_fn(0.1)
        q90 <- quantile_fn(0.9)

        # Check if effect sizes adequately cover the prior distribution
        min_effect <- min(effect_sizes)
        max_effect <- max(effect_sizes)

        if (max_effect < q90) {
          cli::cli_warn(c(
            "Effect size range may not cover design prior adequately",
            "x" = "Maximum effect size ({round(max_effect, 3)}) is less than 90th percentile of design prior ({round(q90, 3)})",
            "i" = "Consider including larger effect sizes in your analysis"
          ))
        }

        if (min_effect > q10) {
          cli::cli_warn(c(
            "Effect size range may not cover design prior adequately",
            "x" = "Minimum effect size ({round(min_effect, 3)}) is greater than 10th percentile of design prior ({round(q10, 3)})",
            "i" = "Consider including smaller effect sizes in your analysis"
          ))
        }
      },
      error = function(e) {
        # If quantile computation fails for custom functions, just warn
        if (weight_type == "function") {
          cli::cli_warn(c(
            "Could not compute quantiles for custom design prior function",
            "i" = "Unable to check effect size coverage"
          ))
        } else {
          cli::cli_abort(c(
            "Error computing quantiles",
            "x" = "{e$message}",
            "i" = "Check your distribution parameters"
          ))
        }
      }
    )
  }

  return(list(
    weight_fn = weight_fn,
    quantile_fn = quantile_fn,
    weight_type = weight_type
  ))
}


#' Validate Weighting Function Implementation
#'
#' Tests that the weighting function parsing and computation in rctbp_power_analysis objects
#' works correctly. This function validates both brms syntax parsing and R function handling,
#' as well as the weighted power computation logic.
#'
#' @param effect_sizes Vector of effect sizes to test with (default: seq(0.2, 0.8, 0.1))
#' @param verbose Whether to print detailed test results (default: TRUE)
#'
#' @return A list containing validation results:
#' \itemize{
#'   \item all_tests_passed: Boolean indicating if all tests passed
#'   \item test_results: List of individual test results
#'   \item errors: Any errors encountered during testing
#' }
#'
#' @details
#' This validation function tests:
#' \itemize{
#'   \item Normal distribution parsing from brms syntax
#'   \item Student-t distribution parsing from brms syntax
#'   \item Custom R function validation
#'   \item Weight normalization (ensures weights sum to 1)
#'   \item Quantile computation for coverage checking
#'   \item Error handling for invalid inputs
#' }
#'
#' @importFrom stats dnorm qnorm
#' @keywords internal
#'

validate_weighting_function <- function(effect_sizes = seq(0.2, 0.8, 0.1),
                                        verbose = TRUE) {
  if (verbose) {
    cat("=== Weighting Function Validation ===\n")
    cat("Testing weighting function implementation in rctbp_power_analysis objects\n\n")
  }

  test_results <- list()
  errors <- character()

  # Test 1: Normal distribution parsing
  if (verbose) {
    cat("Test 1: Normal distribution parsing...\n")
  }
  test_results$normal_parsing <- tryCatch(
    {
      weighting_function <- "normal(0.5, 0.15)"

      # Test using the new generic approach
      density_call <- paste0("brms::d", weighting_function)
      quantile_call <- paste0("brms::q", weighting_function)

      # Create weight function
      weight_fn <- function(x) {
        modified_call <- gsub("\\(", paste0("(", x, ","), density_call)
        eval(parse(text = modified_call))
      }

      # Create quantile function
      quantile_fn <- function(p) {
        modified_call <- gsub("\\(", paste0("(", p, ","), quantile_call)
        eval(parse(text = modified_call))
      }

      # Test the functions work
      test_weights <- sapply(effect_sizes, weight_fn)
      test_quantiles <- quantile_fn(c(0.1, 0.5, 0.9))

      # Validate results
      if (!is.numeric(test_weights) || any(is.na(test_weights))) {
        cli::cli_abort(c(
          "Weight function produced non-numeric or NA values",
          "i" = "Check your distribution parameters"
        ))
      }

      if (!is.numeric(test_quantiles) ||
        any(is.na(test_quantiles))) {
        cli::cli_abort(c(
          "Quantile function produced non-numeric or NA values",
          "i" = "Check your distribution parameters"
        ))
      }

      list(
        passed = TRUE,
        weights = test_weights,
        quantiles = test_quantiles
      )
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 1 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Test 2: Student-t distribution parsing
  if (verbose) {
    cat("Test 2: Student-t distribution parsing...\n")
  }
  test_results$studentt_parsing <- tryCatch(
    {
      weighting_function <- "student_t(6, 0.5, 0.2)"

      # Test using the new generic approach
      density_call <- paste0("brms::d", weighting_function)
      quantile_call <- paste0("brms::q", weighting_function)

      # Create weight function
      weight_fn <- function(x) {
        modified_call <- gsub("\\(", paste0("(", x, ","), density_call)
        eval(parse(text = modified_call))
      }

      # Create quantile function
      quantile_fn <- function(p) {
        modified_call <- gsub("\\(", paste0("(", p, ","), quantile_call)
        eval(parse(text = modified_call))
      }

      # Test the functions work
      test_weights <- sapply(effect_sizes, weight_fn)
      test_quantiles <- quantile_fn(c(0.1, 0.5, 0.9))

      # Validate results
      if (!is.numeric(test_weights) || any(is.na(test_weights))) {
        cli::cli_abort(c(
          "Weight function produced non-numeric or NA values",
          "i" = "Check your distribution parameters"
        ))
      }

      if (!is.numeric(test_quantiles) ||
        any(is.na(test_quantiles))) {
        cli::cli_abort(c(
          "Quantile function produced non-numeric or NA values",
          "i" = "Check your distribution parameters"
        ))
      }

      list(
        passed = TRUE,
        weights = test_weights,
        quantiles = test_quantiles
      )
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 2 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Test 3: Custom R function validation
  if (verbose) {
    cat("Test 3: Custom R function validation...\n")
  }
  test_results$custom_function <- tryCatch(
    {
      # Create a custom weighting function
      custom_fn <- function(x) {
        dnorm(x, mean = 0.4, sd = 0.1)
      }

      # Test validation logic
      test_val <- custom_fn(0.5)
      if (!is.numeric(test_val) || length(test_val) != 1) {
        cli::cli_abort(c(
          "Weighting function must return a single numeric value",
          "i" = "Check your custom function"
        ))
      }

      # Test the function works with effect sizes
      test_weights <- sapply(effect_sizes, custom_fn)

      if (!is.numeric(test_weights) || any(is.na(test_weights))) {
        cli::cli_abort(c(
          "Custom function produced non-numeric or NA values",
          "i" = "Check your custom function"
        ))
      }

      list(passed = TRUE, weights = test_weights)
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 3 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Test 4: Weight normalization
  if (verbose) {
    cat("Test 4: Weight normalization...\n")
  }
  test_results$weight_normalization <- tryCatch(
    {
      # Use normal distribution weights
      weight_fn <- function(x) {
        dnorm(x, mean = 0.5, sd = 0.15)
      }
      weights <- sapply(effect_sizes, weight_fn)
      normalized_weights <- weights / sum(weights)

      # Check that weights sum to 1 (within tolerance)
      weight_sum <- sum(normalized_weights)
      if (abs(weight_sum - 1.0) > 1e-10) {
        cli::cli_abort(c(
          "Normalized weights do not sum to 1",
          "x" = "Sum = {.val {weight_sum}}",
          "i" = "This is an internal error in weight normalization"
        ))
      }

      # Check all weights are positive
      if (any(normalized_weights <= 0)) {
        cli::cli_abort(c(
          "Some normalized weights are non-positive",
          "i" = "Check your weighting function"
        ))
      }

      list(
        passed = TRUE,
        original_weights = weights,
        normalized_weights = normalized_weights,
        sum = weight_sum
      )
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 4 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Test 5: Coverage checking logic
  if (verbose) {
    cat("Test 5: Coverage checking logic...\n")
  }
  test_results$coverage_checking <- tryCatch(
    {
      # Test with normal distribution
      quantile_fn <- function(p) {
        qnorm(p, mean = 0.5, sd = 0.15)
      }

      q10 <- quantile_fn(0.1)
      q90 <- quantile_fn(0.9)

      min_effect <- min(effect_sizes)
      max_effect <- max(effect_sizes)

      # Test coverage warnings would be triggered correctly
      coverage_low <- min_effect <= q10
      coverage_high <- max_effect >= q90

      # With default effect_sizes (0.2 to 0.8) and normal(0.5, 0.15),
      # we expect good coverage
      expected_coverage <- coverage_low && coverage_high

      list(
        passed = TRUE,
        q10 = q10,
        q90 = q90,
        min_effect = min_effect,
        max_effect = max_effect,
        coverage_low = coverage_low,
        coverage_high = coverage_high,
        good_coverage = expected_coverage
      )
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 5 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Test 6: Error handling for invalid inputs
  if (verbose) {
    cat("Test 6: Error handling for invalid inputs...\n")
  }
  test_results$error_handling <- tryCatch(
    {
      errors_caught <- 0
      total_error_tests <- 0

      # Test invalid brms syntax
      total_error_tests <- total_error_tests + 1
      tryCatch(
        {
          # This should fail due to wrong number of parameters
          params <- gsub("normal\\(|\\)", "", "normal(0.5)") # Missing second parameter
          params <- as.numeric(strsplit(params, ",")[[1]])
          if (length(params) != 2) {
            cli::cli_abort("normal() requires 2 parameters")
          }
        },
        error = function(e) {
          errors_caught <<- errors_caught + 1
        }
      )

      # Test invalid R function
      total_error_tests <- total_error_tests + 1
      tryCatch(
        {
          invalid_fn <- function(x) {
            c(1, 2)
          } # Returns vector instead of single value
          test_val <- invalid_fn(0.5)
          if (!is.numeric(test_val) || length(test_val) != 1) {
            cli::cli_abort("Weighting function must return a single numeric value")
          }
        },
        error = function(e) {
          errors_caught <<- errors_caught + 1
        }
      )

      # Test unsupported distribution
      total_error_tests <- total_error_tests + 1
      tryCatch(
        {
          # Test with a distribution that doesn't exist
          density_call <- "brms::dnonexistent(1, 2)"
          test_result <- eval(parse(text = density_call))
        },
        error = function(e) {
          errors_caught <<- errors_caught + 1
        }
      )

      list(
        passed = TRUE,
        errors_caught = errors_caught,
        total_tests = total_error_tests
      )
    },
    error = function(e) {
      errors <<- c(errors, paste("Test 6 failed:", e$message))
      list(passed = FALSE, error = e$message)
    }
  )

  # Summary
  all_passed <- all(sapply(test_results, function(x) {
    x$passed
  }))

  if (verbose) {
    cat("\n=== Validation Summary ===\n")
    cat(
      "Test 1 - Normal parsing:",
      ifelse(test_results$normal_parsing$passed, "PASS", "FAIL"),
      "\n"
    )
    cat(
      "Test 2 - Student-t parsing:",
      ifelse(test_results$studentt_parsing$passed, "PASS", "FAIL"),
      "\n"
    )
    cat(
      "Test 3 - Custom function:",
      ifelse(test_results$custom_function$passed, "PASS", "FAIL"),
      "\n"
    )
    cat(
      "Test 4 - Weight normalization:",
      ifelse(test_results$weight_normalization$passed, "PASS", "FAIL"),
      "\n"
    )
    cat(
      "Test 5 - Coverage checking:",
      ifelse(test_results$coverage_checking$passed, "PASS", "FAIL"),
      "\n"
    )
    cat(
      "Test 6 - Error handling:",
      ifelse(test_results$error_handling$passed, "PASS", "FAIL"),
      "\n"
    )

    if (test_results$error_handling$passed) {
      cat(
        "  Errors properly caught:",
        test_results$error_handling$errors_caught,
        "/",
        test_results$error_handling$total_tests,
        "\n"
      )
    }

    cat(
      "\nOverall result:",
      ifelse(all_passed, "ALL TESTS PASSED", "SOME TESTS FAILED"),
      "\n"
    )

    if (length(errors) > 0) {
      cat("\nErrors encountered:\n")
      for (error in errors) {
        cat("  -", error, "\n")
      }
    }

    if (all_passed) {
      cat("\nOK: Weighting function implementation is working correctly!\n")
    } else {
      cat("\nERROR: Weighting function implementation has issues that need attention.\n")
    }
  }

  return(list(
    all_tests_passed = all_passed,
    test_results = test_results,
    errors = errors
  ))
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/estimation_sequential.R"
# =============================================================================
# SEQUENTIAL ANALYSIS ESTIMATION (DEPRECATED)
# =============================================================================
# DEPRECATION NOTICE: This file is deprecated as of 2025-11.
#
# All code has been moved to backend-specific files:
#   - R/backend_brms.R: estimate_sequential_brms()
#   - R/backend_bf.R: estimate_sequential_bf()
#   - R/utils_results.R: create_error_result()
#
# This file is now empty and will be removed in a future version.
# =============================================================================
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/estimation_single.R"
# =============================================================================
# SINGLE ANALYSIS ESTIMATION (DEPRECATED)
# =============================================================================
# DEPRECATION NOTICE: This file is deprecated as of 2025-11.
#
# All code has been moved to backend-specific files:
#   - R/backend_brms.R: estimate_single_brms()
#   - R/backend_bf.R: estimate_single_bf()
#   - R/utils_results.R: create_error_result()
#
# This file is now empty and will be removed in a future version.
# =============================================================================
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/interim_functions.R"
# =============================================================================
# INTERIM DECISION FUNCTIONS
# =============================================================================
# Factory functions for creating interim decision functions used in sequential
# trial designs. These functions return decision functions with the standard
# interface expected by the estimation code.
#
# Standard Interface:
#   function(interim_summaries, current_n, analysis_at, n_total)
#   Returns: list(decision = "continue"|"stop_success"|"stop_futility",
#                 modified_params = NULL or list of modified parameters)

#' Default Interim Function: Always Continue
#'
#' Creates an interim decision function that always returns "continue".
#' This is the default used when `analysis_at` is specified but no
#' `interim_function` is provided. Useful for sequential monitoring
#' without early stopping rules.
#'
#' @return A function with the standard interim function interface that
#'   always returns `list(decision = "continue", modified_params = NULL)`
#'
#' @details
#' This function is used internally as the default when users specify
#' `analysis_at` without an `interim_function`. It enables sequential
#' analysis (multiple looks at the data) without any stopping rules.
#'
#' The returned function accepts the standard interim function parameters:
#' \itemize{
#'   \item `interim_summaries`: Data frame from `compute_measures()`
#'   \item `current_n`: Current sample size at this analysis
#'   \item `analysis_at`: The scheduled analysis timepoint
#'   \item `n_total`: Maximum planned sample size
#' }
#'
#' @export
#' @seealso [interim_futility_only()], [interim_success_futility()]
#'
#' @examples
#' # Create a design with sequential monitoring but no early stopping
#' \dontrun{
#' design <- build_design(
#'   model = my_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = 0.975,
#'   p_sig_ftl = 0.5,
#'   analysis_at = c(50, 100, 150),
#'   interim_function = interim_continue()  # Or omit - this is the default
#' )
#' }
interim_continue <- function() {
  function(interim_summaries, current_n, analysis_at, n_total) {
    list(
      decision = "continue",
      modified_params = NULL
    )
  }
}


#' Interim Function Factory: Futility Stopping Only
#'
#' Creates an interim decision function that stops for futility only.
#' The trial stops early if the posterior probability of futility exceeds
#' the specified threshold.
#'
#' @param futility_threshold Numeric threshold (0-1) for stopping for futility.
#'   If the maximum `pr_ftl` across target parameters exceeds this threshold,
#'   the trial stops for futility. Default is 0.90.
#'
#' @return A function with the standard interim function interface
#'
#' @details
#' At each interim analysis, this function checks if any target parameter
#' has a posterior probability of futility (`pr_ftl`) exceeding the threshold.
#' If so, it returns "stop_futility"; otherwise "continue".
#'
#' This is useful for trials where you want to stop early if the treatment
#' is clearly ineffective, but continue to the planned sample size for
#' efficacy assessment.
#'
#' @export
#' @seealso [interim_continue()], [interim_success_futility()]
#'
#' @examples
#' \dontrun{
#' # Stop if P(futility) > 0.95
#' design <- build_design(
#'   model = my_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = 0.975,
#'   p_sig_ftl = 0.5,
#'   analysis_at = c(50, 100),
#'   interim_function = interim_futility_only(futility_threshold = 0.95)
#' )
#' }
interim_futility_only <- function(futility_threshold = 0.90) {
  # Validate threshold
  if (!is.numeric(futility_threshold) || length(futility_threshold) != 1 ||
      futility_threshold < 0 || futility_threshold > 1) {
    cli::cli_abort(c(
      "{.arg futility_threshold} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {futility_threshold}}"
    ))
  }

  function(interim_summaries, current_n, analysis_at, n_total) {
    # Check if any parameter exceeds futility threshold
    max_pr_ftl <- max(interim_summaries$pr_ftl, na.rm = TRUE)

    if (!is.na(max_pr_ftl) && max_pr_ftl >= futility_threshold) {
      return(list(
        decision = "stop_futility",
        modified_params = NULL
      ))
    }

    list(
      decision = "continue",
      modified_params = NULL
    )
  }
}


#' Interim Function Factory: Success and Futility Stopping
#'
#' Creates an interim decision function that can stop for either success
#' or futility. The trial stops early if the posterior probability of
#' success or futility exceeds the respective threshold.
#'
#' @param success_threshold Numeric threshold (0-1) for stopping for success.
#'   If the maximum `pr_scs` across target parameters exceeds this threshold,
#'   the trial stops for success. Default is 0.99.
#' @param futility_threshold Numeric threshold (0-1) for stopping for futility.
#'   If the maximum `pr_ftl` across target parameters exceeds this threshold,
#'   the trial stops for futility. Default is 0.90.
#'
#' @return A function with the standard interim function interface
#'
#' @details
#' At each interim analysis, this function first checks for success
#' (early efficacy), then for futility. Success takes precedence if both
#' thresholds are exceeded (unlikely but possible with different parameters).
#'
#' This is the most common interim analysis setup for confirmatory trials.
#'
#' @export
#' @seealso [interim_continue()], [interim_futility_only()]
#'
#' @examples
#' \dontrun{
#' # Stop for overwhelming success or clear futility
#' design <- build_design(
#'   model = my_model,
#'   target_params = "b_armtreat_1",
#'   p_sig_scs = 0.975,
#'   p_sig_ftl = 0.5,
#'   analysis_at = c(50, 100),
#'   interim_function = interim_success_futility(
#'     success_threshold = 0.995,
#'     futility_threshold = 0.90
#'   )
#' )
#' }
interim_success_futility <- function(success_threshold = 0.99,
                                     futility_threshold = 0.90) {
  # Validate thresholds
  if (!is.numeric(success_threshold) || length(success_threshold) != 1 ||
      success_threshold < 0 || success_threshold > 1) {
    cli::cli_abort(c(
      "{.arg success_threshold} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {success_threshold}}"
    ))
  }
  if (!is.numeric(futility_threshold) || length(futility_threshold) != 1 ||
      futility_threshold < 0 || futility_threshold > 1) {
    cli::cli_abort(c(
      "{.arg futility_threshold} must be a single numeric value between 0 and 1",
      "x" = "You supplied {.val {futility_threshold}}"
    ))
  }

  function(interim_summaries, current_n, analysis_at, n_total) {
    max_pr_scs <- max(interim_summaries$pr_scs, na.rm = TRUE)
    max_pr_ftl <- max(interim_summaries$pr_ftl, na.rm = TRUE)

    # Check for conflicting decisions (both success AND futility triggered)
    # This indicates misconfigured thresholds
    success_met <- !is.na(max_pr_scs) && max_pr_scs >= success_threshold
    futility_met <- !is.na(max_pr_ftl) && max_pr_ftl >= futility_threshold

    if (success_met && futility_met) {
      cli::cli_abort(c(
        "Conflicting stopping decisions: both success and futility criteria met",
        "x" = "At n = {current_n}: P(success) = {round(max_pr_scs, 3)} >= {success_threshold}, P(futility) = {round(max_pr_ftl, 3)} >= {futility_threshold}",
        "i" = "This indicates misconfigured decision thresholds. Consider:",
        "*" = "Widening the gap between 'thresholds_success' and 'thresholds_futility' in conditions",
        "*" = "Increasing 'p_sig_scs' and/or 'p_sig_ftl' to require stronger evidence",
        "*" = "Reviewing priors to ensure posteriors aren't unreasonably wide"
      ))
    }

    # Check success
    if (success_met) {
      return(list(
        decision = "stop_success",
        modified_params = NULL
      ))
    }

    # Check futility
    if (futility_met) {
      return(list(
        decision = "stop_futility",
        modified_params = NULL
      ))
    }

    list(
      decision = "continue",
      modified_params = NULL
    )
  }
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/model_cache.R"
# =============================================================================
# MODEL CACHE SYSTEM
# =============================================================================
# Unified model caching for both brms and BayesFlow backends.
# Models are downloaded from GitHub releases on first use and cached locally.
#
# Directory structure:
#   ~/.cache/rctbayespower/           # Linux
#   ~/Library/Caches/rctbayespower/   # macOS
#   %LOCALAPPDATA%/rctbayespower/     # Windows
#        brms/
#           ancova_cont_2arms.rds
#           ancova_cont_3arms.rds
#        bf/
#            ancova_cont_2arms.keras
#            ancova_cont_3arms.keras
#
# GitHub Releases:
#   - brms-models-v0.0.0.9000: Pre-compiled brmsfit objects
#   - bf-models-v0.0.0.9000: Pre-trained BayesFlow approximators

# =============================================================================
# CACHE DIRECTORY MANAGEMENT
# =============================================================================

#' Get Model Cache Directory
#'
#' Returns cross-platform cache directory for downloaded models.
#' Creates directory if it doesn't exist.
#'
#' @param subdir Subdirectory within cache ("brms" or "bf")
#'
#' @return Path to cache directory
#' @keywords internal
get_model_cache_dir <- function(subdir = NULL) {
  if (!requireNamespace("rappdirs", quietly = TRUE)) {
    cli::cli_abort(c(
      "Package 'rappdirs' is required for model caching",
      "i" = "Install it with: install.packages('rappdirs')"
    ))
  }

  cache_dir <- rappdirs::user_cache_dir("rctbayespower")
  if (!is.null(subdir)) {
    cache_dir <- file.path(cache_dir, subdir)
  }
  if (!dir.exists(cache_dir)) {
    dir.create(cache_dir, recursive = TRUE)
  }
  cache_dir
}


# =============================================================================
# MODEL DOWNLOAD
# =============================================================================

#' Download Model from GitHub Releases
#'
#' Downloads a model file from the package's GitHub releases.
#'
#' @param model_name Name of the model (e.g., "ancova_cont_2arms")
#' @param dest_path Full path where the model should be saved
#' @param model_type Type of model: "brms" or "bf"
#'
#' @keywords internal
download_model <- function(model_name, dest_path, model_type = c("brms", "bf")) {
  model_type <- match.arg(model_type)

  # URL pattern: GitHub releases asset
  base_url <- "https://github.com/matthiaskloft/rctbayespower/releases/download"
  version_tag <- paste0(model_type, "-models-v0.0.0.9000")

  ext <- if (model_type == "brms") ".rds" else ".keras"
  url <- paste0(base_url, "/", version_tag, "/", model_name, ext)

  tryCatch({
    utils::download.file(url, dest_path, mode = "wb", quiet = TRUE)
    backend_name <- if (model_type == "bf") "BayesFlow" else "brms"
    cli::cli_alert_success("{backend_name} model downloaded to cache")
  }, error = function(e) {
    cli::cli_abort(c(
      "Failed to download {model_type} model",
      "x" = "URL: {url}",
      "i" = "Error: {e$message}",
      "i" = "Check internet connection or try again later"
    ))
  })
}


# =============================================================================
# BRMS MODEL LOADING
# =============================================================================

#' Load Pre-compiled brms Model
#'
#' Downloads compiled brms model on first use and caches locally.
#' The brms model is a template that can be updated with new data.
#'
#' @param model_name Name of predefined model (e.g., "ancova_cont_2arms")
#' @param force_download Re-download even if cached (default FALSE)
#' @param quiet Suppress download messages (default FALSE)
#'
#' @return Compiled brmsfit object ready for [stats::update()]
#' @export
#'
#' @examples
#' \dontrun{
#' # Load predefined brms model
#' model <- load_brms_model("ancova_cont_2arms")
#'
#' # Force re-download
#' model <- load_brms_model("ancova_cont_2arms", force_download = TRUE)
#' }
load_brms_model <- function(model_name, force_download = FALSE, quiet = FALSE) {

  available_models <- c("ancova_cont_2arms", "ancova_cont_3arms")
  if (!model_name %in% available_models) {
    cli::cli_abort(c(
      "Unknown brms model: {.val {model_name}}",
      "i" = "Available models: {.val {available_models}}"
    ))
  }

  cache_dir <- get_model_cache_dir("brms")
  model_file <- file.path(cache_dir, paste0(model_name, ".rds"))

  if (!file.exists(model_file) || force_download) {
    if (!quiet) {
      cli::cli_alert_info("Downloading brms model: {.val {model_name}}")
    }
    download_model(model_name, model_file, "brms")
  } else {
    if (!quiet) {
      cli::cli_alert_info("Loading brms model from cache: {.val {model_name}}")
    }
  }

  readRDS(model_file)
}


# =============================================================================
# BAYESFLOW MODEL LOADING
# =============================================================================

#' Load Pre-trained BayesFlow Model
#'
#' Downloads BayesFlow/Keras model on first use and caches locally.
#' Model is loaded via Python/reticulate for full BayesFlow compatibility.
#'
#' @param model_name Name of predefined model (e.g., "ancova_cont_2arms")
#' @param force_download Re-download even if cached (default FALSE)
#' @param quiet Suppress download messages (default FALSE)
#'
#' @return BayesFlow approximator object (Python Keras model via reticulate)
#' @export
#'
#' @examples
#' \dontrun{
#' # Load predefined BayesFlow model
#' bf_model <- load_bf_model("ancova_cont_2arms")
#'
#' # Force re-download
#' bf_model <- load_bf_model("ancova_cont_2arms", force_download = TRUE)
#' }
load_bf_model <- function(model_name, force_download = FALSE, quiet = FALSE) {

  available_models <- c("ancova_cont_2arms", "ancova_cont_3arms")
  if (!model_name %in% available_models) {
    cli::cli_abort(c(
      "Unknown BayesFlow model: {.val {model_name}}",
      "i" = "Available models: {.val {available_models}}"
    ))
  }

  cache_dir <- get_model_cache_dir("bf")
  model_file <- file.path(cache_dir, paste0(model_name, ".keras"))

  if (!file.exists(model_file) || force_download) {
    if (!quiet) {
      cli::cli_alert_info("Downloading BayesFlow model: {.val {model_name}}")
    }
    download_model(model_name, model_file, "bf")
  } else {
    if (!quiet) {
      cli::cli_alert_info("Loading BayesFlow model from cache: {.val {model_name}}")
    }
  }

  # Load via Python for full BayesFlow compatibility
  load_bf_model_python(model_file)
}


# =============================================================================
# CACHE MANAGEMENT
# =============================================================================

#' List Available Models
#'
#' Shows all available predefined models and their cache status.
#'
#' @param backend Filter by backend: "all", "brms", or "bf"
#'
#' @return Data frame with model info (model name, backend, cached status)
#' @export
#'
#' @examples
#' \dontrun{
#' # List all models
#' list_models()
#'
#' # List only BayesFlow models
#' list_models("bf")
#' }
list_models <- function(backend = c("all", "brms", "bf")) {
  backend <- match.arg(backend)
  models <- c("ancova_cont_2arms", "ancova_cont_3arms")

  result <- data.frame(model = character(), backend = character(),
                       cached = logical(), stringsAsFactors = FALSE)

  if (backend %in% c("all", "brms")) {
    brms_cache <- get_model_cache_dir("brms")
    brms_cached <- sapply(models, function(m) {
      file.exists(file.path(brms_cache, paste0(m, ".rds")))
    })
    result <- rbind(result, data.frame(
      model = models, backend = "brms", cached = brms_cached,
      stringsAsFactors = FALSE
    ))
  }

  if (backend %in% c("all", "bf")) {
    bf_cache <- get_model_cache_dir("bf")
    bf_cached <- sapply(models, function(m) {
      file.exists(file.path(bf_cache, paste0(m, ".keras")))
    })
    result <- rbind(result, data.frame(
      model = models, backend = "bf", cached = bf_cached,
      stringsAsFactors = FALSE
    ))
  }

  rownames(result) <- NULL
  result
}


#' Clear Model Cache
#'
#' Removes cached model files to free disk space or force re-download.
#'
#' @param backend Which cache to clear: "all", "brms", or "bf"
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Clear all cached models
#' clear_model_cache()
#'
#' # Clear only BayesFlow models
#' clear_model_cache("bf")
#' }
clear_model_cache <- function(backend = c("all", "brms", "bf")) {
  backend <- match.arg(backend)

  cleared <- 0
  if (backend %in% c("all", "brms")) {
    brms_dir <- get_model_cache_dir("brms")
    brms_files <- list.files(brms_dir, pattern = "\\.rds$", full.names = TRUE)
    if (length(brms_files) > 0) {
      unlink(brms_files)
      cleared <- cleared + length(brms_files)
    }
  }

  if (backend %in% c("all", "bf")) {
    bf_dir <- get_model_cache_dir("bf")
    bf_files <- list.files(bf_dir, pattern = "\\.keras$", full.names = TRUE)
    if (length(bf_files) > 0) {
      unlink(bf_files)
      cleared <- cleared + length(bf_files)
    }
  }

  if (cleared > 0) {
    cli::cli_alert_success("Cleared {cleared} cached model(s)")
  } else {
    cli::cli_alert_info("Cache already empty")
  }

  invisible(cleared)
}


#' Get Cache Size
#'
#' Returns the total size of cached models.
#'
#' @param backend Which cache to check: "all", "brms", or "bf"
#'
#' @return Named numeric vector with sizes in bytes
#' @export
#'
#' @examples
#' \dontrun{
#' cache_size <- get_cache_size()
#' print(paste("Total cache:", sum(cache_size) / 1e6, "MB"))
#' }
get_cache_size <- function(backend = c("all", "brms", "bf")) {
  backend <- match.arg(backend)

  sizes <- c(brms = 0, bf = 0)

  if (backend %in% c("all", "brms")) {
    brms_dir <- get_model_cache_dir("brms")
    brms_files <- list.files(brms_dir, pattern = "\\.rds$", full.names = TRUE)
    if (length(brms_files) > 0) {
      sizes["brms"] <- sum(file.info(brms_files)$size, na.rm = TRUE)
    }
  }

  if (backend %in% c("all", "bf")) {
    bf_dir <- get_model_cache_dir("bf")
    bf_files <- list.files(bf_dir, pattern = "\\.keras$", full.names = TRUE)
    if (length(bf_files) > 0) {
      sizes["bf"] <- sum(file.info(bf_files)$size, na.rm = TRUE)
    }
  }

  sizes
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/models_ancova.R"
#' Create General ANCOVA Model with Flexible Specifications
#'
#' Creates a build_model object for ANCOVA (Analysis of Covariance) with flexible
#' specifications for number of arms, contrasts, allocation ratios, and parameters.
#' This is the general function that underlies all ANCOVA model variants in the package.
#'
#' @param prior_intercept Prior for the intercept parameter. If NULL (default),
#'   uses normal(0, 10). Must be a brmsprior object created with [brms::set_prior()].
#' @param prior_sigma Prior for the residual standard deviation. If NULL (default),
#'   uses normal(0, 10). Must be a brmsprior object created with [brms::set_prior()].
#' @param prior_covariate Prior for the covariate effect. If NULL (default),
#'   uses student_t(3, 0, 1). Must be a brmsprior object created with [brms::set_prior()].
#' @param prior_treatment Prior for the treatment effect. If NULL (default),
#'   uses student_t(3, 0, 1). Must be a brmsprior object created with [brms::set_prior()].
#' @param link_sigma Link function for the residual standard deviation. Default is "identity".
#' @param n_arms Number of arms in the trial (must be >= 2). Required parameter.
#' @param contrasts Contrast method for treatment arms. Either a character string
#'   (e.g., "contr.treatment", "contr.sum") or a contrast matrix. Required parameter.
#' @param p_alloc Numeric vector of allocation probabilities summing to 1.
#'   Length must equal n_arms. Required parameter.
#' @param intercept Intercept value for data generation. Required parameter.
#' @param b_arm_treat Treatment effect coefficients for data generation.
#'   Vector length must equal n_arms - 1. Required parameter.
#' @param b_covariate Covariate effect coefficient for data generation. Required parameter.
#' @param sigma Residual standard deviation for data generation (must be > 0). Required parameter.
#'
#' @details
#' This function creates a complete ANCOVA model with the following structure:
#'
#' \strong{Model Formula:} outcome ~ 1 + covariate + arm
#'
#' \strong{Data Structure:} The generated data includes:
#' \itemize{
#'   \item covariate: Standardized normal covariate
#'   \item arm: Factor with levels "ctrl" and treatment arms ("treat_1", "treat_2", etc.)
#'   \item outcome: Continuous outcome generated from the linear model
#' }
#'
#' \strong{Parameters:} The model includes parameters for intercept, covariate effect,
#' treatment effects, and residual standard deviation (sigma).
#'
#' \strong{Model Compilation:} The function compiles the brms model during creation,
#' which may take some time but enables efficient power analysis later.
#'
#' \strong{Convenience Functions:} For common use cases, consider the wrapper functions that can be called via the 'predefined_model' argument in [build_model()]:
#' \itemize{
#'   \item [build_model_ancova_cont_2arms()] - 2-arm continuous ANCOVA
#'   \item [build_model_ancova_cont_3arms()] - 3-arm continuous ANCOVA
#' }
#'
#' @return An S7 object of class "rctbp_model_ancova" ready for use with
#'   [build_design()] and power analysis functions.
#'
#' @export
#' @importFrom brms set_prior brm
#' @importFrom stats gaussian contrasts<- model.matrix
#' @seealso [build_model()], [build_design()], [build_model_ancova_cont_2arms()],
#'   [build_model_ancova_cont_3arms()]
#'
#' @examples
#' \dontrun{
#' # Create 2-arm ANCOVA model
#' model_2arm <- build_model_ancova(
#'   n_arms = 2,
#'   contrasts = "contr.treatment",
#'   p_alloc = c(0.5, 0.5),
#'   intercept = 0,
#'   b_arm_treat = 0.5,
#'   b_covariate = 0.3,
#'   sigma = 1
#' )
#' }
build_model_ancova <- function(prior_intercept = NULL,
                               prior_sigma = NULL,
                               prior_covariate = NULL,
                               prior_treatment = NULL,
                               link_sigma = "identity",
                               n_arms = NULL,
                               contrasts = NULL,
                               p_alloc = NULL,
                               intercept = NULL,
                               b_arm_treat = NULL,
                               b_covariate,
                               sigma = NULL) {
  # Enhanced validation using model properties ---------------------------------

  # Validate that parameter names from simulation function include required parameters
  required_sim_params <- c("n_total",
                           "n_arms",
                           "p_alloc",
                           "intercept",
                           "b_arm_treat",
                           "b_covariate",
                           "sigma")

  # Validation of ANCOVA-specific parameters ----------------------------------

  # Validate p_alloc
  if (!is.null(p_alloc) && !is.null(n_arms)) {
    if (length(p_alloc) != n_arms) {
      cli::cli_abort(c(
        "{.arg p_alloc} must have length equal to {.arg n_arms}",
        "x" = "You supplied {.arg p_alloc} with length {.val {length(p_alloc)}} but {.arg n_arms} = {.val {n_arms}}",
        "i" = "Provide a probability vector with {.val {n_arms}} elements"
      ))
    }
    if (abs(sum(p_alloc) - 1) > 1e-6) {
      cli::cli_abort(c(
        "{.arg p_alloc} must sum to 1",
        "x" = "You supplied {.arg p_alloc} that sums to {.val {sum(p_alloc)}}",
        "i" = "Ensure all probabilities sum to 1.0"
      ))
    }
  }

  # Validate b_arm_treat
  if (!is.null(b_arm_treat) && !is.null(n_arms)) {
    if (length(b_arm_treat) != (n_arms - 1)) {
      cli::cli_abort(c(
        "{.arg b_arm_treat} must have length equal to {.code n_arms - 1}",
        "x" = "You supplied {.arg b_arm_treat} with length {.val {length(b_arm_treat)}} but need {.val {n_arms - 1}} coefficients",
        "i" = "Provide {.val {n_arms - 1}} treatment effect coefficients for {.val {n_arms}} arms"
      ))
    }
  }

  # Validate sigma
  if (!is.null(sigma) && sigma <= 0) {
    cli::cli_abort(c(
      "{.arg sigma} must be positive",
      "x" = "You supplied {.arg sigma} = {.val {sigma}}",
      "i" = "Use a value > 0"
    ))
  }

  # create the data simulation function
  simulate_data_ancova <- local({
    default_n_arms <- n_arms
    default_contrasts <- contrasts
    default_p_alloc <- p_alloc
    default_intercept <- intercept
    default_b_arm_treat <- b_arm_treat
    default_b_covariate <- b_covariate
    default_sigma <- sigma

    function(n_total,
             n_arms = default_n_arms,
             contrasts = default_contrasts,
             p_alloc = default_p_alloc,
             intercept = default_intercept,
             b_arm_treat = default_b_arm_treat,
             b_covariate = default_b_covariate,
             sigma = default_sigma) {
      # validation of inputs -----------------------------------------------------

      # validate n_total, must be numeric and whole number
      if (is.null(n_total) ||
          !is.numeric(n_total) || length(n_total) != 1 ||
          n_total <= 0 || n_total != round(n_total)) {
        cli::cli_abort(c(
          "{.arg n_total} must be a positive integer value",
          "x" = "You supplied {.val {n_total}}",
          "i" = "Use a positive whole number"
        ))
      }
      # validate n_arms, must be integer > 2
      if (is.null(n_arms) ||
          !is.numeric(n_arms) || length(n_arms) != 1 ||
          n_arms < 2 || n_arms != round(n_arms)) {
        cli::cli_abort(c(
          "{.arg n_arms} must be a positive integer >= 2",
          "x" = "You supplied {.val {n_arms}}",
          "i" = "Use an integer value of 2 or more"
        ))
      }
      # validate contrasts, must be a string or matrix for creating contrasts
      if (!is.null(contrasts) &
          !is.character(contrasts) & !is.matrix(contrasts)) {
        cli::cli_abort(c(
          "{.arg contrasts} must be a character string or matrix",
          "x" = "You supplied {.type {contrasts}}",
          "i" = "Use a contrast method name (e.g., {.val contr.treatment}) or a contrast matrix"
        ))
      } else{
        # create contrast matrix
        if (!is.null(contrasts) & is.character(contrasts)) {
          # validate string is one of the valid 'contr.' methods from 'stats'
          valid_contrasts <- c(
            "contr.treatment",
            "contr.sum",
            "contr.poly",
            "contr.helmert",
            "contr.SAS"
          )
          if (!(contrasts %in% valid_contrasts)) {
            cli::cli_abort(c(
              "{.arg contrasts} must be a valid contrast method",
              "x" = "You supplied {.val {contrasts}}",
              "i" = "Use one of: {.val {valid_contrasts}}"
            ))
          }
          # create contrast matrix
          tryCatch({
            contrasts_fn <- get(contrasts)
            contrast_matrix <- contrasts_fn(n_arms)
          }, error = function(e) {
            cli::cli_abort(c(
              "Failed to create contrast matrix from {.arg contrasts}",
              "x" = "Error: {e$message}",
              "i" = "Use a valid contrast method (e.g., {.val contr.treatment})"
            ))
          })
        }
      }
      # validate matrix
      if (!is.null(contrasts) & is.matrix(contrasts)) {
        # validate dimensions of contrast matrix,
        # nrow == n_arms, ncol == n_arms - 1
        if (nrow(contrasts) != n_arms ||
            ncol(contrasts) != n_arms - 1) {
          cli::cli_abort(c(
            "{.arg contrasts} matrix must have dimensions {.code n_arms x (n_arms - 1)}",
            "x" = "You supplied a {.val {nrow(contrasts)}} x {.val {ncol(contrasts)}} matrix but need {.val {n_arms}} x {.val {n_arms - 1}}",
            "i" = "Ensure the contrast matrix has {.val {n_arms}} rows and {.val {n_arms - 1}} columns"
          ))
        }
        tryCatch({
          contrasts_fn <- get(contrasts)
          contrast_matrix <- contrasts
        }, error = function(e) {
          cli::cli_abort(c(
            "Invalid {.arg contrasts} specification",
            "x" = "Error: {e$message}",
            "i" = "Provide a valid contrast method name or matrix"
          ))
        })
      }
      # validate p_alloc, must be of length n_arms
      # must be a numeric vector of probabilities summing to 1
      if (is.null(p_alloc) ||
          !is.numeric(p_alloc) || length(p_alloc) != n_arms ||
          sum(p_alloc) != 1) {
        cli::cli_abort(c(
          "{.arg p_alloc} must be a numeric vector of probabilities summing to 1",
          "x" = "You supplied {.val {p_alloc}} with length {.val {length(p_alloc)}} and sum {.val {sum(p_alloc)}}",
          "i" = "Provide {.val {n_arms}} probabilities that sum to 1.0"
        ))
      }
      # validate intercept, must be numeric
      if (is.null(intercept) || !is.numeric(intercept)) {
        cli::cli_abort(c(
          "{.arg intercept} must be a numeric value",
          "x" = "You supplied {.type {intercept}}",
          "i" = "Provide a numeric intercept value"
        ))
      }
      # validate b_arm_treat, must be numeric
      if (is.null(b_arm_treat) || !is.numeric(b_arm_treat)) {
        cli::cli_abort(c(
          "{.arg b_arm_treat} must be a numeric value",
          "x" = "You supplied {.type {b_arm_treat}}",
          "i" = "Provide numeric treatment effect coefficient(s)"
        ))
      }
      # validate b_covariate, must be numeric
      if (is.null(b_covariate) || !is.numeric(b_covariate)) {
        cli::cli_abort(c(
          "{.arg b_covariate} must be a numeric value",
          "x" = "You supplied {.type {b_covariate}}",
          "i" = "Provide a numeric covariate coefficient"
        ))
      }
      # validate sigma, must be numeric and positive
      if (is.null(sigma) || !is.numeric(sigma) || sigma <= 0) {
        cli::cli_abort(c(
          "{.arg sigma} must be a positive numeric value",
          "x" = "You supplied {.val {sigma}}",
          "i" = "Use a value > 0"
        ))
      }

      # end of validation --------------------------------------------------------


      # simulate data for ANCOVA -------------------------------------------------

      # predictors
      df <- data.frame(
        covariate = stats::rnorm(n_total),
        arm = factor(
          sample(
            x = seq_len(n_arms) - 1,
            size = n_total,
            prob = p_alloc,
            replace = TRUE
          ),
          levels = seq_len(n_arms) - 1,
          labels = c("ctrl", paste0("treat_", seq_len(n_arms - 1)))
        )
      )
      # set contrasts
      contrasts(df$arm) <- contrast_matrix

      # simulate outcomes
      df <- df |>
        dplyr::mutate(
          # outcome
          outcome = stats::rnorm(
            n_total,
            mean = intercept +
              model.matrix( ~ arm, data = df)[, -1, drop = FALSE] %*% b_arm_treat +
              covariate * b_covariate,
            sd = sigma
          )
        )
      return(df)
    }
  })

  # simulate some data to compile the model
  mock_data_ancova <- simulate_data_ancova(
    n_total = 20,
    n_arms = n_arms,
    contrasts = contrasts,
    p_alloc = rep(1, n_arms) / n_arms,
    intercept = 0,
    b_arm_treat = rep(0, n_arms - 1),
    b_covariate = 0,
    sigma = 1
  )

  # priors ---------------------------------------------------------------------
  # use user-specified priors if !is.null(prior_intercept) else use default priors
  # check that the priors are specified with brms::set_prior()

  if (is.null(prior_intercept)) {
    prior_intercept <- brms::set_prior("normal(0, 10)", class = "Intercept")
  } else if (!inherits(prior_intercept, "brmsprior")) {
    cli::cli_abort(c(
      "{.arg prior_intercept} must be a valid brmsprior object",
      "x" = "You supplied {.cls {class(prior_intercept)}}",
      "i" = "Create priors using {.fn brms::set_prior}"
    ))
  }
  if (is.null(prior_sigma)) {
    prior_sigma <- brms::set_prior("normal(0, 10)", class = "sigma", lb = 0)
  } else if (!inherits(prior_sigma, "brmsprior")) {
    cli::cli_abort(c(
      "{.arg prior_sigma} must be a valid brmsprior object",
      "x" = "You supplied {.cls {class(prior_sigma)}}",
      "i" = "Create priors using {.fn brms::set_prior}"
    ))
  }
  if (is.null(prior_covariate)) {
    prior_covariate <- brms::set_prior("student_t(3, 0, 1)", class = "b", coef = "covariate")
  } else if (!inherits(prior_covariate, "brmsprior")) {
    cli::cli_abort(c(
      "{.arg prior_covariate} must be a valid brmsprior object",
      "x" = "You supplied {.cls {class(prior_covariate)}}",
      "i" = "Create priors using {.fn brms::set_prior}"
    ))
  }
  if (is.null(prior_treatment)) {
    for (i in seq_len(n_arms - 1)) {
      prior_treatment <- brms::set_prior("student_t(3, 0, 1)",
                                         class = "b",
                                         coef = paste0("armtreat_", i))
    }
    prior_treatment <- brms::set_prior("student_t(3, 0, 1)", class = "b")
  } else if (!inherits(prior_treatment, "brmsprior")) {
    cli::cli_abort(c(
      "{.arg prior_treatment} must be a valid brmsprior object",
      "x" = "You supplied {.cls {class(prior_treatment)}}",
      "i" = "Create priors using {.fn brms::set_prior}"
    ))
  }

  # combine the priors into a single vector
  priors <- c(prior_covariate,
              prior_treatment,
              prior_intercept,
              prior_sigma)

  # end of priors --------------------------------------------------------------


  # compile the brms model -----------------------------------------------------

  # fit the brms model
  if (should_show(1)) {
    cli::cli_alert_info("Compiling the brms model")
  }

  # model for retrieving parameter names
  brms_model_ancova <-
    suppressMessages(suppressWarnings(
      brms::brm(
        formula = outcome ~ 1 + covariate + arm,
        data = mock_data_ancova,
        family = brms::brmsfamily("gaussian",link_sigma = link_sigma),
        prior = priors,
        chains = 1,
        iter = 500,
        refresh = 0,
        silent = 2
      )
    ))
  if (should_show(1)) {
    cli::cli_alert_success("Model compilation done")
  }



  # build model object ---------------------------------------------------------

  # Create S7 ANCOVA model object
  ancova_model <- rctbp_model(
    data_simulation_fn = simulate_data_ancova,
    brms_model = brms_model_ancova,
    model_name = "ANCOVA",
    n_endpoints = 1L,
    endpoint_types = "continuous",
    n_arms = as.integer(n_arms),
    n_repeated_measures = 0L
  )


  return(ancova_model)
}


# Specific default models

#' Create 2-Arm ANCOVA Model for Continuous Outcomes
#'
#' Creates a 2-arm ANCOVA model with sensible defaults for continuous outcomes.
#' This is a convenience wrapper around [build_model_ancova()].
#'
#' @param ... Additional arguments passed to [build_model_ancova()]. Can override
#'   any of the default parameters.
#'
#' @details
#' Default parameters:
#' \itemize{
#'   \item n_arms = 2
#'   \item contrasts = "contr.treatment"
#'   \item p_alloc = c(0.5, 0.5) (equal allocation)
#'   \item intercept = 0
#'   \item b_arm_treat = NULL (must be specified)
#'   \item b_covariate = NULL (must be specified)
#'   \item sigma = 1
#' }
#'
#' @return An S7 object of class "rctbp_model_ancova" ready for use with
#'   [build_design()] and power analysis functions.
#'
#' @export
#' @seealso [build_model_ancova()], [build_model_ancova_cont_3arms()]
#'
#' @examples
#' \dontrun{
#' # Create 2-arm ANCOVA model (must specify effect sizes)
#' model_2arm <- build_model_ancova_cont_2arms(
#'   b_arm_treat = 0.5,
#'   b_covariate = 0.3
#' )
#' }
build_model_ancova_cont_2arms <- function(...) {
  # collect additional arguments
  dots <- list(...)
  # set default arguments
  default_args <- list(
    prior_intercept = NULL,
    prior_sigma = NULL,
    prior_covariate = NULL,
    prior_treatment = NULL,
    n_arms = 2,
    contrasts = "contr.treatment",
    p_alloc = c(0.5, 0.5),
    intercept = 0,
    b_arm_treat = NULL,
    b_covariate = NULL,
    sigma = 1
  )
  # modify default arguments with user-specified arguments
  final_args <- modifyList(default_args, dots)
  # call the build_model_ancova function with the final arguments
  model <- do.call(build_model_ancova, final_args)

  # add predefined model name
  model@predefined_model <- "ancova_cont_2arms"
  # return the model object
  invisible(model)
}

#' Create 3-Arm ANCOVA Model for Continuous Outcomes
#'
#' Creates a 3-arm ANCOVA model with sensible defaults for continuous outcomes.
#' This is a convenience wrapper around [build_model_ancova()].
#'
#' @param ... Additional arguments passed to [build_model_ancova()]. Can override
#'   any of the default parameters.
#'
#' @details
#' Default parameters:
#' \itemize{
#'   \item n_arms = 3
#'   \item contrasts = "contr.treatment"
#'   \item p_alloc = c(1/3, 1/3, 1/3) (equal allocation)
#'   \item intercept = 0
#'   \item b_arm_treat = NULL (must be specified, length 2)
#'   \item b_covariate = NULL (must be specified)
#'   \item sigma = 1
#' }
#'
#' @return An S7 object of class "rctbp_model_ancova" ready for use with
#'   [build_design()] and power analysis functions.
#'
#' @export
#' @seealso [build_model_ancova()], [build_model_ancova_cont_2arms()]
#'
#' @examples
#' \dontrun{
#' # Create 3-arm ANCOVA model (must specify effect sizes)
#' model_3arm <- build_model_ancova_cont_3arms(
#'   b_arm_treat = c(0.5, 0.7),
#'   b_covariate = 0.3
#' )
#' }
build_model_ancova_cont_3arms <- function(...) {
  # collect additional arguments
  dots <- list(...)
  # set default arguments
  default_args <- list(
    prior_intercept = NULL,
    prior_sigma = NULL,
    prior_covariate = NULL,
    prior_treatment = NULL,
    n_arms = 3,
    contrasts = "contr.treatment",
    p_alloc = rep(1, 3) / 3,
    intercept = 0,
    b_arm_treat = NULL,
    b_covariate = NULL,
    sigma = 1
  )
  # modify default arguments with user-specified arguments
  final_args <- modifyList(default_args, dots)
  # call the build_model_ancova function with the final arguments
  model <- do.call(build_model_ancova, final_args)
  # add predefined model name

  model@predefined_model <- "ancova_cont_3arms"

  # return the model object
  invisible(model)
}


# =============================================================================
# BATCH SIMULATION FUNCTIONS (for BayesFlow Backend)
# =============================================================================

#' Simulate ANCOVA Data - Batched Format (2-arm)
#'
#' Generates multiple simulations at once for NPE/BayesFlow efficiency.
#' Returns matrices instead of data.frames for direct use with neural networks.
#'
#' @param n_sims Number of simulations to generate (batch size)
#' @param n_total Sample size per simulation
#' @param p_alloc Allocation probability for treatment (default 0.5)
#' @param intercept Intercept value (default 0)
#' @param b_arm_treat Treatment effect coefficient
#' @param b_covariate Covariate effect coefficient (default 0)
#' @param sigma Residual standard deviation (default 1)
#'
#' @return List with batch-formatted arrays:
#'   \itemize{
#'     \item outcome: matrix (n_sims x n_total)
#'     \item covariate: matrix (n_sims x n_total)
#'     \item group: matrix (n_sims x n_total), binary treatment indicator
#'     \item N: integer, sample size per simulation
#'     \item p_alloc: numeric, allocation probability
#'   }
#'
#' @details
#' This function is optimized for batch generation using vectorized operations.
#' It generates all simulations simultaneously, making it suitable for
#' BayesFlow/NPE workflows where batching improves efficiency.
#'
#' The group matrix contains binary (0/1) values for 2-arm trials,
#' where 0 = control and 1 = treatment.
#'
#' @keywords internal
#'
#' @examples
#' \dontrun{
#' # Generate 64 simulations with 100 subjects each
#' batch_data <- simulate_data_ancova_cont_2arms_batch(
#'   n_sims = 64,
#'   n_total = 100,
#'   b_arm_treat = 0.5,
#'   b_covariate = 0.3
#' )
#' dim(batch_data$outcome)  # [64, 100]
#' }
simulate_data_ancova_cont_2arms_batch <- function(n_sims, n_total, p_alloc = 0.5,
                                                   intercept = 0, b_arm_treat = 0,
                                                   b_covariate = 0, sigma = 1) {
  # Validate inputs
  n_sims <- as.integer(n_sims)
  n_total <- as.integer(n_total)

  if (n_sims <= 0) {
    cli::cli_abort(c(
      "{.arg n_sims} must be a positive integer",
      "x" = "You supplied {.val {n_sims}}"
    ))
  }
  if (n_total <= 0) {
    cli::cli_abort(c(
      "{.arg n_total} must be a positive integer",
      "x" = "You supplied {.val {n_total}}"
    ))
  }
  if (sigma <= 0) {
    cli::cli_abort(c(
      "{.arg sigma} must be positive",
      "x" = "You supplied {.val {sigma}}"
    ))
  }

  total_elements <- n_sims * n_total

  # Generate all random values at once (vectorized)
  covariate_mat <- matrix(
    stats::rnorm(total_elements),
    nrow = n_sims,
    ncol = n_total
  )

  group_mat <- matrix(
    stats::rbinom(total_elements, 1, p_alloc),
    nrow = n_sims,
    ncol = n_total
  )

  # Calculate outcomes using vectorized operations
  # outcome = intercept + covariate * b_covariate + group * b_arm_treat + error
  outcome_mat <- intercept +
    covariate_mat * b_covariate +
    group_mat * b_arm_treat +
    matrix(stats::rnorm(total_elements, 0, sigma), nrow = n_sims, ncol = n_total)

  list(
    outcome = outcome_mat,
    covariate = covariate_mat,
    group = group_mat,
    N = n_total,
    p_alloc = p_alloc
  )
}


#' Simulate ANCOVA Data - Batched Format (3-arm)
#'
#' Generates multiple 3-arm simulations at once for NPE/BayesFlow efficiency.
#' Returns matrices instead of data.frames for direct use with neural networks.
#'
#' @param n_sims Number of simulations to generate (batch size)
#' @param n_total Sample size per simulation
#' @param p_alloc Allocation probabilities (default c(1/3, 1/3, 1/3))
#' @param intercept Intercept value (default 0)
#' @param b_arm_treat Treatment effect coefficients (length 2 vector)
#' @param b_covariate Covariate effect coefficient (default 0)
#' @param sigma Residual standard deviation (default 1)
#'
#' @return List with batch-formatted arrays:
#'   \itemize{
#'     \item outcome: matrix (n_sims x n_total)
#'     \item covariate: matrix (n_sims x n_total)
#'     \item group: matrix (n_sims x n_total), values 0/1/2 for arms
#'     \item N: integer, sample size per simulation
#'     \item p_alloc: numeric vector, allocation probabilities
#'   }
#'
#' @details
#' Similar to [simulate_data_ancova_cont_2arms_batch()] but for 3-arm trials.
#' The group matrix contains values 0, 1, 2 corresponding to control,
#' treatment 1, and treatment 2 respectively.
#'
#' @keywords internal
#'
#' @examples
#' \dontrun{
#' # Generate 64 simulations with 150 subjects each
#' batch_data <- simulate_data_ancova_cont_3arms_batch(
#'   n_sims = 64,
#'   n_total = 150,
#'   b_arm_treat = c(0.3, 0.5),
#'   b_covariate = 0.3
#' )
#' dim(batch_data$outcome)  # [64, 150]
#' }
simulate_data_ancova_cont_3arms_batch <- function(n_sims, n_total,
                                                   p_alloc = c(1/3, 1/3, 1/3),
                                                   intercept = 0,
                                                   b_arm_treat = c(0, 0),
                                                   b_covariate = 0, sigma = 1) {
  # Validate inputs
  n_sims <- as.integer(n_sims)
  n_total <- as.integer(n_total)

  if (n_sims <= 0) {
    cli::cli_abort(c(
      "{.arg n_sims} must be a positive integer",
      "x" = "You supplied {.val {n_sims}}"
    ))
  }
  if (n_total <= 0) {
    cli::cli_abort(c(
      "{.arg n_total} must be a positive integer",
      "x" = "You supplied {.val {n_total}}"
    ))
  }
  if (length(p_alloc) != 3 || abs(sum(p_alloc) - 1) > 1e-6) {
    cli::cli_abort(c(
      "{.arg p_alloc} must be length 3 and sum to 1",
      "x" = "You supplied {.val {p_alloc}}"
    ))
  }
  if (length(b_arm_treat) != 2) {
    cli::cli_abort(c(
      "{.arg b_arm_treat} must be length 2 for 3-arm trials",
      "x" = "You supplied {.val {b_arm_treat}} with length {.val {length(b_arm_treat)}}"
    ))
  }
  if (sigma <= 0) {
    cli::cli_abort(c(
      "{.arg sigma} must be positive",
      "x" = "You supplied {.val {sigma}}"
    ))
  }

  total_elements <- n_sims * n_total

  # Generate covariates
  covariate_mat <- matrix(
    stats::rnorm(total_elements),
    nrow = n_sims,
    ncol = n_total
  )

  # Generate group assignments (0, 1, 2) using multinomial sampling
  group_vec <- sample(
    x = c(0L, 1L, 2L),
    size = total_elements,
    prob = p_alloc,
    replace = TRUE
  )
  group_mat <- matrix(group_vec, nrow = n_sims, ncol = n_total)

  # Calculate treatment effects
  # For treatment coding: group 0 = control (ref), group 1 = treat_1, group 2 = treat_2
  treat_effect <- matrix(0, nrow = n_sims, ncol = n_total)
  treat_effect[group_mat == 1] <- b_arm_treat[1]
  treat_effect[group_mat == 2] <- b_arm_treat[2]

  # Calculate outcomes
  outcome_mat <- intercept +
    covariate_mat * b_covariate +
    treat_effect +
    matrix(stats::rnorm(total_elements, 0, sigma), nrow = n_sims, ncol = n_total)

  list(
    outcome = outcome_mat,
    covariate = covariate_mat,
    group = group_mat,
    N = n_total,
    p_alloc = p_alloc
  )
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/output_system.R"
#' Output Mode System for rctbayespower
#'
#' Provides dual-mode output rendering (CLI or Markdown) for all package output.
#'
#' @name output_system
#' @keywords internal
NULL

#' Get Current Output Mode
#'
#' Returns the current output mode setting for the package.
#'
#' @return Character string: "cli" or "markdown"
#' @export
#'
#' @examples
#' get_output_mode()
#'
get_output_mode <- function() {
  mode <- getOption("rctbayespower.output_mode", default = "cli")

  if (!mode %in% c("cli", "markdown")) {
    cli::cli_warn(c(
      "Invalid output mode: {.val {mode}}",
      "i" = "Using default {.val cli}"
    ))
    mode <- "cli"
  }

  return(mode)
}

#' Set Output Mode
#'
#' Sets the output mode for all package output.
#'
#' @param mode Character string: "cli" for styled console output (default)
#'   or "markdown" for markdown-formatted output that can be exported.
#'
#' @return Invisibly returns the previous mode setting.
#' @export
#'
#' @examples
#' # Set to markdown mode
#' set_output_mode("markdown")
#'
#' # Set back to CLI mode
#' set_output_mode("cli")
#'
set_output_mode <- function(mode = c("cli", "markdown")) {
  mode <- match.arg(mode)
  old_mode <- getOption("rctbayespower.output_mode", default = "cli")
  options(rctbayespower.output_mode = mode)
  invisible(old_mode)
}

#' Temporarily Change Output Mode
#'
#' Executes code with a temporary output mode setting.
#'
#' @param mode Character string: "cli" or "markdown"
#' @param code Code to execute with the temporary mode
#'
#' @return Result of evaluating `code`
#' @export
#'
#' @examples
#' \dontrun{
#' # Print in markdown mode temporarily
#' with_output_mode("markdown", {
#'   print(my_power_analysis)
#' })
#' }
#'
with_output_mode <- function(mode = c("cli", "markdown"), code) {
  mode <- match.arg(mode)
  old_mode <- set_output_mode(mode)
  on.exit(options(rctbayespower.output_mode = old_mode))
  force(code)
}

#' Export Report to File
#'
#' Exports a report for an rctbayespower object to a markdown file.
#'
#' @param object An S7 object (rctbp_model, rctbp_design, rctbp_conditions, or rctbp_power_analysis)
#' @param file Character string: path to output file
#' @param format Character string: output format (currently only "markdown" supported)
#'
#' @return Invisibly returns the file path
#' @export
#'
#' @examples
#' \dontrun{
#' model <- build_model("ancova_cont_2arms")
#' export_report(model, "model_report.md")
#' }
#'
export_report <- function(object, file, format = "markdown") {
  if (format != "markdown") {
    cli::cli_abort(c(
      "Unsupported format: {.val {format}}",
      "i" = "Currently only {.val markdown} is supported"
    ))
  }

  # Capture output in markdown mode
  output <- with_output_mode("markdown", {
    utils::capture.output(print(object))
  })

  # Write to file
  writeLines(output, file)

  cli::cli_alert_success("Report exported to {.file {file}}")
  invisible(file)
}

#' Convert CLI Semantic Element to Markdown
#'
#' Internal helper to route output based on current mode.
#'
#' @param cli_fn Function to call in CLI mode
#' @param md_text Text to output in markdown mode
#'
#' @return NULL (outputs to console)
#' @keywords internal
#'
cli_or_md <- function(cli_fn, md_text) {
  if (get_output_mode() == "cli") {
    cli_fn()
  } else {
    cat(md_text, "\n", sep = "")
  }
  invisible(NULL)
}

#' Markdown Helper Functions
#'
#' Convert semantic output elements to markdown format.
#'
#' @name markdown_helpers
#' @keywords internal
NULL

#' @rdname markdown_helpers
md_h1 <- function(text) {
  paste0("# ", text, "\n")
}

#' @rdname markdown_helpers
md_h2 <- function(text) {
  paste0("## ", text, "\n")
}

#' @rdname markdown_helpers
md_h3 <- function(text) {
  paste0("### ", text, "\n")
}

#' @rdname markdown_helpers
md_rule <- function() {
  "---\n"
}

#' @rdname markdown_helpers
md_alert <- function(text, type = c("info", "success", "warning", "danger")) {
  type <- match.arg(type)
  symbol <- switch(type,
    info = "\u2139",      # 
    success = "\u2714",   # 
    warning = "!",
    danger = "\u2716"     # 
  )
  paste0(symbol, " ", text, "\n")
}

#' @rdname markdown_helpers
md_bullet <- function(items, indent = 0) {
  prefix <- paste0(rep("  ", indent), collapse = "")
  paste0(prefix, "- ", items, "\n", collapse = "")
}

#' @rdname markdown_helpers
md_code <- function(text) {
  paste0("`", text, "`")
}

#' @rdname markdown_helpers
md_code_block <- function(text, lang = "") {
  paste0("```", lang, "\n", text, "\n```\n")
}

#' Build Output as Character Vector
#'
#' Helper to collect output sections before rendering.
#'
#' @return Character vector that can be collapsed and rendered
#' @keywords internal
#'
new_output_builder <- function() {
  structure(character(), class = "output_builder")
}

#' @keywords internal
add_line <- function(builder, ...) {
  c(builder, paste0(...))
}

#' @keywords internal
add_blank <- function(builder) {
  c(builder, "")
}

#' @keywords internal
render_output <- function(builder) {
  cat(paste(builder, collapse = "\n"), "\n")
  invisible(NULL)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/plot_comparison.R"
# =============================================================================
# COMPARISON PLOT
# =============================================================================
# Compare power vs posterior probability visualization.

#' Create Comparison Plot
#'
#' Internal function to create power vs posterior probability comparison plots.
#'
#' @param plot_data Data frame with power analysis results
#' @param design Design object with p_sig_scs and p_sig_ftl
#' @param analysis_type One of "sample_only" or "effect_only"
#' @param effect_col Column name for effect size (from condition_values)
#' @param metric Ignored (always shows both power and probability)
#' @param decision Filter: "success", "futility", or "both"
#' @param ... Additional arguments (ignored)
#'
#' @return A ggplot2 object
#' @keywords internal
#' @importFrom rlang .data
create_comparison_plot <- function(plot_data,
                                   design,
                                   analysis_type,
                                   effect_col,
                                   metric,
                                   decision,
                                   ...) {
  if (analysis_type == "both") {
    cli::cli_abort(c(
      "Comparison plot requires single varying dimension",
      "x" = "Your analysis varies multiple dimensions",
      "i" = "Use {.code type = 'heatmap'} for multi-dimensional analysis"
    ))
  }

  # Determine x variable and label
  if (analysis_type == "sample_only") {
    x_var <- "n_total"
    x_label <- "Total Sample Size"
    title_base <- "Power vs Posterior Probability (Sample Size Analysis)"
  } else if (analysis_type == "effect_only") {
    x_var <- effect_col
    x_label <- paste("Effect Size (", effect_col, ")", sep = "")
    title_base <- "Power vs Posterior Probability (Effect Size Analysis)"
  } else {
    cli::cli_abort(c(
      "Unknown analysis type for comparison plot",
      "x" = "Analysis type: {.val {analysis_type}}"
    ))
  }

  # Force metric to "both" for comparison plot (always shows both power and probability)
  plot_data_long <- pivot_plot_data_long(plot_data, decision, metric = "both")

  # Build ggplot
  p <- ggplot2::ggplot(
    plot_data_long,
    ggplot2::aes(
      x = .data[[x_var]],
      y = .data$value,
      color = .data$outcome,
      linetype = .data$measure,
      shape = .data$measure,
      group = interaction(.data$outcome, .data$measure)
    )
  ) +
    ggplot2::geom_line(linewidth = 1) +
    ggplot2::geom_point(size = 3) +
    ggplot2::scale_y_continuous(
      labels = scales::percent_format(),
      limits = c(0, 1)
    ) +
    ggplot2::scale_color_manual(values = rctbp_colors()) +
    ggplot2::scale_linetype_manual(
      values = c("Power" = "solid", "Probability" = "dashed")
    ) +
    ggplot2::scale_shape_manual(
      values = c("Power" = 16, "Probability" = 18)
    ) +
    ggplot2::labs(
      x = x_label,
      y = "Value",
      title = title_base,
      subtitle = "Solid: Power | Dashed: Posterior Probability",
      color = "Outcome",
      linetype = "Measure",
      shape = "Measure"
    ) +
    rctbp_theme()

  p
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/plot_heatmap.R"
# =============================================================================
# HEATMAP PLOT
# =============================================================================
# 2D heatmap visualization when both sample size and effect size vary.

#' Create Heatmap Plot
#'
#' Internal function to create heatmap visualizations using ggplot2.
#'
#' @param plot_data Data frame with power analysis results
#' @param design Design object with p_sig_scs and p_sig_ftl
#' @param analysis_type Must be "both" for heatmap
#' @param effect_col Column name for effect size (from condition_values)
#' @param metric Filter: "power", "prob", or "both"
#' @param decision Filter: "success", "futility", or "both"
#' @param show_target Whether to show target power contour lines
#' @param target_power Target power level for contour (default: design@p_sig_scs)
#' @param ... Additional arguments (ignored)
#'
#' @return A ggplot2 object
#' @keywords internal
#' @importFrom rlang .data
create_heatmap_plot <- function(plot_data,
                                design,
                                analysis_type,
                                effect_col,
                                metric,
                                decision,
                                show_target,
                                target_power,
                                ...) {
  if (analysis_type != "both") {
    cli::cli_abort(c(
      "Heatmap plot requires both sample sizes and effect sizes to vary",
      "x" = "Your analysis only varies one dimension",
      "i" = "Use {.code type = 'power_curve'} instead"
    ))
  }

  # Check that effect_col exists
  if (is.null(effect_col) || !effect_col %in% names(plot_data)) {
    cli::cli_abort(c(
      "Effect size column not found in data",
      "x" = "Expected column {.val {effect_col}} not found",
      "i" = "Check that effect size was included in condition_values"
    ))
  }

  # Check for sufficient data points
  if (nrow(plot_data) < 4) {
    cli::cli_abort(c(
      "Heatmap requires at least 4 data points (2x2 grid)",
      "x" = "You have {.val {nrow(plot_data)}} data points",
      "i" = "Use {.code type = 'power_curve'} instead"
    ))
  }

  # Pivot data to long format
  plot_data_long <- pivot_plot_data_long(plot_data, decision, metric)

  # Remove rows with NA values
  plot_data_long <- dplyr::filter(plot_data_long, !is.na(.data$value))

  if (nrow(plot_data_long) == 0) {
    cli::cli_abort(c(
      "No valid data points for heatmap",
      "x" = "All power analysis results contain NA values",
      "i" = "Check your analysis for errors or convergence issues"
    ))
  }

  # Set default target power from design if not specified
  if (is.null(target_power)) {
    target_power <- design@p_sig_scs
  }

  # Build ggplot heatmap
  p <- ggplot2::ggplot(
    plot_data_long,
    ggplot2::aes(
      x = .data[[effect_col]],
      y = .data$n_total,
      fill = .data$value
    )
  ) +
    ggplot2::geom_tile() +
    ggplot2::scale_fill_gradient(
      low = "lightblue",
      high = "darkblue",
      labels = scales::percent_format(),
      limits = c(0, 1)
    ) +
    ggplot2::labs(
      x = paste("Effect Size (", effect_col, ")", sep = ""),
      y = "Total Sample Size",
      fill = "Value",
      title = "Power Analysis Heatmap",
      subtitle = paste(
        "Targets - Success:",
        scales::percent(design@p_sig_scs, accuracy = 0.1),
        "| Futility:",
        scales::percent(design@p_sig_ftl, accuracy = 0.1)
      )
    ) +
    rctbp_theme() +
    ggplot2::theme(legend.position = "right")

  # Add contour line at target power level (for power facets only)
  if (show_target && (metric == "power" || metric == "both")) {
    power_data <- dplyr::filter(plot_data_long, .data$measure == "Power")
    if (nrow(power_data) > 0) {
      p <- p +
        ggplot2::geom_contour(
          data = power_data,
          ggplot2::aes(z = .data$value),
          breaks = target_power,
          color = "white",
          linewidth = 1,
          linetype = "dashed"
        )
    }
  }

  # Determine faceting based on what's being shown
  n_outcomes <- length(unique(plot_data_long$outcome))
  n_measures <- length(unique(plot_data_long$measure))

  if (n_outcomes > 1 && n_measures > 1) {
    p <- p + ggplot2::facet_grid(
      rows = ggplot2::vars(.data$outcome),
      cols = ggplot2::vars(.data$measure)
    )
  } else if (n_outcomes > 1) {
    p <- p + ggplot2::facet_wrap(~ outcome)
  } else if (n_measures > 1) {
    p <- p + ggplot2::facet_wrap(~ measure)
  }

  p
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/plot_helpers.R"
# =============================================================================
# PLOT HELPERS
# =============================================================================
# Shared utilities for power analysis plotting functions.

#' Pivot plot data to long format for ggplot2
#'
#' Transforms wide-format power analysis results to long format suitable for
#' ggplot2 visualization. Handles both value columns and optional standard errors.
#'
#' @param plot_data Data frame with power analysis results
#' @param decision Filter: "success", "futility", or "both"
#' @param metric Filter: "power", "prob", or "both"
#' @param include_se Whether to include standard error columns
#'
#' @return Data frame in long format with columns: measure, outcome, value, and optionally se
#' @keywords internal
pivot_plot_data_long <- function(plot_data, decision, metric, include_se = FALSE) {
  # Determine columns to pivot
  value_cols <- c("pwr_scs", "pwr_ftl", "pr_scs", "pr_ftl")

  # Pivot power and probability columns to long format
  plot_data_long <- plot_data |>
    tidyr::pivot_longer(
      cols = dplyr::all_of(value_cols),
      names_to = c("measure", "outcome"),
      names_pattern = "(pwr|pr)_(scs|ftl)",
      values_to = "value"
    ) |>
    dplyr::mutate(
      measure = dplyr::case_match(
        .data$measure,
        "pwr" ~ "Power",
        "pr" ~ "Probability"
      ),
      outcome = dplyr::case_match(
        .data$outcome,
        "scs" ~ "Success",
        "ftl" ~ "Futility"
      )
    )

  # Add standard errors if requested and available
  if (include_se) {
    se_cols <- c("se_pwr_scs", "se_pwr_ftl", "se_pr_scs", "se_pr_ftl")
    available_se_cols <- se_cols[se_cols %in% names(plot_data)]

    if (length(available_se_cols) == 0) {
      cli::cli_warn(c(
        "Cannot show MCSE ribbons - no standard error columns found in data",
        "i" = "Expected columns: {.val {se_cols}}"
      ))
    } else if (length(available_se_cols) < length(se_cols)) {
      cli::cli_warn(c(
        "Some MCSE columns missing - ribbons may be incomplete",
        "i" = "Found: {.val {available_se_cols}}",
        "i" = "Missing: {.val {setdiff(se_cols, available_se_cols)}}"
      ))
    }

    if (length(available_se_cols) > 0) {
      # Pivot SE columns similarly
      se_data <- plot_data |>
        tidyr::pivot_longer(
          cols = dplyr::all_of(available_se_cols),
          names_to = c("se_measure", "se_outcome"),
          names_pattern = "se_(pwr|pr)_(scs|ftl)",
          values_to = "se"
        ) |>
        dplyr::mutate(
          se_measure = dplyr::case_match(.data$se_measure, "pwr" ~ "Power", "pr" ~ "Probability"),
          se_outcome = dplyr::case_match(.data$se_outcome, "scs" ~ "Success", "ftl" ~ "Futility")
        ) |>
        dplyr::select(-dplyr::any_of(value_cols))

      # Join SE data - need to match on all common columns plus measure/outcome
      join_cols <- intersect(names(plot_data_long), names(se_data))
      join_cols <- setdiff(join_cols, c("se", "se_measure", "se_outcome"))

      plot_data_long <- plot_data_long |>
        dplyr::left_join(
          se_data |> dplyr::rename(measure = "se_measure", outcome = "se_outcome"),
          by = c(join_cols, "measure", "outcome")
        )
    }
  }

  # Filter based on decision parameter
  if (decision == "success") {
    plot_data_long <- dplyr::filter(plot_data_long, .data$outcome == "Success")
  } else if (decision == "futility") {
    plot_data_long <- dplyr::filter(plot_data_long, .data$outcome == "Futility")
  }

  # Filter based on metric parameter
  if (metric == "power") {
    plot_data_long <- dplyr::filter(plot_data_long, .data$measure == "Power")
  } else if (metric == "prob") {
    plot_data_long <- dplyr::filter(plot_data_long, .data$measure == "Probability")
  }

  plot_data_long
}

#' Standard color palette for power analysis plots
#'
#' Returns named vector of colors for success/futility outcomes.
#'
#' @return Named character vector of colors
#' @keywords internal
rctbp_colors <- function() {
  c(
    "Success" = "steelblue",
    "Futility" = "darkred"
  )
}

#' Standard ggplot2 theme for power analysis plots
#'
#' Returns a minimal theme with standard formatting for power analysis plots.
#'
#' @return A ggplot2 theme object
#' @keywords internal
rctbp_theme <- function() {
  ggplot2::theme_minimal() +
    ggplot2::theme(
      legend.position = "bottom",
      plot.title = ggplot2::element_text(size = 14, face = "bold"),
      plot.subtitle = ggplot2::element_text(size = 10)
    )
}

#' Convert ggplot2 object to interactive plotly
#'
#' Wrapper around ggplotly with standard legend positioning.
#'
#' @param p A ggplot2 object
#' @param legend_position Legend position: "bottom" (default) or "right"
#'
#' @return A plotly object
#' @keywords internal
to_interactive <- function(p, legend_position = "bottom") {
  pl <- plotly::ggplotly(p)

  if (legend_position == "bottom") {
    pl <- pl |>
      plotly::layout(
        legend = list(orientation = "h", x = 0.5, xanchor = "center", y = -0.15)
      )
  }

  pl
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/plot_power_analysis.R"
#' Plot Power Analysis Results
#'
#' Create comprehensive visualizations for power analysis results from rctbp_power_analysis objects.
#' Supports different plot types based on analysis type (sample_only, effect_only, or both varying).
#'
#' @param x An rctbp_power_analysis object that has been run with results
#' @param type Type of plot to create:
#'   \itemize{
#'     \item "auto" - Automatically detect best plot type based on analysis (default)
#'     \item "power_curve" - Power curve across single varying dimension
#'     \item "heatmap" - 2D heatmap when both sample sizes and effect sizes vary
#'     \item "comparison" - Compare power vs posterior probabilities
#'   }
#' @param metric Which statistic type to display:
#'   \itemize{
#'     \item "both" - Both power and posterior probabilities (default)
#'     \item "power" - Power (decision rate) only
#'     \item "prob" - Posterior probabilities only
#'   }
#' @param decision Which decision criterion to display:
#'   \itemize{
#'     \item "both" - Both success and futility (default)
#'     \item "success" - Success metrics only
#'     \item "futility" - Futility metrics only
#'   }
#' @param show_target Whether to show target power lines (default: TRUE)
#' @param show_mcse Whether to show Monte Carlo standard error ribbons for uncertainty
#'   visualization (default: FALSE). Only applies to power_curve plots.
#' @param facet_by For power_curve plots, controls faceting:
#'   \itemize{
#'     \item "decision" - Facet by decision type (Success/Futility) (default)
#'     \item "metric" - Facet by metric type (Power/Probability)
#'     \item "effect_size" - Facet by effect size, vary sample size on x-axis
#'     \item "sample_size" - Facet by sample size, vary effect size on x-axis
#'   }
#' @param group_by Variable to use for line coloring in power_curve plots:
#'   \itemize{
#'     \item "effect_size" - Color by effect size values (default)
#'     \item "decision" - Color by success/futility decision
#'     \item "metric" - Color by power/probability metric
#'     \item "sample_size" - Color by sample size values
#'   }
#' @param target_power Optional numeric value (0-1) for drawing contour lines on heatmaps
#'   at the specified target power level. If NULL (default), uses the design's p_sig_scs.
#' @param interactive Whether to return an interactive plotly object (TRUE, default) or
#'   a static ggplot2 object (FALSE).
#' @param ... Additional arguments passed to plotting functions
#'
#' @return A plotly object (if interactive = TRUE) or ggplot2 object (if interactive = FALSE)
#' @export
#' @importFrom stats as.formula
#' @importFrom rlang .data
#' @importFrom ggplot2 ggplot aes geom_line geom_point geom_tile geom_hline geom_ribbon %+%
#' @importFrom ggplot2 geom_contour
#' @importFrom ggplot2 scale_y_continuous scale_x_continuous scale_fill_gradient
#' @importFrom ggplot2 scale_color_manual scale_linetype_manual scale_shape_manual scale_fill_manual
#' @importFrom ggplot2 scale_color_brewer scale_fill_brewer scale_color_viridis_d scale_fill_viridis_d
#' @importFrom ggplot2 labs theme_minimal theme element_text facet_wrap facet_grid vars
#' @importFrom plotly ggplotly
#' @importFrom scales percent_format percent
#'
#' @name plot.rctbp_power_analysis
#' @export
S7::method(plot, rctbp_power_analysis) <- function(x,
                                                   type = "auto",
                                                   metric = "both",
                                                   decision = "both",
                                                   show_target = TRUE,
                                                   show_mcse = FALSE,
                                                   facet_by = "decision",
                                                   group_by = "effect_size",
                                                   target_power = NULL,
                                                   interactive = TRUE,
                                                   ...) {
  # Check if analysis has been run
  if (nrow(x@results_conditions) == 0) {
    cli::cli_abort(c(
      "No simulation results found",
      "x" = "The power analysis has not been run yet",
      "i" = "Run the analysis first using {.fn run}"
    ))
  }

  # Call the internal plotting dispatcher
  create_power_plot(x,
                    type,
                    metric,
                    decision,
                    show_target,
                    show_mcse,
                    facet_by,
                    group_by,
                    target_power,
                    interactive,
                    ...)
}

#' Internal plotting dispatcher
#'
#' Routes to appropriate plot function based on type and analysis dimensions.
#'
#' @inheritParams plot.rctbp_power_analysis
#' @keywords internal
create_power_plot <- function(x,
                              type = "auto",
                              metric = "both",
                              decision = "both",
                              show_target = TRUE,
                              show_mcse = FALSE,
                              facet_by = "decision",
                              group_by = "effect_size",
                              target_power = NULL,
                              interactive = TRUE,
                              ...) {
  # Check for valid data
  if (nrow(x@results_conditions) == 0) {
    cli::cli_abort(c(
      "No power analysis results to plot",
      "x" = "The summarized results are empty or missing",
      "i" = "Check that the power analysis object was run successfully"
    ))
  }

  # Use S7 object structure directly
  # For sequential: use results_interim (per-look data), for single-look: use results_conditions
  plot_data <- if (x@has_interim) x@results_interim else x@results_conditions
  design <- x@design
  conditions <- x@conditions

  # Check for missing essential columns
  required_cols <- c("pwr_scs", "pwr_ftl", "pr_scs", "pr_ftl")
  missing_cols <- setdiff(required_cols, names(plot_data))
  if (length(missing_cols) > 0) {
    cli::cli_abort(c(
      "Missing required columns in results",
      "x" = "Missing: {.val {missing_cols}}",
      "i" = "Check that the power analysis completed successfully"
    ))
  }

  # =============================================================================
  # DETECT EFFECT SIZE COLUMN FROM CONDITIONS
  # =============================================================================
  # The effect size column name comes from condition_values, not target_params.
  # target_params contains brms parameter names (e.g., "b_armtreat_1"),
  # but condition_values uses user-specified names (e.g., "b_arm_treat").
  condition_value_names <- names(conditions@condition_values)
  effect_cols <- setdiff(condition_value_names, "n_total")

  # Use first effect column if available, otherwise NULL
  effect_col <- if (length(effect_cols) > 0) effect_cols[1] else NULL

  # =============================================================================
  # DETERMINE ANALYSIS TYPE FROM DATA DIMENSIONS
  # =============================================================================
  unique_n_total <- if ("n_total" %in% names(plot_data)) {
    length(unique(plot_data$n_total))
  } else {
    1
  }

  unique_effects <- if (!is.null(effect_col) && effect_col %in% names(plot_data)) {
    length(unique(plot_data[[effect_col]]))
  } else {
    1
  }

  # Check for sequential analysis (multiple looks)
  has_looks <- "id_look" %in% names(plot_data) && length(unique(plot_data$id_look)) > 1

  # Determine analysis type
  if (unique_n_total > 1 && unique_effects > 1) {
    analysis_type <- "both"
  } else if (unique_n_total > 1 && unique_effects == 1) {
    analysis_type <- "sample_only"
  } else if (unique_n_total == 1 && unique_effects > 1) {
    analysis_type <- "effect_only"
  } else {
    analysis_type <- "single"
  }

  # =============================================================================
  # AUTO-DETECT PLOT TYPE
  # =============================================================================
  if (type == "auto") {
    if (analysis_type == "sample_only") {
      type <- "power_curve"
    } else if (analysis_type == "effect_only") {
      type <- "power_curve"
    } else if (analysis_type == "both") {
      type <- "heatmap"
    } else {
      cli::cli_abort(c(
        "Cannot auto-detect plot type",
        "x" = "Analysis type: {.val {analysis_type}}",
        "i" = "Specify {.arg type} explicitly"
      ))
    }
  }

  # =============================================================================
  # VALIDATE PARAMETERS
  # =============================================================================
  if (!metric %in% c("power", "prob", "both")) {
    cli::cli_abort(c(
      "{.arg metric} must be {.val power}, {.val prob}, or {.val both}",
      "x" = "You supplied {.val {metric}}"
    ))
  }

  if (!decision %in% c("success", "futility", "both")) {
    cli::cli_abort(c(
      "{.arg decision} must be {.val success}, {.val futility}, or {.val both}",
      "x" = "You supplied {.val {decision}}"
    ))
  }

  if (!facet_by %in% c("metric", "decision", "effect_size", "sample_size")) {
    cli::cli_abort(c(
      "{.arg facet_by} must be {.val metric}, {.val decision}, {.val effect_size}, or {.val sample_size}",
      "x" = "You supplied {.val {facet_by}}"
    ))
  }

  if (!group_by %in% c("metric", "decision", "effect_size", "sample_size")) {
    cli::cli_abort(c(
      "{.arg group_by} must be {.val metric}, {.val decision}, {.val effect_size}, or {.val sample_size}",
      "x" = "You supplied {.val {group_by}}"
    ))
  }

  # =============================================================================
  # DISPATCH TO PLOT FUNCTION
  # =============================================================================
  if (type == "power_curve") {
    p <- create_power_curve_plot(
      plot_data,
      design,
      analysis_type,
      effect_col,
      metric,
      decision,
      show_target,
      show_mcse,
      facet_by,
      has_looks,
      group_by,
      ...
    )
  } else if (type == "heatmap") {
    p <- create_heatmap_plot(
      plot_data,
      design,
      analysis_type,
      effect_col,
      metric,
      decision,
      show_target,
      target_power,
      ...
    )
  } else if (type == "comparison") {
    p <- create_comparison_plot(
      plot_data,
      design,
      analysis_type,
      effect_col,
      metric,
      decision,
      ...
    )
  } else {
    cli::cli_abort(c(
      "Unknown plot type: {.val {type}}",
      "i" = "Use {.val power_curve}, {.val heatmap}, or {.val comparison}"
    ))
  }

  # Convert to interactive plotly if requested

  if (interactive) {
    to_interactive(p, legend_position = if (type == "heatmap") "right" else "bottom")
  } else {
    p
  }
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/plot_power_curve.R"
# =============================================================================
# POWER CURVE PLOT
# =============================================================================
# Power curve visualization for single-dimension or faceted analysis.

#' Create Power Curve Plot
#'
#' Internal function to create power curve visualizations using ggplot2.
#'
#' @param plot_data Data frame with power analysis results
#' @param design Design object with p_sig_scs and p_sig_ftl
#' @param analysis_type One of "sample_only", "effect_only", or "both"
#' @param effect_col Column name for effect size (from condition_values)
#' @param metric Filter: "power", "prob", or "both"
#' @param decision Filter: "success", "futility", or "both"
#' @param show_target Whether to show target power lines
#' @param show_mcse Whether to show MCSE uncertainty ribbons
#' @param facet_by Faceting: "metric", "decision", "effect_size", or "sample_size"
#' @param has_looks Whether data has multiple interim looks
#' @param group_by Variable to use for line coloring: "decision", "metric",
#'   "effect_size", or "sample_size"
#' @param ... Additional arguments (ignored)
#'
#' @return A ggplot2 object
#' @keywords internal
#' @importFrom rlang .data
create_power_curve_plot <- function(plot_data,
                                    design,
                                    analysis_type,
                                    effect_col,
                                    metric,
                                    decision,
                                    show_target,
                                    show_mcse,
                                    facet_by,
                                    has_looks,
                                    group_by = "effect_size",
                                    ...) {
  # =============================================================================
  # DETERMINE PLOT STRUCTURE (x-axis, faceting, title)
  # =============================================================================
  # Default x-axis is sample size
  x_var <- "n_total"
  x_label <- "Total Sample Size"
  facet_var <- NULL
  title_base <- "Power Analysis"

  # Handle facet_by options
  # "decision" -> facet by outcome (Success/Futility)
  # "metric" -> facet by measure (Power/Probability)
  if (facet_by == "decision") {
    facet_var <- "outcome"
    title_base <- "Power by Decision"
  } else if (facet_by == "metric") {
    facet_var <- "measure"
    title_base <- "Power by Metric"
  } else if (facet_by == "effect_size" && !is.null(effect_col)) {
    facet_var <- effect_col
    title_base <- "Power by Effect Size"
  } else if (facet_by == "sample_size") {
    x_var <- if (!is.null(effect_col)) effect_col else "n_total"
    x_label <- if (!is.null(effect_col)) paste("Effect Size (", effect_col, ")") else "Total Sample Size"
    facet_var <- "n_total"
    title_base <- "Power by Sample Size"
  }

  # Adjust title based on analysis type
  if (analysis_type == "sample_only") {
    title_base <- paste(title_base, "- Sample Size Analysis")
  } else if (analysis_type == "effect_only") {
    x_var <- effect_col
    x_label <- paste("Effect Size (", effect_col, ")")
    title_base <- paste(title_base, "- Effect Size Analysis")
  }

  # Pivot data to long format (with SE if show_mcse)
  # Note: pivot_plot_data_long expects (decision, metric) in new naming
  plot_data_long <- pivot_plot_data_long(plot_data, decision, metric, include_se = show_mcse)

  # Determine y-axis label
  y_label <- switch(
    metric,
    "power" = "Power",
    "prob" = "Posterior Probability",
    "both" = "Power / Posterior Probability"
  )

  # =============================================================================
  # PREPARE GROUPING AND COLORING

  # =============================================================================
  # Resolve group_by to actual column name
  # API names map to internal column names:
  # - "decision" -> "outcome" (Success/Futility)
  # - "metric" -> "measure" (Power/Probability)
  color_var <- switch(
    group_by,
    "decision" = "outcome",
    "metric" = "measure",
    "effect_size" = effect_col,
    "sample_size" = "n_total",
    "outcome"  # default fallback
  )

  # Check if color variable exists in data
  if (!color_var %in% names(plot_data_long)) {
    cli::cli_warn(c(
      "Cannot group by {.val {group_by}} - column not found in data",
      "i" = "Falling back to grouping by decision"
    ))
    color_var <- "outcome"
    group_by <- "decision"
  }

  # Create color group column (factor for proper legend ordering)
  if (color_var == "outcome") {
    plot_data_long$color_group <- factor(
      plot_data_long$outcome,
      levels = c("Success", "Futility")
    )
    color_label <- "Decision"
  } else if (color_var == "measure") {
    plot_data_long$color_group <- factor(
      plot_data_long$measure,
      levels = c("Power", "Probability")
    )
    color_label <- "Metric"
  } else if (color_var == "n_total") {
    plot_data_long$color_group <- factor(
      paste0("N=", plot_data_long$n_total),
      levels = paste0("N=", sort(unique(plot_data_long$n_total)))
    )
    color_label <- "Sample Size"
  } else {
    # Effect size column
    plot_data_long$color_group <- factor(
      paste0(effect_col, "=", plot_data_long[[color_var]]),
      levels = paste0(effect_col, "=", sort(unique(plot_data_long[[color_var]])))
    )
    color_label <- "Effect Size"
  }

  # Create facet label if faceting
  if (!is.null(facet_var)) {
    if (facet_var == "outcome") {
      # Facet by Success/Futility - use outcome directly
      plot_data_long$facet_label <- factor(
        plot_data_long$outcome,
        levels = c("Success", "Futility")
      )
    } else if (facet_var == "measure") {
      # Facet by Power/Probability - use measure directly
      plot_data_long$facet_label <- factor(
        plot_data_long$measure,
        levels = c("Power", "Probability")
      )
    } else if (facet_var == "n_total") {
      plot_data_long$facet_label <- factor(
        paste0("N = ", plot_data_long$n_total),
        levels = paste0("N = ", sort(unique(plot_data_long$n_total)))
      )
    } else {
      # Effect size column
      plot_data_long$facet_label <- factor(
        paste0(facet_var, " = ", plot_data_long[[facet_var]]),
        levels = paste0(facet_var, " = ", sort(unique(plot_data_long[[facet_var]])))
      )
    }
  }

  # Build group interaction for proper line connectivity
  # Must include: outcome, measure, and any facet/color variables to avoid cross-connections
  group_vars <- c("outcome", "measure")
  if (!is.null(facet_var) && facet_var %in% names(plot_data_long)) {
    group_vars <- c(group_vars, facet_var)
  }
  if (color_var != "outcome" && color_var %in% names(plot_data_long)) {
    group_vars <- c(group_vars, color_var)
  }

  # Create group column
  if (length(group_vars) == 2) {
    plot_data_long$line_group <- interaction(
      plot_data_long[[group_vars[1]]],
      plot_data_long[[group_vars[2]]]
    )
  } else {
    plot_data_long$line_group <- interaction(
      plot_data_long[, group_vars, drop = FALSE]
    )
  }

  # =============================================================================
  # BUILD GGPLOT
  # =============================================================================
  p <- ggplot2::ggplot(
    plot_data_long,
    ggplot2::aes(
      x = .data[[x_var]],
      y = .data$value,
      color = .data$color_group,
      linetype = .data$measure,
      shape = .data$measure,
      group = .data$line_group
    )
  )

  # Add MCSE ribbons if requested
  if (show_mcse && "se" %in% names(plot_data_long)) {
    p <- p +
      ggplot2::geom_ribbon(
        ggplot2::aes(
          ymin = pmax(0, .data$value - 1.96 * .data$se),
          ymax = pmin(1, .data$value + 1.96 * .data$se),
          fill = .data$color_group
        ),
        alpha = 0.2,
        linetype = 0,
        show.legend = FALSE
      )
  }

  p <- p +
    ggplot2::geom_line(linewidth = 1) +
    ggplot2::geom_point(size = 3) +
    ggplot2::scale_y_continuous(
      labels = scales::percent_format(),
      limits = c(0, 1)
    ) +
    ggplot2::scale_linetype_manual(
      values = c("Power" = "solid", "Probability" = "dashed")
    ) +
    ggplot2::scale_shape_manual(
      values = c("Power" = 16, "Probability" = 18)
    ) +
    ggplot2::labs(
      x = x_label,
      y = y_label,
      title = paste("Bayesian RCT", title_base),
      color = color_label,
      linetype = "Measure",
      shape = "Measure"
    ) +
    rctbp_theme()

  # Add color scale based on group_by
  if (group_by == "outcome") {
    p <- p + ggplot2::scale_color_manual(values = rctbp_colors())
    if (show_mcse && "se" %in% names(plot_data_long)) {
      p <- p + ggplot2::scale_fill_manual(values = rctbp_colors())
    }
  } else {
    # Use a color palette for non-outcome groupings
    n_groups <- length(unique(plot_data_long$color_group))
    if (n_groups <= 8) {
      p <- p + ggplot2::scale_color_brewer(palette = "Set1")
      if (show_mcse && "se" %in% names(plot_data_long)) {
        p <- p + ggplot2::scale_fill_brewer(palette = "Set1")
      }
    } else {
      p <- p + ggplot2::scale_color_viridis_d()
      if (show_mcse && "se" %in% names(plot_data_long)) {
        p <- p + ggplot2::scale_fill_viridis_d()
      }
    }
  }

  # Add target lines if requested (only for decision-based coloring)
  # Note: Skip target lines when boundary functions are used (thresholds vary by look)
  if (show_target && (metric == "power" || metric == "both") && group_by == "decision") {
    # Only add target lines for numeric thresholds, not boundary functions
    p_scs_numeric <- is.numeric(design@p_sig_scs)
    p_ftl_numeric <- is.numeric(design@p_sig_ftl)

    if (p_scs_numeric || p_ftl_numeric) {
      outcomes <- character()
      yintercepts <- numeric()

      if (p_scs_numeric && (decision == "success" || decision == "both")) {
        outcomes <- c(outcomes, "Success")
        yintercepts <- c(yintercepts, design@p_sig_scs)
      }
      if (p_ftl_numeric && (decision == "futility" || decision == "both")) {
        outcomes <- c(outcomes, "Futility")
        yintercepts <- c(yintercepts, design@p_sig_ftl)
      }

      if (length(outcomes) > 0) {
        target_data <- data.frame(
          outcome = outcomes,
          yintercept = yintercepts,
          stringsAsFactors = FALSE
        )
        target_data$line_color <- ifelse(
          target_data$outcome == "Success",
          rctbp_colors()["Success"],
          rctbp_colors()["Futility"]
        )

        # Add each target line separately to get correct colors
        for (i in seq_len(nrow(target_data))) {
          p <- p +
            ggplot2::geom_hline(
              yintercept = target_data$yintercept[i],
              color = target_data$line_color[i],
              linetype = "dashed",
              linewidth = 0.5,
              alpha = 0.6
            )
        }
      }
    }
  }

  # Add faceting if needed
  if (!is.null(facet_var)) {
    p <- p + ggplot2::facet_wrap(~ facet_label)
  }

  p
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/python_simulators.R"
# =============================================================================
# PYTHON SIMULATOR INTEGRATION
# =============================================================================
# Functions to load and use Python simulation functions from inst/python/
# for efficient data generation with BayesFlow backend.
#
# The Python simulators mirror the R batch simulation functions but are more
# efficient when used with the BayesFlow backend as they avoid R/Python
# data conversion overhead.

# Package-level cache for Python simulator module
.py_sim_cache <- new.env(parent = emptyenv())


#' Load Python Simulators Module
#'
#' Loads the Python simulators from inst/python/simulators/ and caches
#' the module for reuse. These simulators are optimized for use with
#' the BayesFlow backend.
#'
#' @return Python module with simulator functions
#'
#' @details
#' The module provides:
#' - `simulate_ancova_2arms()`: 2-arm ANCOVA batch simulation
#' - `simulate_ancova_3arms()`: 3-arm ANCOVA batch simulation
#' - `ANCOVASimulator2Arms`: Class for 2-arm simulation with prior sampling
#' - `ANCOVASimulator3Arms`: Class for 3-arm simulation with prior sampling
#'
#' @export
#'
#' @examples
#' \dontrun{
#' py_sims <- load_python_simulators()
#' data <- py_sims$simulate_ancova_2arms(
#'   n_sims = 64L,
#'   n_total = 100L,
#'   b_arm_treat = 0.5,
#'   b_covariate = 0.3
#' )
#' dim(data$outcome)  # [64, 100]
#' }
load_python_simulators <- function() {
  # Return cached if available
  if (exists("simulators", envir = .py_sim_cache) &&
      !is.null(.py_sim_cache$simulators)) {
    return(.py_sim_cache$simulators)
  }

  # Ensure Python is available
  if (!requireNamespace("reticulate", quietly = TRUE)) {
    cli::cli_abort(c(
      "Package {.pkg reticulate} is required for Python simulators",
      "i" = "Install with: {.code install.packages('reticulate')}"
    ))
  }

  # Initialize BayesFlow Python environment (sets up numpy etc.)
  init_bf_python()

  # Find simulator module path
  sim_path <- system.file("python", package = "rctbayespower")
  if (sim_path == "") {
    cli::cli_abort(c(
      "Python simulators not found in package",
      "i" = "Expected location: inst/python/simulators/"
    ))
  }

  # Import simulators module
  .py_sim_cache$simulators <- tryCatch({
    reticulate::import_from_path("simulators", path = sim_path)
  }, error = function(e) {
    cli::cli_abort(c(
      "Failed to load Python simulators",
      "x" = conditionMessage(e),
      "i" = "Ensure numpy is installed: {.code reticulate::py_require('numpy')}"
    ))
  })

  .py_sim_cache$simulators
}


#' Create Python Simulation Function for Model
#'
#' Creates an R wrapper function that calls the Python simulator.
#' This function can be used as `data_simulation_fn` in `rctbp_model`
#' for efficient simulation with BayesFlow backend.
#'
#' @param model_type Character: "ancova_cont_2arms" or "ancova_cont_3arms"
#' @param ... Default parameter values passed to the simulator
#'
#' @return Function compatible with rctbp_model@data_simulation_fn
#'
#' @details
#' The returned function accepts the same parameters as the R simulation
#' functions but calls Python internally for efficiency. This is particularly
#' useful for BayesFlow power analysis where many simulations are needed.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Create Python-backed simulation function
#' py_sim_fn <- create_python_sim_fn(
#'   "ancova_cont_2arms",
#'   p_alloc = 0.5,
#'   intercept = 0,
#'   b_covariate = 0.3,
#'   sigma = 1
#' )
#'
#' # Use in model (for BayesFlow backend)
#' model <- rctbp_model(
#'   data_simulation_fn = py_sim_fn,
#'   bayesflow_model = bf_model,
#'   backend = "bf"
#' )
#' }
create_python_sim_fn <- function(model_type, ...) {
  defaults <- list(...)

  if (model_type == "ancova_cont_2arms") {
    # Return wrapper function for 2-arm ANCOVA
    function(n_sims = 1L, n_total, p_alloc = NULL, intercept = NULL,
             b_arm_treat = NULL, b_covariate = NULL, sigma = NULL) {

      py_sims <- load_python_simulators()

      # Merge defaults with provided values
      params <- list(
        n_sims = as.integer(n_sims),
        n_total = as.integer(n_total),
        p_alloc = p_alloc %||% defaults$p_alloc %||% 0.5,
        intercept = intercept %||% defaults$intercept %||% 0,
        b_arm_treat = b_arm_treat %||% defaults$b_arm_treat %||% 0,
        b_covariate = b_covariate %||% defaults$b_covariate %||% 0,
        sigma = sigma %||% defaults$sigma %||% 1
      )

      do.call(py_sims$simulate_ancova_2arms, params)
    }

  } else if (model_type == "ancova_cont_3arms") {
    # Return wrapper function for 3-arm ANCOVA
    function(n_sims = 1L, n_total, p_alloc = NULL, intercept = NULL,
             b_arm_treat = NULL, b_covariate = NULL, sigma = NULL) {

      py_sims <- load_python_simulators()

      params <- list(
        n_sims = as.integer(n_sims),
        n_total = as.integer(n_total),
        p_alloc = p_alloc %||% defaults$p_alloc %||% c(1/3, 1/3, 1/3),
        intercept = intercept %||% defaults$intercept %||% 0,
        b_arm_treat = b_arm_treat %||% defaults$b_arm_treat %||% c(0, 0),
        b_covariate = b_covariate %||% defaults$b_covariate %||% 0,
        sigma = sigma %||% defaults$sigma %||% 1
      )

      do.call(py_sims$simulate_ancova_3arms, params)
    }

  } else {
    cli::cli_abort(c(
      "Unknown model type: {.val {model_type}}",
      "i" = "Supported types: 'ancova_cont_2arms', 'ancova_cont_3arms'"
    ))
  }
}


#' Get Python Simulator Class
#'
#' Returns a Python simulator class that can be used directly with
#' BayesFlow for training or inference.
#'
#' @param model_type Character: "ancova_cont_2arms" or "ancova_cont_3arms"
#' @param ... Parameters to initialize the simulator class
#'
#' @return Python simulator class instance
#'
#' @details
#' The simulator class provides:
#' - `__call__(n_sims, n_total, ...)`: Generate simulated data
#' - `sample_prior(n_sims)`: Sample parameters from prior distribution
#'
#' This is useful for BayesFlow training workflows where the simulator
#' needs to be passed directly to BayesFlow functions.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Get simulator class for BayesFlow training
#' simulator <- get_python_simulator(
#'   "ancova_cont_2arms",
#'   b_arm_treat = 0.5,
#'   b_covariate = 0.3
#' )
#'
#' # Use with BayesFlow
#' import bayesflow as bf
#' workflow <- bf$make_simulator(simulator)
#' }
get_python_simulator <- function(model_type, ...) {
  py_sims <- load_python_simulators()

  params <- list(...)

  if (model_type == "ancova_cont_2arms") {
    do.call(py_sims$ANCOVASimulator2Arms, params)
  } else if (model_type == "ancova_cont_3arms") {
    do.call(py_sims$ANCOVASimulator3Arms, params)
  } else {
    cli::cli_abort(c(
      "Unknown model type: {.val {model_type}}",
      "i" = "Supported types: 'ancova_cont_2arms', 'ancova_cont_3arms'"
    ))
  }
}


#' Check if Python Simulators are Available
#'
#' Checks if the Python simulator infrastructure is available.
#' This includes checking for reticulate, Python, numpy, and the
#' simulator module itself.
#'
#' @param silent If TRUE, return FALSE instead of error (default FALSE)
#'
#' @return Logical indicating availability
#' @export
#'
#' @examples
#' if (check_python_sims_available(silent = TRUE)) {
#'   message("Python simulators available!")
#' }
check_python_sims_available <- function(silent = FALSE) {
  # Check reticulate
  if (!requireNamespace("reticulate", quietly = TRUE)) {
    if (silent) return(FALSE)
    cli::cli_abort("Package 'reticulate' is required")
  }

  # Check Python
  if (!reticulate::py_available(initialize = TRUE)) {
    if (silent) return(FALSE)
    cli::cli_abort("Python is not available")
  }

  # Check numpy
  if (!reticulate::py_module_available("numpy")) {
    if (silent) return(FALSE)
    cli::cli_abort("Python package 'numpy' is required")
  }

  # Check simulator module exists
  sim_path <- system.file("python", "simulators", "__init__.py",
                          package = "rctbayespower")
  if (sim_path == "") {
    if (silent) return(FALSE)
    cli::cli_abort("Python simulators not found in package")
  }

  TRUE
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/rctbayespower-package.R"
#' The 'rctbayespower' package.
#'
#' @description
#' Bayesian power analysis for randomized controlled trials (RCTs) using brms and Stan.
#'
#' @name rctbayespower-package
#' @aliases rctbayespower
#' @keywords internal
#'
"_PACKAGE"

## Suppress R CMD check warnings for undefined global variables
if (getRversion() >= "2.15.1") {
  # Global variables for R CMD check
  utils::globalVariables(
    c(
      # New column names
      "par_name",
      "thr_scs",
      "thr_ftl",
      "p_sig_scs",
      "p_sig_ftl",
      "id_cond",
      "id_iter",
      "id_look",
      "pr_scs",
      "pr_ftl",
      "dec_scs",
      "dec_ftl",
      "post_med",
      "post_mad",
      "post_mn",
      "post_sd",
      "pwr_scs",
      "pwr_ftl",
      "error_msg",
      # Variables from summarize_sims function
      "pr_scs_mean",
      "pr_scs_mcse",
      "pr_ftl_mean",
      "pr_ftl_mcse",
      "pwr_scs_mean",
      "pwr_scs_mcse",
      "pwr_ftl_mean",
      "pwr_ftl_mcse",
      "post_med_mean",
      "post_med_mcse",
      "post_mad_mean",
      "post_mad_mcse",
      "post_mn_mean",
      "post_mn_mcse",
      "post_sd_mean",
      "post_sd_mcse",
      "conv_rate_mean",
      "conv_rate_mcse",
      "rhat_mean",
      "rhat_mcse",
      "ess_bulk_mean",
      "ess_bulk_mcse",
      "ess_tail_mean",
      "ess_tail_mcse",
      # Standard diagnostics (unchanged)
      "rhat",
      "ess_bulk",
      "ess_tail",
      "converged",
      # Other variables
      "arm",
      "baseline",
      "convergence_rate",
      "measures",
      "res",
      "i",
      "required_parameters",
      "covariate",
      "x",
      "n",
      # Variables from summarize_sims_with_interim function
      "n_analyzed",
      "stop_reason",
      "stop_n",
      "n_planned",
      "effective_n",
      "stopped_early",
      "n_mn",
      # Per-look stopping stats
      "n_stp_look",
      "n_scs_look",
      "n_ftl_look",
      "prop_stp_look",
      "prop_scs_look",
      "prop_ftl_look",
      "cumul_stp",
      # Overall stopping proportions
      "prop_stp_scs",
      "prop_stp_ftl"
    )
  )
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/report_builders.R"
#' Report Builders for S7 Objects
#'
#' Build structured report data for rendering in different output modes.
#'
#' @name report_builders
#' @keywords internal
NULL

#' Format Boundary Specification for Display
#'
#' Formats a probability threshold or boundary function into a display string.
#'
#' @param threshold Either a numeric value or a boundary function
#' @return Character string describing the threshold
#' @keywords internal
format_boundary <- function(threshold) {
  if (is.function(threshold)) {
    # Check for boundary function metadata
    boundary_type <- attr(threshold, "boundary_type")
    boundary_params <- attr(threshold, "boundary_params")

    if (!is.null(boundary_type) && !is.null(boundary_params)) {
      # Format based on boundary type
      switch(boundary_type,
        "obf" = paste0("O'Brien-Fleming (", boundary_params$base, ")"),
        "pocock" = paste0("Pocock (", boundary_params$threshold, ")"),
        "linear" = paste0("Linear (", boundary_params$start, " \u2192 ", boundary_params$end, ")"),
        "power" = paste0("Power (base=", boundary_params$base, ", rho=", boundary_params$rho, ")"),
        # Fallback for unknown types
        paste0("Function (", boundary_type, ")")
      )
    } else {
      # Generic function without metadata - evaluate at sample points
      sample_vals <- c(0.5, 1.0)
      sample_results <- sapply(sample_vals, threshold)
      paste0("Function (at 50%: ", round(sample_results[1], 4),
             ", at 100%: ", round(sample_results[2], 4), ")")
    }
  } else {
    as.character(threshold)
  }
}

#' Format Value Range
#'
#' Formats a numeric vector as a range string "min-max" with optional percentage.
#'
#' @param vals Numeric vector
#' @param pct Logical; if TRUE, format as percentages (default FALSE)
#' @param digits Number of decimal digits (default 1)
#' @return Character string representing the range
#' @keywords internal
fmt_range <- function(vals, pct = FALSE, digits = 1) {
  rng <- range(vals, na.rm = TRUE)
  if (pct) {
    paste0(round(rng[1] * 100, digits), "%-", round(rng[2] * 100, digits), "%")
  } else {
    paste0(round(rng[1], digits), "-", round(rng[2], digits))
  }
}

#' Format Parameters Compactly
#'
#' Formats a named list of parameters as "name=value, name=value" string.
#'
#' @param params Named list of parameters
#' @return Character string with formatted parameters
#' @keywords internal
fmt_params <- function(params) {
  paste(
    names(params),
    sapply(params, function(v) if (is.numeric(v)) round(v, 3) else as.character(v)),
    sep = "=",
    collapse = ", "
  )
}

#' Build Report for rctbp_model
#'
#' Creates structured report data for a model object.
#'
#' @param x rctbp_model object
#' @return List with report sections
#' @keywords internal
#'
build_report.rctbp_model <- function(x) {
  list(
    title = "S7 Object: rctbp_model",
    sections = list(
      list(
        name = "Model Information",
        items = list(
          "Model name" = x@model_name,
          "Backend" = x@backend,
          "Predefined model" = if (is.null(x@predefined_model)) "None" else x@predefined_model,
          "Number of endpoints" = x@n_endpoints,
          "Endpoint types" = paste(x@endpoint_types, collapse = ", "),
          "Number of arms" = x@n_arms,
          "Number of repeated measures" = if (is.null(x@n_repeated_measures)) "NULL" else x@n_repeated_measures,
          "Parameter names - simulation function" = paste(x@parameter_names_sim_fn, collapse = ", ")
        )
      ),
      if (x@backend == "brms") {
        list(
          name = "BRMS Model",
          items = list(
            "Parameter names - brms model" = paste(x@parameter_names_brms, collapse = ", ")
          ),
          brms_model = x@brms_model
        )
      } else if (x@backend == "bf") {
        list(
          name = "BayesFlow Model",
          bayesflow_model = x@bayesflow_model
        )
      },
      if (x@backend == "brms" && length(x@backend_args_brms) > 0) {
        list(
          name = "brms Arguments",
          backend_args = x@backend_args_brms
        )
      }
    )
  )
}

#' Build Report for rctbp_design
#'
#' Creates structured report data for a design object.
#'
#' @param x rctbp_design object
#' @return List with report sections
#' @keywords internal
#'
build_report.rctbp_design <- function(x) {
  list(
    title = "S7 Object: rctbp_design",
    sections = list(
      list(
        name = "Model Specifications",
        items = list(
          "Number of endpoints" = x@model@n_endpoints,
          "Endpoint types" = paste(x@model@endpoint_types, collapse = ", "),
          "Number of arms" = x@model@n_arms,
          "Number of repeated measures" = if (is.null(x@model@n_repeated_measures)) "NULL" else x@model@n_repeated_measures,
          "Parameter names - simulation function" = paste(x@model@parameter_names_sim_fn, collapse = ", "),
          "Parameter names - brms model" = paste(x@model@parameter_names_brms, collapse = ", ")
        )
      ),
      list(
        name = "Design Specifications",
        items = list(
          "Design name" = if (is.null(x@design_name)) "NULL" else x@design_name,
          "Target parameters" = paste(x@target_params, collapse = ", "),
          "Probability threshold for success" = format_boundary(x@p_sig_scs),
          "Probability threshold for futility" = format_boundary(x@p_sig_ftl)
        ),
        note = "Effect size thresholds are specified per-condition in build_conditions()."
      ),
      list(
        name = "Interim Analysis (Design Defaults)",
        items = list(
          "Analysis timepoints" = if (is.null(x@analysis_at)) "None (single-look)" else paste(x@analysis_at, collapse = ", "),
          "Stopping rules" = if (is.null(x@interim_function)) {
            if (is.null(x@analysis_at)) "N/A (single-look)" else "Default (dec_scs=1 or dec_ftl=1)"
          } else {
            "Custom (interim_function specified)"
          },
          "Adaptive design" = x@adaptive
        ),
        note = if (!is.null(x@analysis_at)) {
          "These defaults apply to all conditions unless overridden."
        } else NULL
      ),
      if (x@model@backend == "brms") {
        list(
          name = "brms Model",
          brms_model = x@model@brms_model
        )
      } else if (x@model@backend == "bf") {
        list(
          name = "BayesFlow Model",
          bayesflow_model = x@model@bayesflow_model
        )
      }
    )
  )
}

#' Build Report for rctbp_conditions
#'
#' Creates structured report data for a conditions object.
#'
#' @param x rctbp_conditions object
#' @return List with report sections
#' @keywords internal
#'
build_report.rctbp_conditions <- function(x) {
  n_conditions <- nrow(x@conditions_grid)
  n_params <- ncol(x@conditions_grid) - 1  # Subtract 1 for id_cond column
  n_static_params <- length(x@static_values)

  target_pwr_display <- if (is.null(x@target_pwr)) {
    "Not set (will show highest power)"
  } else {
    paste0(round(x@target_pwr * 100, 1), "%")
  }

  list(
    title = "S7 Object: rctbp_conditions",
    sections = list(
      list(
        name = "Summary",
        items = list(
          "Number of conditions" = n_conditions,
          "Number of varying parameters" = n_params,
          "Number of static parameters" = n_static_params,
          "Target power for optimal condition" = target_pwr_display
        )
      ),
      list(
        name = "Condition Grid",
        grid = x@conditions_grid
      )
    )
  )
}

#' Build Report for rctbp_power_analysis
#'
#' Creates structured report data for a power analysis object.
#'
#' @param x rctbp_power_analysis object
#' @param target_pwr Target power for optimal condition (default NULL shows highest)
#' @return List with report sections
#' @keywords internal
#'
build_report.rctbp_power_analysis <- function(x, target_pwr = NULL) {
  design <- x@conditions@design
  has_results <- nrow(x@results_conditions) > 0 || nrow(x@results_raw) > 0

  target_pwr_display <- if (is.null(target_pwr)) {
    "Not set (showing highest power)"
  } else {
    paste0(round(target_pwr * 100, 1), "%")
  }

  report <- list(
    title = "Power Analysis Summary",
    sections = list(
      list(
        name = "Design Summary",
        items = list(
          "Target parameters" = paste(design@target_params, collapse = ", "),
          "Success probability threshold" = format_boundary(design@p_sig_scs),
          "Futility probability threshold" = format_boundary(design@p_sig_ftl),
          "Target power" = target_pwr_display
        )
      )
    )
  )

  if (has_results) {
    # Completed analysis
    n_conditions <- nrow(x@conditions@conditions_grid)

    # Check for interim analysis results (via S7 property)
    has_interim <- x@has_interim
    # For sequential: power metrics in results_interim, overall stats in results_conditions
    # For single-look: power metrics in results_conditions
    results_df <- if (has_interim) x@results_interim else x@results_conditions
    interim_overall <- if (has_interim) x@results_conditions else NULL

    # Power ranges
    power_ranges <- NULL
    power_cols <- intersect(names(results_df), c("pwr_scs", "pwr_ftl"))
    if (length(power_cols) > 0) {
      power_ranges <- lapply(power_cols, function(col) {
        power_range <- range(results_df[[col]], na.rm = TRUE)
        list(
          name = gsub("pwr_", "", col),
          range = paste0(round(power_range[1] * 100, 1), "% - ", round(power_range[2] * 100, 1), "%")
        )
      })
    }

    report$status <- "COMPLETED"
    report$sections <- c(report$sections, list(
      list(
        name = "Results Summary",
        items = list(
          "Analysis runtime" = if (!is.null(x@elapsed_time)) paste0(round(x@elapsed_time, 2), " minutes") else "Not available",
          "Conditions analyzed" = n_conditions,
          "Simulations per condition" = x@n_sims,
          "Total simulations" = n_conditions * x@n_sims,
          "Design type" = if (has_interim) "Sequential (with interim analyses)" else "Single-look"
        ),
        power_ranges = power_ranges
      )
    ))

    # Find optimal condition for target power
    optimal <- find_optimal_condition(
      results_summ = results_df,
      conditions_grid = x@conditions@conditions_grid,
      target_pwr = target_pwr,
      interim_overall = interim_overall,
      power_col = "pwr_scs"
    )

    # Helper to format condition parameters
    format_params <- function(params) {
      paste(
        names(params),
        sapply(params, function(v) {
          if (is.numeric(v)) round(v, 3) else as.character(v)
        }),
        sep = " = ",
        collapse = ", "
      )
    }

    # Helper to format interim stats (matches column names from get_interim_stats)
    format_interim <- function(interim) {
      if (is.null(interim)) return(NULL)
      list(
        n_mn = round(interim$n_mn, 0),
        n_mdn = round(interim$n_mdn, 0),
        n_mode = round(interim$n_mode, 0),
        prop_at_mode = paste0(round(interim$prop_at_mode * 100, 1), "%"),
        prop_stp_early = paste0(round(interim$prop_stp_early * 100, 1), "%"),
        prop_stp_scs = paste0(round(interim$prop_stp_scs * 100, 1), "%"),
        prop_stp_ftl = paste0(round(interim$prop_stp_ftl * 100, 1), "%"),
        prop_no_dec = paste0(round(interim$prop_no_dec * 100, 1), "%")
      )
    }

    if (optimal$found || optimal$mode == "highest") {
      # Found optimal or showing highest power
      param_str <- format_params(optimal$condition_params)
      interim_fmt <- format_interim(optimal$interim)

      section_name <- if (optimal$mode == "highest") {
        "Highest Power Condition"
      } else {
        "Optimal Condition"
      }

      note_text <- if (optimal$mode == "highest") {
        "Condition with highest achieved power. Set target with: print(x, target_pwr = 0.8)"
      } else {
        "Smallest sample size achieving target power. Override with: print(x, target_pwr = 0.9)"
      }

      report$sections <- c(report$sections, list(
        list(
          name = section_name,
          optimal_condition = list(
            found = TRUE,
            mode = optimal$mode,
            target_pwr = if (!is.null(target_pwr)) paste0(round(target_pwr * 100, 1), "%") else NULL,
            achieved_pwr = paste0(round(optimal$achieved_pwr * 100, 1), "%"),
            n_total = optimal$n_total,
            condition_id = optimal$condition_id,
            params = param_str,
            interim = interim_fmt
          ),
          note = note_text
        )
      ))
    } else if (!is.null(optimal$closest)) {
      # No condition meets target - show closest
      param_str <- format_params(optimal$closest$condition_params)
      interim_fmt <- format_interim(optimal$closest$interim)

      report$sections <- c(report$sections, list(
        list(
          name = "Optimal Condition",
          optimal_condition = list(
            found = FALSE,
            mode = "target",
            target_pwr = paste0(round(target_pwr * 100, 1), "%"),
            closest_pwr = paste0(round(optimal$closest$achieved_pwr * 100, 1), "%"),
            closest_n = optimal$closest$n_total,
            closest_id = optimal$closest$condition_id,
            params = param_str,
            interim = interim_fmt
          ),
          note = "No condition achieves target power. Showing closest. Try larger sample sizes."
        )
      ))
    }

    report$sections <- c(report$sections, list(
      list(
        name = "Available Actions",
        actions = c(
          "plot() - Create visualizations",
          "power_config@results_conditions - Access condition-level results",
          "power_config@results_interim - Access per-look results (sequential only)",
          "power_config@results_raw - Access raw simulation results"
        )
      )
    ))
  } else {
    # Pending analysis
    n_conditions <- nrow(x@conditions@conditions_grid)
    total_sims <- n_conditions * x@n_sims

    report$status <- "PENDING"
    report$sections <- c(report$sections, list(
      list(
        name = "Analysis Configuration",
        items = list(
          "Number of simulations per condition" = x@n_sims,
          "Number of cores for parallel execution" = x@n_cores,
          "Verbose output" = x@verbose,
          "Design prior" = if (is.null(x@design_prior)) {
            "None"
          } else if (is.function(x@design_prior)) {
            "Custom function"
          } else {
            as.character(x@design_prior)
          }
        ),
        brms_args = if (length(x@brms_args) > 0) x@brms_args else NULL
      ),
      list(
        name = "Analysis Preview",
        items = list(
          "Total conditions" = n_conditions,
          "Total simulations" = total_sims
        )
      ),
      list(
        name = "Available Actions",
        actions = c(
          "run() - Execute the analysis",
          "power_config@conditions - View condition details"
        )
      )
    ))
  }

  return(report)
}

#' Find Optimal Condition Based on Target Power
#'
#' Identifies the optimal condition based on target power. If target_pwr is NULL,
#' returns the condition with highest power. If target_pwr is specified, finds
#' the smallest sample size that achieves at least that power.
#'
#' @param results_summ A data.frame with summarized power analysis results
#' @param conditions_grid A data.frame with condition parameter combinations
#' @param target_pwr Target power level (0 to 1), or NULL for highest power
#' @param interim_overall Optional data.frame with interim analysis stats
#' @param power_col Column name for power values (default "pwr_scs")
#'
#' @return A list with:
#'   \item{found}{Logical indicating if an optimal condition was found}
#'   \item{mode}{"highest" if target_pwr is NULL, "target" otherwise}
#'   \item{target_pwr}{The target power used (NULL if mode = "highest")}
#'   \item{n_total}{Sample size of optimal condition}
#'   \item{achieved_pwr}{Achieved power of optimal condition}
#'   \item{condition_id}{ID of optimal condition}
#'   \item{condition_params}{Named list of condition parameters}
#'   \item{interim}{Interim analysis stats for this condition (if available)}
#'   \item{closest}{If no condition meets target, info on closest condition}
#'
#' @keywords internal
#'
find_optimal_condition <- function(results_summ, conditions_grid, target_pwr,
                                    interim_overall = NULL,
                                    power_col = "pwr_scs") {
  # Default return for cases where no optimal found
  not_found <- list(
    found = FALSE,
    mode = if (is.null(target_pwr)) "highest" else "target",
    target_pwr = target_pwr,
    n_total = NA_real_,
    achieved_pwr = NA_real_,
    condition_id = NA_integer_,
    condition_params = NULL,
    interim = NULL,
    closest = NULL
  )

  # Validate inputs
  if (nrow(results_summ) == 0 || nrow(conditions_grid) == 0) {
    return(not_found)
  }

  if (!power_col %in% names(results_summ)) {
    return(not_found)
  }

  # Check if n_total is in conditions_grid
  if (!"n_total" %in% names(conditions_grid)) {
    return(not_found)
  }

  # Identify condition ID column in results_summ
  if (!"id_cond" %in% names(results_summ)) {
    return(not_found)
  }
  id_col <- "id_cond"

  # Join results with conditions grid
  # Get unique power per condition (handle multi-parameter cases)
  results_by_cond <- stats::aggregate(
    stats::as.formula(paste(power_col, "~", id_col)),
    data = results_summ,
    FUN = max  # Take max power if multiple parameters
  )
  names(results_by_cond) <- c("id_cond", "power")

  # Merge with conditions grid
  merged <- merge(
    conditions_grid,
    results_by_cond,
    by = "id_cond",
    all.x = FALSE
  )

  if (nrow(merged) == 0) {
    return(not_found)
  }

  # Helper to extract interim stats for a condition
 get_interim_stats <- function(cond_id) {
    if (is.null(interim_overall) || nrow(interim_overall) == 0) {
      return(NULL)
    }
    # Find row for this condition
    if (!"id_cond" %in% names(interim_overall)) {
      return(NULL)
    }
    row <- interim_overall[interim_overall[["id_cond"]] == cond_id, , drop = FALSE]
    if (nrow(row) == 0) {
      return(NULL)
    }
    list(
      n_mn = row$n_mn[1],
      n_mdn = row$n_mdn[1],
      n_mode = row$n_mode[1],
      prop_at_mode = row$prop_at_mode[1],
      prop_stp_early = row$prop_stp_early[1],
      prop_stp_scs = row$prop_stp_scs[1],
      prop_stp_ftl = row$prop_stp_ftl[1],
      prop_no_dec = row$prop_no_dec[1]
    )
  }

  # Helper to build result
  build_result <- function(row, mode, target, closest_info = NULL) {
    param_cols <- setdiff(names(row), c("id_cond", "power"))
    condition_params <- as.list(row[1, param_cols, drop = FALSE])

    list(
      found = is.null(closest_info),
      mode = mode,
      target_pwr = target,
      n_total = row$n_total[1],
      achieved_pwr = row$power[1],
      condition_id = row$id_cond[1],
      condition_params = condition_params,
      interim = get_interim_stats(row$id_cond[1]),
      closest = closest_info
    )
  }

  # Mode 1: NULL target_pwr - return highest power condition
  if (is.null(target_pwr)) {
    best_idx <- which.max(merged$power)
    optimal <- merged[best_idx, , drop = FALSE]
    return(build_result(optimal, mode = "highest", target = NULL))
  }

  # Mode 2: Specific target_pwr - find smallest n_total meeting target
  meets_target <- merged[merged$power >= target_pwr, , drop = FALSE]

  if (nrow(meets_target) > 0) {
    # Find minimum n_total among those meeting target
    min_n <- min(meets_target$n_total, na.rm = TRUE)
    candidates <- meets_target[meets_target$n_total == min_n, , drop = FALSE]

    # If multiple, pick highest power
    best_idx <- which.max(candidates$power)
    optimal <- candidates[best_idx, , drop = FALSE]

    return(build_result(optimal, mode = "target", target = target_pwr))
  } else {
    # No condition meets target; find closest (highest power)
    best_idx <- which.max(merged$power)
    closest <- merged[best_idx, , drop = FALSE]

    param_cols <- setdiff(names(closest), c("id_cond", "power"))
    closest_params <- as.list(closest[1, param_cols, drop = FALSE])

    closest_info <- list(
      n_total = closest$n_total[1],
      achieved_pwr = closest$power[1],
      condition_id = closest$id_cond[1],
      condition_params = closest_params,
      interim = get_interim_stats(closest$id_cond[1])
    )

    return(list(
      found = FALSE,
      mode = "target",
      target_pwr = target_pwr,
      n_total = NA_real_,
      achieved_pwr = NA_real_,
      condition_id = NA_integer_,
      condition_params = NULL,
      interim = NULL,
      closest = closest_info
    ))
  }
}


# =============================================================================
# TOPIC-SPECIFIC REPORTS
# =============================================================================

#' Report on Power Metrics per Condition
#'
#' Displays power analysis metrics for each simulation condition, including
#' success/futility rates, posterior estimates, and convergence diagnostics.
#'
#' @param x An rctbp_power_analysis object with results
#' @param format Output format: "cli" for styled console output (default)
#'   or "markdown" for markdown-formatted output suitable for Quarto/RMarkdown.
#' @param heading_level Integer specifying the starting heading level for
#'   markdown output (default 2). Use this to integrate reports into documents
#'   where you need headings to start at a different level (e.g., 3 for `###`).
#'
#' @return Invisibly returns the input object. Prints report as side effect.
#'
#' @details
#' The report includes a table with:
#' \itemize{
#'   \item Condition identifiers and sample sizes
#'   \item Power metrics: `pwr_scs` (success rate), `pwr_ftl` (futility rate)
#'   \item Posterior estimates: `post_mn`, `post_sd`
#'   \item Convergence: `rhat`, `ess_bulk`
#' }
#'
#' For sequential designs, power metrics are taken from the final analysis look.
#'
#' @seealso [report()], [report_stopping()], [report_stopping_by_look()]
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Power metrics report
#' report_power(result)
#'
#' # Markdown format for Quarto integration
#' report_power(result, format = "markdown")
#' }
report_power <- function(x, format = c("cli", "markdown"), heading_level = 2L) {
  format <- match.arg(format)
  heading_level <- as.integer(heading_level)

  # Check for S7 class
  if (!inherits(x, "rctbp_power_analysis") &&
      !inherits(x, "rctbayespower::rctbp_power_analysis")) {
    cli::cli_abort("{.arg x} must be an rctbp_power_analysis object")
  }

  has_results <- nrow(x@results_conditions) > 0
  if (!has_results) {
    cli::cli_abort(c(
      "Analysis has not been run",
      "i" = "Use {.code run(x)} first"
    ))
  }

  # Get power metrics
  if (x@has_interim) {
    # For sequential: use final look from results_interim
    results_interim <- x@results_interim
    final_look <- max(results_interim$id_look)
    power_df <- results_interim[results_interim$id_look == final_look, , drop = FALSE]
  } else {
    # For single-look: use results_conditions directly
    power_df <- x@results_conditions
  }

  # Select relevant columns
  cols <- c("id_cond", "n_total", "par_name", "pwr_scs", "se_pwr_scs",
            "pwr_ftl", "se_pwr_ftl", "post_mn", "post_sd", "rhat", "ess_bulk")
  cols_available <- intersect(cols, names(power_df))
  power_table <- power_df[, cols_available, drop = FALSE]

  # Sort by power (descending)
  if ("pwr_scs" %in% names(power_table)) {
    power_table <- power_table[order(-power_table$pwr_scs), ]
  }

  # Build report
  report <- list(
    title = "Power Metrics by Condition",
    sections = list(
      list(
        name = "Power Results",
        grid = power_table
      )
    )
  )

  render_report(report, format = format, heading_level = heading_level)
  invisible(x)
}


#' Report on Early Stopping Metrics per Condition
#'
#' Displays early stopping statistics aggregated per condition. This report
#' is only available for sequential designs with interim analyses.
#'
#' @param x An rctbp_power_analysis object with sequential design results
#' @param format Output format: "cli" for styled console output (default)
#'   or "markdown" for markdown-formatted output suitable for Quarto/RMarkdown.
#' @param heading_level Integer specifying the starting heading level for
#'   markdown output (default 2). Use this to integrate reports into documents
#'   where you need headings to start at a different level (e.g., 3 for `###`).
#'
#' @return Invisibly returns the input object. Prints report as side effect.
#'
#' @details
#' The report includes a table with per-condition statistics:
#' \itemize{
#'   \item Sample sizes: `n_total`, `n_planned`, `n_mn`, `n_mdn`, `n_mode`
#'   \item Stopping proportions: `prop_stp_early`, `prop_stp_scs`, `prop_stp_ftl`
#'   \item Modal stopping: `prop_at_mode` (proportion stopped at modal N)
#'   \item No decision rate: `prop_no_dec`
#' }
#'
#' @seealso [report()], [report_power()], [report_stopping_by_look()]
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Early stopping summary per condition
#' report_stopping(result)
#'
#' # Markdown format for Quarto integration
#' report_stopping(result, format = "markdown")
#' }
report_stopping <- function(x, format = c("cli", "markdown"), heading_level = 2L) {
  format <- match.arg(format)
  heading_level <- as.integer(heading_level)

  # Check for S7 class
  if (!inherits(x, "rctbp_power_analysis") &&
      !inherits(x, "rctbayespower::rctbp_power_analysis")) {
    cli::cli_abort("{.arg x} must be an rctbp_power_analysis object")
  }

  if (!x@has_interim) {
    cli::cli_abort(c(
      "Early stopping report requires a sequential design",
      "i" = "This analysis does not have interim analyses"
    ))
  }

  has_results <- nrow(x@results_conditions) > 0
  if (!has_results) {
    cli::cli_abort(c(
      "Analysis has not been run",
      "i" = "Use {.code run(x)} first"
    ))
  }

  # Use results_conditions which has overall stopping stats
  stopping_df <- x@results_conditions

  # Select relevant columns
  cols <- c("id_cond", "n_total", "n_planned", "n_mn", "se_n_mn", "n_mdn",
            "n_mode", "prop_at_mode", "prop_stp_early", "prop_stp_scs",
            "prop_stp_ftl", "prop_no_dec")
  cols_available <- intersect(cols, names(stopping_df))
  stopping_table <- stopping_df[, cols_available, drop = FALSE]

  # Build report
  report <- list(
    title = "Early Stopping by Condition",
    sections = list(
      list(
        name = "Stopping Statistics",
        grid = stopping_table
      )
    )
  )

  render_report(report, format = format, heading_level = heading_level)
  invisible(x)
}


#' Report on Early Stopping per Look and Condition
#'
#' Displays detailed early stopping statistics broken down by analysis look
#' and condition. This report is only available for sequential designs.
#'
#' @param x An rctbp_power_analysis object with sequential design results
#' @param format Output format: "cli" for styled console output (default)
#'   or "markdown" for markdown-formatted output suitable for Quarto/RMarkdown.
#' @param heading_level Integer specifying the starting heading level for
#'   markdown output (default 2). Use this to integrate reports into documents
#'   where you need headings to start at a different level (e.g., 3 for `###`).
#'
#' @return Invisibly returns the input object. Prints report as side effect.
#'
#' @details
#' The report includes a table with per-look  per-condition statistics:
#' \itemize{
#'   \item Look identifiers: `id_cond`, `id_look`, `n_analyzed`
#'   \item Power at this look: `pwr_scs`, `pwr_ftl`
#'   \item Stopping at this look: `prop_stp_look`, `prop_scs_look`, `prop_ftl_look`
#'   \item Cumulative stopping: `cumul_stp`
#' }
#'
#' @seealso [report()], [report_power()], [report_stopping()]
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Per-look stopping breakdown
#' report_stopping_by_look(result)
#'
#' # Markdown format for Quarto integration
#' report_stopping_by_look(result, format = "markdown")
#' }
report_stopping_by_look <- function(x, format = c("cli", "markdown"),
                                     heading_level = 2L) {
  format <- match.arg(format)
  heading_level <- as.integer(heading_level)

  # Check for S7 class
  if (!inherits(x, "rctbp_power_analysis") &&
      !inherits(x, "rctbayespower::rctbp_power_analysis")) {
    cli::cli_abort("{.arg x} must be an rctbp_power_analysis object")
  }

  if (!x@has_interim) {
    cli::cli_abort(c(
      "Per-look report requires a sequential design",
      "i" = "This analysis does not have interim analyses"
    ))
  }

  has_results <- nrow(x@results_interim) > 0
  if (!has_results) {
    cli::cli_abort(c(
      "Analysis has not been run",
      "i" = "Use {.code run(x)} first"
    ))
  }

  # Use results_interim which has per-look data
  look_df <- x@results_interim
  n_looks <- length(unique(look_df$id_look))

  # Select relevant columns
  cols <- c("id_cond", "id_look", "n_analyzed", "pwr_scs", "pwr_ftl",
            "prop_stp_look", "prop_scs_look", "prop_ftl_look", "cumul_stp")
  cols_available <- intersect(cols, names(look_df))
  look_table <- look_df[, cols_available, drop = FALSE]

  # Sort by condition then look
  look_table <- look_table[order(look_table$id_cond, look_table$id_look), ]

  # Build report
  report <- list(
    title = paste0("Early Stopping by Look (", n_looks, " looks)"),
    sections = list(
      list(
        name = "Per-Look Statistics",
        grid = look_table
      )
    )
  )

  render_report(report, format = format, heading_level = heading_level)
  invisible(x)
}


# Legacy aliases for backward compatibility
#' @rdname report_stopping
#' @export
report_early_stopping <- report_stopping

#' @rdname report_power
#' @export
report_conditions <- report_power


#' Generate Topic-Specific Reports
#'
#' Unified interface for generating detailed reports on specific aspects
#' of power analysis results. Multiple topics can be specified to generate
#' concatenated reports.
#'
#' @param x An rctbp_power_analysis object
#' @param topic Character vector specifying report topic(s). Valid values:
#'   \describe{
#'     \item{"power"}{Power metrics per condition}
#'     \item{"stopping"}{Early stopping summary per condition (sequential only)}
#'     \item{"stopping_by_look"}{Early stopping per look  condition (sequential only)}
#'   }
#'   Multiple topics can be specified to generate concatenated reports.
#' @param format Output format: "cli" for styled console output (default)
#'   or "markdown" for markdown-formatted output suitable for Quarto/RMarkdown.
#' @param heading_level Integer specifying the starting heading level for
#'   markdown output (default 2). Use this to integrate reports into documents
#'   where you need headings to start at a different level (e.g., 3 for `###`).
#' @param ... Additional arguments (currently unused)
#'
#' @return Invisibly returns the input object.
#'
#' @seealso [report_power()], [report_stopping()], [report_stopping_by_look()],
#'   [summary.rctbp_power_analysis()]
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Power metrics per condition
#' report(result, topic = "power")
#'
#' # Early stopping summary per condition (sequential only)
#' report(result, topic = "stopping")
#'
#' # Per-look stopping breakdown (sequential only)
#' report(result, topic = "stopping_by_look")
#'
#' # Multiple topics - generates concatenated reports
#' report(result, topic = c("power", "stopping", "stopping_by_look"))
#'
#' # Markdown format for Quarto integration
#' report(result, topic = "power", format = "markdown")
#'
#' # Start headings at level 3 (###) for embedding in a document section
#' report(result, topic = "stopping", format = "markdown", heading_level = 3)
#' }
report <- function(x, topic = "power",
                   format = c("cli", "markdown"), heading_level = 2L, ...) {
  # Validate topics
  valid_topics <- c("power", "stopping", "stopping_by_look")
  invalid <- setdiff(topic, valid_topics)
  if (length(invalid) > 0) {
    cli::cli_abort(c(
      "Invalid topic{?s}: {.val {invalid}}",
      "i" = "Valid topics are: {.val {valid_topics}}"
    ))
  }

  format <- match.arg(format)
  heading_level <- as.integer(heading_level)

  if (heading_level < 1 || heading_level > 6) {
    cli::cli_abort("{.arg heading_level} must be between 1 and 6")
  }

  # Generate each report in sequence
  for (t in topic) {
    switch(t,
      "power" = report_power(x, format = format, heading_level = heading_level),
      "stopping" = report_stopping(x, format = format, heading_level = heading_level),
      "stopping_by_look" = report_stopping_by_look(x, format = format,
                                                    heading_level = heading_level)
    )
  }

  invisible(x)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/report_renderers.R"
#' Report Renderers for Different Output Modes
#'
#' Render structured report data in CLI or Markdown format.
#'
#' @name report_renderers
#' @keywords internal
#' @importFrom stats setNames
NULL


# =============================================================================
# TABLE FORMATTING HELPERS
# =============================================================================

#' Format a Data Frame as a CLI Table
#'
#' Creates a nicely formatted table for CLI output with proper alignment.
#'
#' @param df Data frame to format
#' @param max_rows Maximum rows to show (default 20)
#' @return Character vector of formatted lines
#' @keywords internal
format_table_cli <- function(df, max_rows = 20) {
 if (nrow(df) == 0) {
    return("(no data)")
  }

  # Convert to data frame if tibble
 df <- as.data.frame(df)

  # Truncate if too many rows
  truncated <- FALSE
  if (nrow(df) > max_rows) {
    df <- df[1:max_rows, , drop = FALSE]
    truncated <- TRUE
  }

  # Format numeric columns
  for (col in names(df)) {
    if (is.numeric(df[[col]])) {
      # Detect if likely a proportion based on column name AND value range
      # Only format as percentage if column name suggests it's a proportion/probability
      is_proportion_col <- grepl("^(pwr_|prop_|p_sig_|se_pwr_)", col, ignore.case = TRUE)
      vals <- df[[col]][!is.na(df[[col]])]
      if (is_proportion_col && length(vals) > 0 && all(vals >= 0 & vals <= 1)) {
        # Format as percentage
        df[[col]] <- sprintf("%.1f%%", df[[col]] * 100)
      } else if (all(abs(vals) < 100) && any(vals != floor(vals))) {
        # Small decimals - show 2 decimal places
        df[[col]] <- sprintf("%.2f", df[[col]])
      } else {
        # Integers or large numbers - no decimals
        df[[col]] <- sprintf("%.0f", df[[col]])
      }
    } else if (is.list(df[[col]])) {
      # Handle list columns (like par_name)
      df[[col]] <- sapply(df[[col]], function(x) {
        if (is.null(x)) "NULL" else paste(x, collapse = ", ")
      })
    } else {
      df[[col]] <- as.character(df[[col]])
    }
    df[[col]][is.na(df[[col]])] <- "-"
  }

  # Calculate column widths
  col_widths <- sapply(names(df), function(col) {
    max(nchar(col), max(nchar(df[[col]]), na.rm = TRUE))
  })

  # Create header
  header <- paste(
    mapply(function(name, width) sprintf(paste0("%-", width, "s"), name),
           names(df), col_widths),
    collapse = "  "
  )

  # Create separator
  separator <- paste(sapply(col_widths, function(w) strrep("\u2500", w)), collapse = "  ")

  # Create rows
  rows <- apply(df, 1, function(row) {
    paste(
      mapply(function(val, width) sprintf(paste0("%", width, "s"), val),
             row, col_widths),
      collapse = "  "
    )
  })

  # Combine
  result <- c(header, separator, rows)

  if (truncated) {
    result <- c(result, paste0("... (", nrow(df), " of ", max_rows, "+ rows shown)"))
  }

  result
}


#' Format a Data Frame as a Markdown Table
#'
#' Creates a markdown-formatted table with proper alignment.
#'
#' @param df Data frame to format
#' @param max_rows Maximum rows to show (default 20)
#' @return Character vector of formatted lines
#' @keywords internal
format_table_markdown <- function(df, max_rows = 20) {
  if (nrow(df) == 0) {
    return("_(no data)_")
  }

  # Convert to data frame if tibble
  df <- as.data.frame(df)

  # Truncate if too many rows
  truncated <- FALSE
  original_nrow <- nrow(df)
  if (nrow(df) > max_rows) {
    df <- df[1:max_rows, , drop = FALSE]
    truncated <- TRUE
  }

  # Detect numeric columns for alignment
  is_numeric <- sapply(df, is.numeric)

  # Format values
  for (col in names(df)) {
    if (is.numeric(df[[col]])) {
      # Detect if likely a proportion based on column name AND value range
      # Only format as percentage if column name suggests it's a proportion/probability
      is_proportion_col <- grepl("^(pwr_|prop_|p_sig_|se_pwr_)", col, ignore.case = TRUE)
      vals <- df[[col]][!is.na(df[[col]])]
      if (is_proportion_col && length(vals) > 0 && all(vals >= 0 & vals <= 1)) {
        df[[col]] <- sprintf("%.1f%%", df[[col]] * 100)
      } else if (all(abs(vals) < 100) && any(vals != floor(vals))) {
        df[[col]] <- sprintf("%.2f", df[[col]])
      } else {
        df[[col]] <- sprintf("%.0f", df[[col]])
      }
    } else if (is.list(df[[col]])) {
      df[[col]] <- sapply(df[[col]], function(x) {
        if (is.null(x)) "NULL" else paste(x, collapse = ", ")
      })
    } else {
      df[[col]] <- as.character(df[[col]])
    }
    df[[col]][is.na(df[[col]])] <- "-"
  }

  # Calculate column widths
  col_widths <- sapply(names(df), function(col) {
    max(nchar(col), max(nchar(df[[col]]), na.rm = TRUE))
  })

  # Create header row
  header <- paste0("| ", paste(
    mapply(function(name, width) sprintf(paste0("%-", width, "s"), name),
           names(df), col_widths),
    collapse = " | "
  ), " |")

  # Create alignment row
  align_row <- paste0("| ", paste(
    mapply(function(numeric, width) {
      if (numeric) {
        paste0(strrep("-", width - 1), ":")  # Right align for numbers
      } else {
        strrep("-", width)  # Left align for text
      }
    }, is_numeric, col_widths),
    collapse = " | "
  ), " |")

  # Create data rows
  rows <- apply(df, 1, function(row) {
    paste0("| ", paste(
      mapply(function(val, width) sprintf(paste0("%", width, "s"), val),
             row, col_widths),
      collapse = " | "
    ), " |")
  })

  result <- c(header, align_row, rows)

  if (truncated) {
    result <- c(result, "", paste0("_... (showing ", max_rows, " of ", original_nrow, " rows)_"))
  }

  result
}

#' Render Report in CLI Mode
#'
#' Renders a structured report using cli package functions.
#'
#' @param report List with report data from build_report.*()
#' @return NULL (outputs to console)
#' @keywords internal
#'
render_cli <- function(report) {
  # Title
  cli::cli_h1(report$title)

  # Status
  if (!is.null(report$status)) {
    if (report$status == "COMPLETED") {
      cli::cli_alert_success("Analysis completed")
    } else {
      cli::cli_alert_info("Analysis not yet run")
    }
    cli::cli_text("")
  }

  # Render sections
  for (section in report$sections) {
    if (is.null(section)) next

    cli::cli_h2(section$name)

    # Items as definition list
    if (!is.null(section$items)) {
      items_vec <- sapply(names(section$items), function(key) {
        paste0("{.strong ", key, "}: ", section$items[[key]])
      })
      names(items_vec) <- rep("*", length(items_vec))
      cli::cli_bullets(items_vec)
    }

    # Power ranges
    if (!is.null(section$power_ranges)) {
      cli::cli_text("")
      cli::cli_text("{.emph Power ranges:}")
      for (pr in section$power_ranges) {
        cli::cli_bullets(c("*" = paste0("{.field ", pr$name, "}: ", pr$range)))
      }
    }

    # Actions
    if (!is.null(section$actions)) {
      for (action in section$actions) {
        cli::cli_bullets(c("*" = action))
      }
    }

    # Optimal condition
    if (!is.null(section$optimal_condition)) {
      oc <- section$optimal_condition
      if (isTRUE(oc$found)) {
        bullets <- c(
          "*" = paste0("{.strong Achieved power}: ", oc$achieved_pwr),
          "*" = paste0("{.strong Sample size (n_total)}: ", oc$n_total),
          "*" = paste0("{.strong Condition ID}: ", oc$condition_id),
          "*" = paste0("{.strong Parameters}: ", oc$params)
        )
        # Add target power if in target mode
        if (!is.null(oc$target_pwr)) {
          bullets <- c("*" = paste0("{.strong Target power}: ", oc$target_pwr), bullets)
        }
        cli::cli_bullets(bullets)

        # Add interim stats if available
        if (!is.null(oc$interim)) {
          cli::cli_text("")
          cli::cli_text("{.emph Early stopping (this condition):}")
          cli::cli_bullets(c(
            "*" = paste0("{.field Mean N}: ", oc$interim$n_mn),
            "*" = paste0("{.field Median N}: ", oc$interim$n_mdn),
            "*" = paste0("{.field Mode N}: ", oc$interim$n_mode, " (", oc$interim$prop_at_mode, " of trials)"),
            "*" = paste0("{.field Stopped early}: ", oc$interim$prop_stp_early),
            "*" = paste0("{.field Stopped for success}: ", oc$interim$prop_stp_scs),
            "*" = paste0("{.field Stopped for futility}: ", oc$interim$prop_stp_ftl),
            "*" = paste0("{.field No decision}: ", oc$interim$prop_no_dec)
          ))
        }
      } else {
        cli::cli_alert_warning("No condition achieves the target power of {oc$target_pwr}")
        cli::cli_bullets(c(
          "*" = paste0("{.strong Closest power}: ", oc$closest_pwr),
          "*" = paste0("{.strong Sample size (n_total)}: ", oc$closest_n),
          "*" = paste0("{.strong Condition ID}: ", oc$closest_id),
          "*" = paste0("{.strong Parameters}: ", oc$params)
        ))

        # Add interim stats for closest if available
        if (!is.null(oc$interim)) {
          cli::cli_text("")
          cli::cli_text("{.emph Early stopping (closest condition):}")
          cli::cli_bullets(c(
            "*" = paste0("{.field Mean N}: ", oc$interim$n_mn),
            "*" = paste0("{.field Median N}: ", oc$interim$n_mdn),
            "*" = paste0("{.field Mode N}: ", oc$interim$n_mode, " (", oc$interim$prop_at_mode, " of trials)"),
            "*" = paste0("{.field Stopped early}: ", oc$interim$prop_stp_early),
            "*" = paste0("{.field Stopped for success}: ", oc$interim$prop_stp_scs),
            "*" = paste0("{.field Stopped for futility}: ", oc$interim$prop_stp_ftl),
            "*" = paste0("{.field No decision}: ", oc$interim$prop_no_dec)
          ))
        }
      }
    }

    # BRMS args
    if (!is.null(section$brms_args)) {
      cli::cli_text("{.strong BRMS arguments:}")
      for (arg_name in names(section$brms_args)) {
        cli::cli_bullets(c("*" = paste0("{.code ", arg_name, "}: ", section$brms_args[[arg_name]])))
      }
    }

    # Note
    if (!is.null(section$note)) {
      cli::cli_text("")
      cli::cli_alert_info(section$note)
    }

    # Grid (data frame)
    if (!is.null(section$grid)) {
      cli::cli_text("")
      table_lines <- format_table_cli(section$grid)
      cat(paste(table_lines, collapse = "\n"), "\n")
    }

    # BRMS model
    if (!is.null(section$brms_model)) {
      cli::cli_text("")
      print(section$brms_model)
    }

    # Bayesflow model - show summary if available
    if (!is.null(section$bayesflow_model)) {
      cli::cli_text("")
      bf_model <- section$bayesflow_model
      # Try to call summary() method (works for Keras/BayesFlow models)
      tryCatch({
        if (!is.null(bf_model$summary)) {
          bf_model$summary()
        } else {
          print(bf_model)
        }
      }, error = function(e) {
        print(bf_model)
      })
    }

    # Backend args
    if (!is.null(section$backend_args)) {
      cli::cli_text("")
      print(section$backend_args)
    }

    cli::cli_text("")  # Blank line between sections
  }

  invisible(NULL)
}

#' Render Report in Markdown Mode
#'
#' Renders a structured report in markdown format.
#'
#' @param report List with report data from build_report.*()
#' @param heading_level Integer specifying the starting heading level (default 2)
#' @return NULL (outputs to console)
#' @keywords internal
#'
render_markdown <- function(report, heading_level = 2L) {
  output <- character()

  # Helper to create heading with the appropriate level
  make_heading <- function(text, level_offset = 0) {
    level <- min(heading_level + level_offset, 6)  # Cap at h6
    paste0(strrep("#", level), " ", text)
  }

  # Title (primary heading)
  output <- c(output, make_heading(report$title, 0), "")

  # Status
  if (!is.null(report$status)) {
    status_symbol <- ifelse(report$status == "COMPLETED", "\u2714", "\u2139")  #  or 
    status_text <- paste0("**STATUS**: [", report$status, "] ",
                         ifelse(report$status == "COMPLETED", "Analysis completed", "Analysis not yet run"))
    output <- c(output, paste0(status_symbol, " ", status_text), "")
  }

  output <- c(output, "---", "")

  # Render sections
  for (section in report$sections) {
    if (is.null(section)) next

    output <- c(output, make_heading(section$name, 1), "")

    # Items as bullet list
    if (!is.null(section$items)) {
      for (key in names(section$items)) {
        output <- c(output, paste0("- **", key, "**: ", section$items[[key]]))
      }
      output <- c(output, "")
    }

    # Power ranges
    if (!is.null(section$power_ranges)) {
      output <- c(output, "_Power ranges:_", "")
      for (pr in section$power_ranges) {
        output <- c(output, paste0("- **", pr$name, "**: ", pr$range))
      }
      output <- c(output, "")
    }

    # Actions
    if (!is.null(section$actions)) {
      for (action in section$actions) {
        output <- c(output, paste0("- ", action))
      }
      output <- c(output, "")
    }

    # Optimal condition
    if (!is.null(section$optimal_condition)) {
      oc <- section$optimal_condition
      if (isTRUE(oc$found)) {
        # Add target power only if in target mode
        if (!is.null(oc$target_pwr)) {
          output <- c(output, paste0("- **Target power**: ", oc$target_pwr))
        }
        output <- c(output,
          paste0("- **Achieved power**: ", oc$achieved_pwr),
          paste0("- **Sample size (n_total)**: ", oc$n_total),
          paste0("- **Condition ID**: ", oc$condition_id),
          paste0("- **Parameters**: ", oc$params)
        )

        # Add interim stats if available
        if (!is.null(oc$interim)) {
          output <- c(output,
            "",
            "_Early stopping (this condition):_",
            paste0("- Mean N: ", oc$interim$n_mn),
            paste0("- Median N: ", oc$interim$n_mdn),
            paste0("- Mode N: ", oc$interim$n_mode, " (", oc$interim$prop_at_mode, " of trials)"),
            paste0("- Stopped early: ", oc$interim$prop_stp_early),
            paste0("- Stopped for success: ", oc$interim$prop_stp_scs),
            paste0("- Stopped for futility: ", oc$interim$prop_stp_ftl),
            paste0("- No decision: ", oc$interim$prop_no_dec)
          )
        }
        output <- c(output, "")
      } else {
        output <- c(output,
          paste0("\u26A0\uFE0F No condition achieves the target power of ", oc$target_pwr),
          "",
          paste0("- **Closest power**: ", oc$closest_pwr),
          paste0("- **Sample size (n_total)**: ", oc$closest_n),
          paste0("- **Condition ID**: ", oc$closest_id),
          paste0("- **Parameters**: ", oc$params)
        )

        # Add interim stats for closest if available
        if (!is.null(oc$interim)) {
          output <- c(output,
            "",
            "_Early stopping (closest condition):_",
            paste0("- Mean N: ", oc$interim$n_mn),
            paste0("- Median N: ", oc$interim$n_mdn),
            paste0("- Mode N: ", oc$interim$n_mode, " (", oc$interim$prop_at_mode, " of trials)"),
            paste0("- Stopped early: ", oc$interim$prop_stp_early),
            paste0("- Stopped for success: ", oc$interim$prop_stp_scs),
            paste0("- Stopped for futility: ", oc$interim$prop_stp_ftl),
            paste0("- No decision: ", oc$interim$prop_no_dec)
          )
        }
        output <- c(output, "")
      }
    }

    # BRMS args
    if (!is.null(section$brms_args)) {
      output <- c(output, "**BRMS arguments:**", "")
      for (arg_name in names(section$brms_args)) {
        output <- c(output, paste0("- `", arg_name, "`: ", section$brms_args[[arg_name]]))
      }
      output <- c(output, "")
    }

    # Note
    if (!is.null(section$note)) {
      output <- c(output, "> **Note**: ", paste0("> ", strsplit(section$note, "\n")[[1]]), "")
    }

    # Grid (data frame)
    if (!is.null(section$grid)) {
      table_lines <- format_table_markdown(section$grid)
      output <- c(output, table_lines, "")
    }

    # BRMS model
    if (!is.null(section$brms_model)) {
      output <- c(output, "```")
      model_output <- utils::capture.output(print(section$brms_model))
      output <- c(output, model_output)
      output <- c(output, "```", "")
    }

    # Bayesflow model - show summary if available
    if (!is.null(section$bayesflow_model)) {
      output <- c(output, "```")
      bf_model <- section$bayesflow_model
      model_output <- tryCatch({
        if (!is.null(bf_model$summary)) {
          utils::capture.output(bf_model$summary())
        } else {
          utils::capture.output(print(bf_model))
        }
      }, error = function(e) {
        utils::capture.output(print(bf_model))
      })
      output <- c(output, model_output)
      output <- c(output, "```", "")
    }

    # Backend args
    if (!is.null(section$backend_args)) {
      output <- c(output, "```")
      args_output <- utils::capture.output(print(section$backend_args))
      output <- c(output, args_output)
      output <- c(output, "```", "")
    }
  }

  # Output everything as one block
  cat(paste(output, collapse = "\n"), "\n")
  invisible(NULL)
}

#' Render Report Based on Output Format
#'
#' Routes to appropriate renderer based on specified format or global setting.
#'
#' @param report List with report data from build_report.*()
#' @param format Output format: "cli" or "markdown". If NULL (default), uses
#'   the global output mode setting from [get_output_mode()].
#' @param heading_level Integer specifying the starting heading level for
#'   markdown output (default 2). Ignored for CLI format.
#' @return NULL (outputs to console)
#' @keywords internal
#'
render_report <- function(report, format = NULL, heading_level = 2L) {
  # Use explicit format if provided, otherwise fall back to global setting
  if (is.null(format)) {
    format <- get_output_mode()
  }

  if (format == "cli") {
    render_cli(report)
  } else {
    render_markdown(report, heading_level = heading_level)
  }

  invisible(NULL)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/required_fn_args.R"
#' Identify Required Parameters for Design or Model Objects
#'
#' Generic wrapper function that identifies required parameters for either
#' rctbp_design or rctbp_model objects by dispatching to the
#' appropriate specific function.
#'
#' @param object Either an rctbp_design or rctbp_model object
#' @param print Logical. If TRUE (default), prints the required parameters to console
#'
#' @return For design objects: a list with simulation, interim, and all parameters.
#'   For model objects: a character vector of required parameters.
#'   Both returned invisibly.
#'
#' @examples
#' \dontrun{
#' required_fn_args(my_object)
#' }
#'
#' @export
required_fn_args <- function(object, print = TRUE) {
  if (inherits(object, "rctbayespower::rctbp_design") || inherits(object, "rctbp_design")) {
    return(required_fn_args_design(object, print))
  } else if (inherits(object, "rctbayespower::rctbp_model") || inherits(object, "rctbp_model")) {
    return(required_fn_args_model(object, print))
  } else {
    cli::cli_abort(c(
      "{.arg object} must be either an rctbp_design or rctbp_model object",
      "x" = "You supplied {.cls {class(object)}}",
      "i" = "Provide a design or model object created with {.fn build_design} or {.fn build_model}"
    ))
  }
}


#' Get Function Arguments Without Default Values
#'
#' Extracts the names of function arguments that do not have default values.
#' This is a utility function used for identifying required parameters.
#'
#' @param fn A function object to analyze
#'
#' @return A character vector containing the names of arguments without default values
#'
#' @keywords internal
#'
get_arg_defaults <- function(fn) {
  fmls <- formals(fn)
  env <- environment(fn)

  result <- lapply(fmls, function(default_expr) {
    if (identical(default_expr, quote(expr =))) {
      list(
        has_default = FALSE,
        is_null = FALSE,
        value = quote(expr =)
      )
    } else {
      val <- try(eval(default_expr, envir = env), silent = TRUE)
      list(
        has_default = TRUE,
        is_null = is.null(val),
        value = val
      )
    }
  })

  # Extract names for special cases
  missing_default <- names(result)[!vapply(result, `[[`, logical(1), "has_default")]
  null_default    <- names(result)[vapply(result, `[[`, logical(1), "is_null")]
  # Remove unevaluable or "no-default" entries from evaluated_defaults
  evaluated_defaults <- lapply(result, `[[`, "value")
  evaluated_defaults <- evaluated_defaults[!names(evaluated_defaults) %in% missing_default]
  # Always include evaluated_defaults
  output <- list(evaluated_defaults = evaluated_defaults)

  # Only include if non-empty
  if (length(missing_default) > 0) {
    output$missing_default <- missing_default
  }
  if (length(null_default) > 0) {
    output$null_default <- null_default
  }

  output
}


#' Identify Required Parameters for a Design
#'
#' Extracts the required parameters (those without default values) from the
#' data simulation function and interim analysis function of an rctbayespower
#' design object. This helps users identify which parameters must be specified
#' before running simulations.
#'
#' @param design An rctbp_design object
#' @param print Logical. If TRUE (default), prints the required parameters to console
#'
#' @return A character vector containing the names of all required parameters
#'
#' @keywords internal
required_fn_args_design <- function(design, print = TRUE) {
  # check that design is a valid rctbp_design object (allow both namespaced and non-namespaced for testing)
  if (!inherits(design, "rctbayespower::rctbp_design") && !inherits(design, "rctbp_design")) {
    cli::cli_abort(c(
      "{.arg design} must be a valid rctbp_design object",
      "x" = "You supplied {.cls {class(design)}}",
      "i" = "Create a design object using {.fn build_design}"
    ))
  }

  # get args without defaults for the data simulation function
  params_sim <- get_args_without_defaults(design@model@data_simulation_fn)

  # Decision parameters (per-condition): thresholds, analysis_at, interim_function, adaptive
  params_decision <- c("thresholds_success", "thresholds_futility", "analysis_at",
                       "interim_function", "adaptive")

  params_all <- c(params_sim, params_decision)

  # print the parameters if requested
  if (print) {
    cli::cli_h3("Arguments that need user specification")
    cli::cli_text("")

    cli::cli_text("{.strong Simulation function parameters:}")
    if (length(params_sim) > 0) {
      param_items <- params_sim
      names(param_items) <- rep("*", length(param_items))
      cli::cli_bullets(param_items)
    } else {
      cli::cli_text("  {.emph (none)}")
    }
    cli::cli_text("")

    cli::cli_text("{.strong Decision parameters (per-condition):}")
    if (length(params_decision) > 0) {
      param_items <- params_decision
      names(param_items) <- rep("*", length(param_items))
      cli::cli_bullets(param_items)
    } else {
      cli::cli_text("  {.emph (none)}")
    }
  }

  # return the parameters needed for the design
  invisible(list(
    params_sim = params_sim,
    params_decision = params_decision,
    params_all = params_all
  ))
}


#' Identify Required Parameters for a Model
#'
#' Extracts the required parameters (those without default values) from the
#' data simulation function of an rctbayespower model object.
#'
#' @param model An rctbayespower_model object
#' @param print Logical. If TRUE (default), prints the required parameters to console
#'
#' @return A character vector containing the names of required parameters (returned invisibly)
#'
#' @keywords internal
required_fn_args_model <- function(model, print = TRUE) {
  # check that model is a valid rctbayespower_model object (allow both namespaced and non-namespaced for testing)
  if (!inherits(model, "rctbayespower::rctbp_model") && !inherits(model, "rctbp_model")) {
    cli::cli_abort(c(
      "{.arg model} must be a valid rctbp_model object",
      "x" = "You supplied {.cls {class(model)}}",
      "i" = "Create a model object using {.fn build_model}"
    ))
  }

  # get args without defaults for the model
  params <- get_args_without_defaults(model@data_simulation_fn)

  # print the parameters if requested
  if (print) {
    cli::cli_h3("Arguments that need user specification")
    cli::cli_text("")

    cli::cli_text("{.strong Simulation function parameters:}")
    if (length(params) > 0) {
      param_items <- params
      names(param_items) <- rep("*", length(param_items))
      cli::cli_bullets(param_items)
    } else {
      cli::cli_text("  {.emph (none)}")
    }
  }

  invisible(params)
}


#' Get Function Arguments Without Default Values
#'
#' Internal utility function that extracts the names of function arguments
#' that do not have default values (either missing or explicitly NULL).
#'
#' @param fn A function object to analyze
#'
#' @return A character vector containing the names of arguments without default values
#'
#' @keywords internal
#'
get_args_without_defaults <- function(fn) {
  # Get the args
  args_list <- get_arg_defaults(fn)

  missing <- args_list$missing_default
  nulls <- args_list$null_default

  # Combine the names of missing and null args
  c(missing, nulls)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/s3_wrappers.R"
# =============================================================================
# S3 METHOD WRAPPERS FOR S7 CLASSES
# =============================================================================
# These S3 generic wrappers enable standard R methods (print, plot, run) to
# work with S7 class objects. Without these wrappers, base R wouldn't recognize
# S7 methods when users call generic functions.
#
# How it works: When a user calls plot(power_analysis_object), R's S3 dispatch
# system looks for plot.rctbp_power_analysis. These wrappers delegate to the
# actual S7 method implementations.

#' @export
plot.rctbp_power_analysis <- function(x, ...) {
  S7::method(plot, rctbp_power_analysis)(x, ...)
}

#' @export
print.rctbp_power_analysis <- function(x, ...) {
  S7::method(print, rctbp_power_analysis)(x, ...)
}

#' @export
summary.rctbp_power_analysis <- function(object, ...) {
  S7::method(summary, rctbp_power_analysis)(object, ...)
}

#' @export
print.rctbp_conditions <- function(x, ...) {
  S7::method(print, rctbp_conditions)(x, ...)
}

#' @export
print.rctbp_design <- function(x, ...) {
  S7::method(print, rctbp_design)(x, ...)
}

#' @export
print.rctbp_model <- function(x, ...) {
  S7::method(print, rctbp_model)(x, ...)
}

#' @export
run.rctbp_power_analysis <- function(x, ...) {
  S7::method(run, rctbp_power_analysis)(x, ...)
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/simulate_single_run.R"
# =============================================================================
# CORE SIMULATION FUNCTION (DEPRECATED)
# =============================================================================
# DEPRECATION NOTICE: This file is deprecated as of 2025-11.
#
# The simulate_single_run() function has been superseded by:
#   - R/worker_functions.R: worker_process_single()
#   - R/backend_brms.R: estimate_single_brms()
#   - R/backend_bf.R: estimate_single_bf()
#
# This file is kept for backward compatibility with legacy code in R/legacy/.
# It will be removed in a future version.
# =============================================================================

#' Single Run Simulation for RCT Bayesian Power Analysis (DEPRECATED)
#'
#' @description
#' `r lifecycle::badge('deprecated')`
#'
#' This function is deprecated. Use [worker_process_single()] instead.
#'
#' @param condition_arguments A single entry from the condition_arguments list
#' @param id_iter Iteration identifier
#' @param design A rctbp_design object
#'
#' @return A data frame with power analysis measures
#'
#' @keywords internal
simulate_single_run <- function(condition_arguments,
                                id_iter,
                                design) {

  # Forward to new worker function
  worker_process_single(
    id_cond = condition_arguments$id_cond,
    id_iter = id_iter,
    condition_args = condition_arguments,
    design = design
  )
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/utils_results.R"
# =============================================================================
# SHARED RESULT UTILITIES
# =============================================================================
# Functions used across multiple backend files for creating standardized
# result data structures.

#' Create Error Result Row
#'
#' Helper function to create a standardized error result data frame when
#' estimation or extraction fails. Used by all backend estimation functions.
#'
#' @param id_iter Iteration identifier
#' @param id_cond Condition identifier
#' @param id_analysis Analysis identifier (0 for single, 1+ for sequential)
#' @param error_msg Error message describing the failure
#'
#' @return Data frame with NA values and error information, matching the
#'   standard output schema for power analysis results
#' @keywords internal
create_error_result <- function(id_iter, id_cond, id_analysis, error_msg) {
  data.frame(
    par_name = NA_character_,
    thr_scs = NA_real_,
    thr_ftl = NA_real_,
    p_sig_scs = NA_real_,
    p_sig_ftl = NA_real_,
    pr_scs = NA_real_,
    pr_ftl = NA_real_,
    dec_scs = NA_real_,
    dec_ftl = NA_real_,
    post_med = NA_real_,
    post_mad = NA_real_,
    post_mn = NA_real_,
    post_sd = NA_real_,
    rhat = NA_real_,
    ess_bulk = NA_real_,
    ess_tail = NA_real_,
    id_iter = id_iter,
    id_cond = id_cond,
    id_look = id_analysis,
    n_analyzed = NA_integer_,
    stopped = NA,
    stop_reason = NA_character_,
    converged = 0L,
    error_msg = error_msg,
    stringsAsFactors = FALSE
  )
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/verbosity.R"
#' Verbosity Control System
#'
#' Provides three-level verbosity control (0 = quiet, 1 = normal, 2 = verbose)
#' for all package output.
#'
#' @name verbosity
#' @keywords internal
NULL

#' Get Current Verbosity Level
#'
#' Returns the current verbosity level setting for the package.
#'
#' @return Integer: 0 (quiet), 1 (normal), or 2 (verbose)
#' @export
#'
#' @examples
#' get_verbosity()
#'
get_verbosity <- function() {
  level <- getOption("rctbayespower.verbosity", default = 1)

  if (!level %in% c(0, 1, 2)) {
    cli::cli_warn(c(
      "Invalid verbosity level: {.val {level}}",
      "i" = "Using default {.val 1} (normal)"
    ))
    level <- 1
  }

  return(level)
}

#' Set Verbosity Level
#'
#' Sets the verbosity level for all package output.
#'
#' @param verbosity Integer: verbosity level
#'   * 0 = quiet (minimal output, errors only)
#'   * 1 = normal (standard output, default)
#'   * 2 = verbose (detailed output for debugging)
#'
#' @return Invisibly returns the previous verbosity level.
#' @export
#'
#' @examples
#' # Set to quiet mode
#' set_verbosity(0)
#'
#' # Set to verbose mode
#' set_verbosity(2)
#'
#' # Back to normal
#' set_verbosity(1)
#'
set_verbosity <- function(verbosity = c(0, 1, 2)) {
  if (!verbosity %in% c(0, 1, 2)) {
    cli::cli_abort(c(
      "{.arg verbosity} must be 0, 1, or 2",
      "x" = "You supplied {.val {verbosity}}"
    ))
  }

  old_level <- getOption("rctbayespower.verbosity", default = 1)
  options(rctbayespower.verbosity = verbosity)
  invisible(old_level)
}

#' Temporarily Change Verbosity Level
#'
#' Executes code with a temporary verbosity level.
#'
#' @param verbosity Integer: verbosity level (0, 1, or 2)
#' @param code Code to execute with the temporary verbosity
#'
#' @return Result of evaluating `code`
#' @export
#'
#' @examples
#' \dontrun{
#' # Run analysis in quiet mode temporarily
#' with_verbosity(0, {
#'   result <- power_analysis(conditions, n_sims = 100)
#' })
#' }
#'
with_verbosity <- function(verbosity = c(0, 1, 2), code) {
  if (!verbosity %in% c(0, 1, 2)) {
    cli::cli_abort(c(
      "{.arg verbosity} must be 0, 1, or 2",
      "x" = "You supplied {.val {verbosity}}"
    ))
  }

  old_level <- set_verbosity(verbosity)
  on.exit(options(rctbayespower.verbosity = old_level))
  force(code)
}

#' Check if Message Should Be Displayed
#'
#' Determines whether a message at a given level should be shown
#' based on current verbosity settings.
#'
#' @param required_level Integer: minimum verbosity level required to show message
#'   * 0 = always show (errors, critical messages)
#'   * 1 = show in normal and verbose modes (standard messages)
#'   * 2 = show only in verbose mode (debug messages)
#'
#' @return Logical: TRUE if message should be displayed, FALSE otherwise
#' @export
#'
#' @examples
#' # Check if verbose messages should be shown
#' if (should_show(2)) {
#'   message("This is a debug message")
#' }
#'
should_show <- function(required_level = 1) {
  current_level <- get_verbosity()
  return(current_level >= required_level)
}

#' Conditional Message Output
#'
#' Outputs a message only if current verbosity level permits.
#'
#' @param msg Character string: message to display
#' @param level Integer: minimum verbosity level required (0, 1, or 2)
#' @param .envir Environment for string interpolation
#'
#' @return NULL (outputs to console if verbosity permits)
#' @keywords internal
#'
verbosity_message <- function(msg, level = 1, .envir = parent.frame()) {
  if (should_show(level)) {
    cli::cli_alert_info(msg, .envir = .envir)
  }
  invisible(NULL)
}

#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/worker_functions.R"
# Worker Functions for Parallel Execution
# Process work units (id_cond, id_iter) with appropriate estimation strategy

#' Process Single Work Unit
#'
#' Processes one (id_cond, id_iter) work unit. Detects strategy from design
#' and calls appropriate estimation function.
#'
#' @param id_cond Condition identifier
#' @param id_iter Iteration identifier (within condition)
#' @param condition_args Condition arguments with sim_args
#' @param design Design object (S7 or list for parallel workers)
#'
#' @return Data frame with results for all analyses (1 row for single, n rows for sequential)
#' @keywords internal
worker_process_single <- function(id_cond, id_iter, condition_args, design) {

  # Extract design components (handle S7 and list)
  if (inherits(design, "rctbayespower::rctbp_design") || inherits(design, "rctbp_design")) {
    data_simulation_fn <- design@model@data_simulation_fn
    backend <- design@model@backend
    estimation_model <- if (backend == "brms") design@model@brms_model else design@model@bayesflow_model
    backend_args <- if (backend == "brms") design@model@backend_args_brms else design@model@backend_args_bf
    target_params <- design@target_params
    p_sig_scs <- design@p_sig_scs
    p_sig_ftl <- design@p_sig_ftl
  } else if (is.list(design)) {
    # Regular list for parallel workers
    data_simulation_fn <- design$model_data_simulation_fn
    backend <- design$model_backend
    estimation_model <- if (backend == "brms") {
      design$model_brms_model
    } else {
      design$model_bayesflow_model
    }
    backend_args <- design$model_backend_args
    target_params <- design$target_params
    p_sig_scs <- design$p_sig_scs
    p_sig_ftl <- design$p_sig_ftl
  } else {
    cli::cli_abort(c(
      "Invalid design object",
      "i" = "This is an internal error - please report"
    ))
  }

  # Extract decision parameters from condition_args
  decision_args <- condition_args$decision_args
  thresholds_success <- decision_args$thresholds_success
  thresholds_futility <- decision_args$thresholds_futility
  analysis_at <- decision_args$analysis_at
  interim_function <- decision_args$interim_function
  adaptive <- decision_args$adaptive %||% FALSE

  # =============================================================================
  # STRATEGY DETECTION (Single vs Sequential vs Adaptive)
  # =============================================================================
  # Determines analysis strategy based on decision parameters:
  #
  # "single" - No interim analyses (analysis_at = NULL or empty)
  #   - One analysis at n_total only
  #   - Most common scenario
  #
  # "sequential" - Has interim timepoints, non-adaptive
  #   - Fixed interim analyses at predefined sample sizes
  #   - Early stopping possible but parameters don't change
  #
  # "adaptive" - Has interim timepoints, adaptive = TRUE
  #   - Parameters can be modified between looks
  #   - Not yet implemented (planned feature)
  has_interims <- !is.null(analysis_at) && length(analysis_at) > 0
  strategy <- if (!has_interims) {
    "single"
  } else if (!adaptive) {
    "sequential"
  } else {
    "adaptive"
  }

  # Simulate full dataset (unless adaptive - which handles its own simulation)
  if (strategy %in% c("single", "sequential")) {
    full_data <- tryCatch({
      do.call(data_simulation_fn, condition_args$sim_args)
    }, error = function(e) {
      cli::cli_warn(c(
        "Data simulation failed",
        "x" = "Condition {id_cond}, iteration {id_iter}",
        "i" = "Error: {e$message}"
      ))
      return(NULL)
    })

    if (is.null(full_data)) {
      return(create_error_result(id_iter, id_cond, id_analysis = 0L, "Data simulation failed"))
    }
  }

  # Call appropriate estimation function
  result <- switch(strategy,
    single = {
      if (backend == "brms") {
        estimate_single_brms(
          data = full_data,
          model = estimation_model,
          backend_args = backend_args,
          target_params = target_params,
          thresholds_success = thresholds_success,
          thresholds_futility = thresholds_futility,
          p_sig_scs = p_sig_scs,
          p_sig_ftl = p_sig_ftl,
          id_iter = id_iter,
          id_cond = id_cond
        )
      } else if (backend == "bf") {
        estimate_single_bf(
          data = full_data,
          model = estimation_model,
          backend_args = backend_args,
          target_params = target_params,
          thresholds_success = thresholds_success,
          thresholds_futility = thresholds_futility,
          p_sig_scs = p_sig_scs,
          p_sig_ftl = p_sig_ftl,
          id_iter = id_iter,
          id_cond = id_cond
        )
      } else {
        cli::cli_abort(c(
          "Unknown backend: {.val {backend}}",
          "i" = "Supported backends: {.val brms}, {.val bf}"
        ))
      }
    },
    sequential = {
      if (backend == "brms") {
        estimate_sequential_brms(
          full_data = full_data,
          model = estimation_model,
          backend_args = backend_args,
          target_params = target_params,
          thresholds_success = thresholds_success,
          thresholds_futility = thresholds_futility,
          p_sig_scs = p_sig_scs,
          p_sig_ftl = p_sig_ftl,
          analysis_at = analysis_at,
          interim_function = interim_function,
          id_iter = id_iter,
          id_cond = id_cond
        )
      } else if (backend == "bf") {
        # For single simulation with BayesFlow sequential, pass as list
        estimate_sequential_bf(
          full_data_list = list(full_data),
          model = estimation_model,
          backend_args = backend_args,
          target_params = target_params,
          thresholds_success = thresholds_success,
          thresholds_futility = thresholds_futility,
          p_sig_scs = p_sig_scs,
          p_sig_ftl = p_sig_ftl,
          analysis_at = analysis_at,
          interim_function = interim_function,
          id_iter = id_iter,
          id_cond = id_cond
        )
      } else {
        cli::cli_abort(c(
          "Unknown backend: {.val {backend}}",
          "i" = "Supported backends: {.val brms}, {.val bf}"
        ))
      }
    },
    adaptive = {
      cli::cli_abort(c(
        "Adaptive strategy not yet implemented",
        "i" = "This feature is planned for future releases"
      ))
    }
  )

  return(result)
}


#' Process Batch of Work Units (BayesFlow Batching)
#'
#' Processes a batch of (id_cond, id_iter) work units together. Only used when
#' backend = "bf" (BayesFlow) and batch_size > 1. Enables efficient batch processing
#' through neural posterior estimation.
#'
#' @param work_units List of work unit specifications, each with id_cond, id_iter, condition_args
#' @param design Design object (S7 or list for parallel workers)
#'
#' @return Data frame with results for all work units and all analyses
#' @keywords internal
worker_process_batch <- function(work_units, design) {

  batch_size <- length(work_units)

  # Extract design components
  if (inherits(design, "rctbayespower::rctbp_design") || inherits(design, "rctbp_design")) {
    data_simulation_fn <- design@model@data_simulation_fn
    backend <- design@model@backend
    estimation_model <- design@model@bayesflow_model  # Must be BayesFlow
    backend_args <- design@model@backend_args_bf
    target_params <- design@target_params
    p_sig_scs <- design@p_sig_scs
    p_sig_ftl <- design@p_sig_ftl
  } else if (is.list(design)) {
    data_simulation_fn <- design$model_data_simulation_fn
    backend <- design$model_backend
    estimation_model <- design$model_bayesflow_model
    backend_args <- design$model_backend_args  # Already selected in prepare_design_for_workers
    target_params <- design$target_params
    p_sig_scs <- design$p_sig_scs
    p_sig_ftl <- design$p_sig_ftl
  } else {
    cli::cli_abort(c(
      "Invalid design object",
      "i" = "This is an internal error - please report"
    ))
  }

  # Validate this is BayesFlow
  if (backend != "bf") {
    cli::cli_abort(c(
      "{.fn worker_process_batch} should only be called for BayesFlow backend",
      "x" = "Backend is {.val {backend}}",
      "i" = "This is an internal error - please report"
    ))
  }

  # Extract IDs
  id_cond_vec <- sapply(work_units, function(wu) wu$id_cond)
  id_iter_vec <- sapply(work_units, function(wu) wu$id_iter)

  # Extract decision parameters from first work unit (assume homogenous batch)
  # Note: For batching to work efficiently, all work units should have same decision_args
  decision_args <- work_units[[1]]$condition_args$decision_args
  thresholds_success <- decision_args$thresholds_success
  thresholds_futility <- decision_args$thresholds_futility
  analysis_at <- decision_args$analysis_at
  interim_function <- decision_args$interim_function
  adaptive <- decision_args$adaptive %||% FALSE

  if (adaptive) {
    cli::cli_abort(c(
      "Adaptive strategy with batching not yet implemented",
      "i" = "This feature is planned for future releases"
    ))
  }

  # Detect strategy
  has_interims <- !is.null(analysis_at) && length(analysis_at) > 0
  strategy <- if (!has_interims) "single" else "sequential"

  # Simulate full datasets for all work units
  full_data_list <- lapply(work_units, function(wu) {
    tryCatch({
      do.call(data_simulation_fn, wu$condition_args$sim_args)
    }, error = function(e) {
      cli::cli_warn(c(
        "Data simulation failed",
        "x" = "Condition {wu$id_cond}, iteration {wu$id_iter}",
        "i" = "Error: {e$message}"
      ))
      return(NULL)
    })
  })

  # Check for simulation failures
  failed_sims <- which(sapply(full_data_list, is.null))
  if (length(failed_sims) > 0) {
    # Create error results for failed simulations
    error_results <- lapply(failed_sims, function(i) {
      create_error_result(
        id_iter_vec[i],
        id_cond_vec[i],
        id_analysis = 0L,
        "Data simulation failed"
      )
    })

    # Remove failed from batch
    if (length(failed_sims) == batch_size) {
      # All failed
      return(dplyr::bind_rows(error_results))
    }

    # Continue with successful sims
    full_data_list <- full_data_list[-failed_sims]
    id_cond_vec <- id_cond_vec[-failed_sims]
    id_iter_vec <- id_iter_vec[-failed_sims]
  } else {
    error_results <- list()
  }

  # Call appropriate estimation function (BayesFlow backend)
  result <- switch(strategy,
    single = {
      estimate_single_bf(
        data = full_data_list,
        model = estimation_model,
        backend_args = backend_args,
        target_params = target_params,
        thresholds_success = thresholds_success,
        thresholds_futility = thresholds_futility,
        p_sig_scs = p_sig_scs,
        p_sig_ftl = p_sig_ftl,
        id_iter = id_iter_vec,
        id_cond = id_cond_vec
      )
    },
    sequential = {
      estimate_sequential_bf(
        full_data_list = full_data_list,
        model = estimation_model,
        backend_args = backend_args,
        target_params = target_params,
        thresholds_success = thresholds_success,
        thresholds_futility = thresholds_futility,
        p_sig_scs = p_sig_scs,
        p_sig_ftl = p_sig_ftl,
        analysis_at = analysis_at,
        interim_function = interim_function,
        id_iter = id_iter_vec,
        id_cond = id_cond_vec
      )
    }
  )

  # Combine with any error results
  if (length(error_results) > 0) {
    result <- dplyr::bind_rows(result, dplyr::bind_rows(error_results))
  }

  return(result)
}


#' Prepare Design Components for Parallel Workers
#'
#' Serializes design object into a list that can be passed to parallel workers.
#' Necessary because S7 objects may not serialize correctly across cluster boundaries.
#'
#' @param design S7 rctbp_design object
#'
#' @return Named list with all necessary design components
#' @keywords internal
prepare_design_for_workers <- function(design) {
  # Backend is already resolved at model creation time (never "auto")
  backend <- design@model@backend

  # Select appropriate backend_args based on backend
  backend_args <- if (backend == "brms") {
    design@model@backend_args_brms
  } else {
    design@model@backend_args_bf
  }

  list(
    # Model components
    model_data_simulation_fn = design@model@data_simulation_fn,
    model_backend = backend,
    model_brms_model = design@model@brms_model,
    model_bayesflow_model = design@model@bayesflow_model,
    model_backend_args = backend_args,
    model_backend_args_brms = design@model@backend_args_brms,
    model_backend_args_bf = design@model@backend_args_bf,
    # Design parameters
    target_params = design@target_params,
    p_sig_scs = design@p_sig_scs,
    p_sig_ftl = design@p_sig_ftl,
    # Interim analysis parameters (design-level defaults)
    analysis_at = design@analysis_at,
    interim_function = design@interim_function,
    adaptive = design@adaptive
  )
}
#line 1 "C:/Users/Matze/Documents/GitHub/rctbayespower/R/zzz.R"
.onLoad <- function(libname, pkgname) {
  S7::methods_register()
}
